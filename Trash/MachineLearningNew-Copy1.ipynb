{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib.ticker import (AutoMinorLocator, MultipleLocator)\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "from multiprocessing import Process, Queue\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import SGD, adam, adagrad, rmsprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing own code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generateSpace import *\n",
    "from plotLattice import *\n",
    "from getTriangleLengths import *\n",
    "from potentialEnergyPerTrio import *\n",
    "from potentialEnergy import *\n",
    "from potentialEnergyPerParticle import *\n",
    "from numberOfCalculations import *\n",
    "from makeRandomDatabase import *\n",
    "from prepareDatabaseForMachineLearning import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing own code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistory(fitDetails):\n",
    "    \"\"\"\n",
    "    hjhgj\n",
    "    \"\"\"\n",
    "    history = {'loss': fitDetails.history['loss'], 'val_loss': fitDetails.history['val_loss']}\n",
    "    \n",
    "    maximumValue = max(max(history['loss']), max(history['val_loss']))\n",
    "    minimumValue = min(min(history['loss']), min(history['val_loss'])) / 10#math.pow(math.floor(math.log(min(min(history['loss']), min(history['val_loss'])), 10)) - 1, 10)\n",
    "    numberOfEpochs = len(history['loss'])\n",
    "    \n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(16, 16))\n",
    "    ax1.plot([i for i in range(1, numberOfEpochs + 1)], history['loss'], label='Training loss')\n",
    "    ax1.plot([i for i in range(1, numberOfEpochs + 1)], history['val_loss'], label='Validation loss')\n",
    "    ax1.legend()\n",
    "    ax1.set_xlim(1, numberOfEpochs)\n",
    "    ax1.set_ylim(0, maximumValue)\n",
    "    ax1.set_xlabel('Epochs')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.set_title('Loss (training and validation) vs the number of epochs.')\n",
    "    \n",
    "    ax2.plot(history['loss'], history['val_loss'])\n",
    "    ax2.set_xlim(0, maximumValue)\n",
    "    ax2.set_ylim(0, maximumValue)\n",
    "    ax2.set_xlabel('Training loss')\n",
    "    ax2.set_ylabel('Validation loss')\n",
    "    ax2.set_title('Validation loss vs training loss.')\n",
    "    \n",
    "    ax3.plot([i for i in range(1, numberOfEpochs + 1)], history['loss'], label='Training loss')\n",
    "    ax3.plot([i for i in range(1, numberOfEpochs + 1)], history['val_loss'], label='Validation loss')\n",
    "    ax3.legend()\n",
    "    ax3.set_xlim(1, numberOfEpochs)\n",
    "    ax3.set_yscale('log')\n",
    "    ax3.set_ylim(minimumValue, maximumValue)\n",
    "    ax3.set_xlabel('Epochs')\n",
    "    ax3.set_ylabel('Loss')\n",
    "    ax3.set_title('Loss (training and validation) vs the number of epochs (logarithmic)')\n",
    "    \n",
    "    ax4.plot(history['loss'], history['val_loss'])\n",
    "    ax4.set_xscale('log')\n",
    "    ax4.set_xlim(minimumValue, maximumValue)\n",
    "    ax4.set_yscale('log')\n",
    "    ax4.set_ylim(minimumValue, maximumValue)\n",
    "    ax4.set_xlabel('Training loss')\n",
    "    ax4.set_ylabel('Validation loss')\n",
    "    ax4.set_title('Validation loss vs training loss (logarithmic).')\n",
    "    \n",
    "    plt.show()\n",
    "    history['figure'] = [fig, ax1, ax2, ax3, ax4]\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makePredictionPlot(model, dataFrame, amountOfPoints=1000):\n",
    "    prediction = []\n",
    "    realValue = []\n",
    "    \n",
    "    for i in range(min(len(dataFrame['potentialEnergy']), amountOfPoints)):\n",
    "        index = dataFrame.index[i]\n",
    "        prediction.append(model.predict([[dataFrame['eigenvalues'][index]]])[0][0])\n",
    "        realValue.append(dataFrame['potentialEnergy'][index])\n",
    "    \n",
    "    fig, ((ax1)) = plt.subplots(nrows=1, ncols=1, figsize=(8, 8))\n",
    "    ax1.scatter(realValue, prediction, s=5)\n",
    "    ax1.set_xlim(min(min(prediction), min(realValue)), max(max(prediction), max(realValue)))\n",
    "    ax1.set_ylim(min(min(prediction), min(realValue)), max(max(prediction), max(realValue)))\n",
    "    ax1.set_xlabel('Real energy')\n",
    "    ax1.set_ylabel('Predicted energy')\n",
    "    ax1.set_title('Predicted energy vs real energy.')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotAndPredict(model, dataFrame, indexNumber):\n",
    "    prediction = model.predict([[dataFrame['eigenvalues'][indexNumber]]])[0][0]\n",
    "    realValue = dataFrame['potentialEnergy'][indexNumber]\n",
    "    \n",
    "    fig, ax = plotLatticeFromDataFrame(dataFrame, indexNumber)\n",
    "    print('For index', indexNumber, 'Predicted', prediction, 'was', realValue)\n",
    "    return fig, ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDenseNetwork(inputSize, settings, learningRate, kernalInitializer='he_uniform', lossFunction='MAE'):#https://keras.io/api/losses/regression_losses/\n",
    "    \"\"\"\n",
    "    jhgjhgj\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(Dense(settings[0][1], input_dim=inputSize, use_bias=settings[0][3], activation=settings[0][2]))#, kernel_initializer=kernalInitializer))\n",
    "    #model.add(Activation(settings[0][2]))\n",
    "    \n",
    "    for i in range(1, len(settings)):\n",
    "        setting = settings[i]#0 is kind of layer, rest are settings.\n",
    "        \n",
    "        model.add(Dense(setting[1], use_bias=setting[3], activation=setting[2]))#, kernel_initializer=kernalInitializer))\n",
    "        #model.add(Activation(setting[2]))\n",
    "    \n",
    "    opt = adam(lr=learningRate)\n",
    "    model.compile(loss=lossFunction, optimizer=opt)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Offset is 0\n"
     ]
    }
   ],
   "source": [
    "filename = 'databaseModLen_min_cut0.90_widths86_Width1.5-10_data50k_3-1sur_epoch30_maxDelta0.1_R020_M2M3Prepared'#'OLDcombinedDataR100M2M3M4Prepared'#'combinedDataPrepared'\n",
    "orderOfMatrix = [-2, -3]# Make a list if you want to do the calculations yourself and make a other type of you want to import it\n",
    "inputSize=4 * len(orderOfMatrix)\n",
    "\n",
    "data = pd.read_json(filename + '.json', orient='columns')\n",
    "data['particleCoordinates'] = data['particleCoordinates'].apply(np.array)\n",
    "data['eigenvalues'] = data['eigenvalues'].apply(np.array)\n",
    "#data['eigenvalues'] = data['eigenvalues'].apply(np.transpose)\n",
    "#data['eigenvalues'] = data['eigenvalues'].apply(list)\n",
    "\n",
    "offset = 0#min(data['potentialEnergy'])\n",
    "print('Offset is', offset)\n",
    "data['potentialEnergy'] = data['potentialEnergy']-offset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>particleCoordinates</th>\n",
       "      <th>widthOfCell</th>\n",
       "      <th>numberOfSurroundingCells</th>\n",
       "      <th>potentialEnergy</th>\n",
       "      <th>eigenvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[[1.4851748541, 0.45410715], [0.699539793, 0.3...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>-15.264735</td>\n",
       "      <td>[-1406.7430930758, -898.2445696454, -793.63886...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[[0.8931141095, 0.403968044], [0.0066127184, 0...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>292.884013</td>\n",
       "      <td>[-1689.6169133652, -1196.4314841118, -551.7495...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[[0.8006036438, 1.2870102185], [0.1622535191, ...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>222.036911</td>\n",
       "      <td>[-2154.1589768768, -1516.1702173065, -244.1157...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[[0.0032439773, 0.8478620114000001], [0.862247...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>65.092996</td>\n",
       "      <td>[-1970.4875095234, -1038.8554905677, -586.2448...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[[1.0603370699, 0.0158879152], [0.6553086937, ...</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3</td>\n",
       "      <td>297.883951</td>\n",
       "      <td>[-2559.1963779576, -1588.3441057448, 122.34044...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                particleCoordinates  widthOfCell  \\\n",
       "0      0  [[1.4851748541, 0.45410715], [0.699539793, 0.3...          1.5   \n",
       "1      1  [[0.8931141095, 0.403968044], [0.0066127184, 0...          1.5   \n",
       "2      2  [[0.8006036438, 1.2870102185], [0.1622535191, ...          1.5   \n",
       "3      3  [[0.0032439773, 0.8478620114000001], [0.862247...          1.5   \n",
       "4      4  [[1.0603370699, 0.0158879152], [0.6553086937, ...          1.5   \n",
       "\n",
       "   numberOfSurroundingCells  potentialEnergy  \\\n",
       "0                         3       -15.264735   \n",
       "1                         3       292.884013   \n",
       "2                         3       222.036911   \n",
       "3                         3        65.092996   \n",
       "4                         3       297.883951   \n",
       "\n",
       "                                         eigenvalues  \n",
       "0  [-1406.7430930758, -898.2445696454, -793.63886...  \n",
       "1  [-1689.6169133652, -1196.4314841118, -551.7495...  \n",
       "2  [-2154.1589768768, -1516.1702173065, -244.1157...  \n",
       "3  [-1970.4875095234, -1038.8554905677, -586.2448...  \n",
       "4  [-2559.1963779576, -1588.3441057448, 122.34044...  "
      ]
     },
     "execution_count": 367,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>widthOfCell</th>\n",
       "      <th>numberOfSurroundingCells</th>\n",
       "      <th>potentialEnergy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>31521.000000</td>\n",
       "      <td>31521.000000</td>\n",
       "      <td>31521.0</td>\n",
       "      <td>31521.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>15760.000000</td>\n",
       "      <td>4.678008</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.203249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9099.473254</td>\n",
       "      <td>2.068157</td>\n",
       "      <td>0.0</td>\n",
       "      <td>65.557457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-45.142848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>7880.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-4.392354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>15760.000000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-1.825203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>23640.000000</td>\n",
       "      <td>6.100000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>-0.800430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31520.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2665.287506</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              index   widthOfCell  numberOfSurroundingCells  potentialEnergy\n",
       "count  31521.000000  31521.000000                   31521.0     31521.000000\n",
       "mean   15760.000000      4.678008                       3.0         4.203249\n",
       "std     9099.473254      2.068157                       0.0        65.557457\n",
       "min        0.000000      1.500000                       3.0       -45.142848\n",
       "25%     7880.000000      3.000000                       3.0        -4.392354\n",
       "50%    15760.000000      4.500000                       3.0        -1.825203\n",
       "75%    23640.000000      6.100000                       3.0        -0.800430\n",
       "max    31520.000000     10.000000                       3.0      2665.287506"
      ]
     },
     "execution_count": 368,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntempData = dataFiltered.copy()\\ntempOfsett = 0#1.01 * min(tempData['potentialEnergy'])\\ntempData['potentialEnergy'] = tempData['potentialEnergy'] - tempOfsett\\nmedianData = tempData.groupby('widthOfCell').describe(percentiles=[0.05, 0.5, 0.95])\\nfig, ax = plt.subplots(figsize=(8, 8))\\nax.plot(medianData.index, medianData['potentialEnergy']['min'])\\nax.plot(medianData.index, medianData['potentialEnergy']['5%'])\\nax.plot(medianData.index, medianData['potentialEnergy']['50%'])\\nax.plot(medianData.index, medianData['potentialEnergy']['95%'])\\nplt.legend(['min', '5%', '50%', '95%'])\\nplt.ylim(-30, 30)\\nax.set_title('Plot of the energy per particle for each width of cell with offset of ' + str(tempOfsett) + '.')\\nax.set_ylabel('Energy per particle')\\nax.set_xlabel('Width of cell')\\nplt.show()\\n#\""
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "qLow = data['potentialEnergy'].quantile(0.01)\n",
    "qHi  = data['potentialEnergy'].quantile(0.99)\n",
    "dataFiltered = data.copy()\n",
    "#dataFiltered = data[(data['potentialEnergy'] < qHi) & (data['potentialEnergy'] > qLow)]\n",
    "\n",
    "x = dataFiltered['potentialEnergy']\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "sortedX = np.sort(x) - 1.01 * min(x)\n",
    "ax.plot([100*i/len(x) for i in range(0, len(x))], sortedX)\n",
    "ax.set_xlim(0, 100)\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Plot of the energy per particle for each percentile.')\n",
    "ax.set_ylabel('Energy per particle')\n",
    "ax.set_xlabel('Percentile')\n",
    "plt.show()\n",
    "\n",
    "#\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "tempData = dataFiltered.copy()\n",
    "tempOfsett = 1.01 * min(tempData['potentialEnergy'])\n",
    "tempData['potentialEnergy'] = tempData['potentialEnergy'] - tempOfsett\n",
    "medianData = tempData.groupby('widthOfCell').describe(percentiles=[0.05, 0.5, 0.95])\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(medianData.index, medianData['potentialEnergy']['5%'])\n",
    "ax.plot(medianData.index, medianData['potentialEnergy']['5%'])\n",
    "ax.plot(medianData.index, medianData['potentialEnergy']['50%'])\n",
    "ax.plot(medianData.index, medianData['potentialEnergy']['95%'])\n",
    "plt.legend(['5%', '50%', '95%'])\n",
    "ax.set_yscale('log')\n",
    "ax.set_title('Plot of the energy per particle for each width of cell with offset of ' + str(tempOfsett) + '.')\n",
    "ax.set_ylabel('Energy per particle')\n",
    "ax.set_xlabel('Width of cell')\n",
    "plt.show()\n",
    "#\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "tempData = dataFiltered.copy()\n",
    "tempOfsett = 0#1.01 * min(tempData['potentialEnergy'])\n",
    "tempData['potentialEnergy'] = tempData['potentialEnergy'] - tempOfsett\n",
    "medianData = tempData.groupby('widthOfCell').describe(percentiles=[0.05, 0.5, 0.95])\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(medianData.index, medianData['potentialEnergy']['min'])\n",
    "ax.plot(medianData.index, medianData['potentialEnergy']['5%'])\n",
    "ax.plot(medianData.index, medianData['potentialEnergy']['50%'])\n",
    "ax.plot(medianData.index, medianData['potentialEnergy']['95%'])\n",
    "plt.legend(['min', '5%', '50%', '95%'])\n",
    "plt.ylim(-30, 30)\n",
    "ax.set_title('Plot of the energy per particle for each width of cell with offset of ' + str(tempOfsett) + '.')\n",
    "ax.set_ylabel('Energy per particle')\n",
    "ax.set_xlabel('Width of cell')\n",
    "plt.show()\n",
    "#\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\ntrainingData={'potentialEnergy':[]}\\n\\nfor i in range(len(data['eigenvalues'][data.index[0]])):\\n    trainingData['eigenValue'+str(i)]=[]\\n    \\nfor i in data.index:\\n    #trainingData['potentialEnergy'].append(data['potentialEnergy'][i])\\n    trainingData['potentialEnergy'].append(sum(data['eigenvalues'][i]))\\n    #print(sum(data['eigenvalues'][i]))\\n    for j in range(len(data['eigenvalues'][i])):\\n        trainingData['eigenValue'+str(j)].append(data['eigenvalues'][i][j])\\n\\ntrainingData = pd.DataFrame(trainingData)\\n\\n\\n#trainingData = data.copy()#data.sample(frac=0.8)\\n#validationData = data.copy()#data.drop(trainingData.index)\\n\\n#trainingData['potentialEnergy'] = trainingData['eigenvalues'].apply(sum)\\n\\ny = trainingData.iloc[:, 0:1].values\\nX = trainingData.iloc[:, 1:len(data['eigenvalues'][data.index[0]])+1].values\\n\\nprint(X)\\nprint(y)\\n#print(X[0])\\n#print(y[0][0])\\n\\nprint(np.shape(X))\\nprint(np.shape(y))\\ntrainingData.head()\""
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "trainingData={'potentialEnergy':[]}\n",
    "\n",
    "for i in range(len(data['eigenvalues'][data.index[0]])):\n",
    "    trainingData['eigenValue'+str(i)]=[]\n",
    "    \n",
    "for i in data.index:\n",
    "    #trainingData['potentialEnergy'].append(data['potentialEnergy'][i])\n",
    "    trainingData['potentialEnergy'].append(sum(data['eigenvalues'][i]))\n",
    "    #print(sum(data['eigenvalues'][i]))\n",
    "    for j in range(len(data['eigenvalues'][i])):\n",
    "        trainingData['eigenValue'+str(j)].append(data['eigenvalues'][i][j])\n",
    "\n",
    "trainingData = pd.DataFrame(trainingData)\n",
    "\n",
    "\n",
    "#trainingData = data.copy()#data.sample(frac=0.8)\n",
    "#validationData = data.copy()#data.drop(trainingData.index)\n",
    "\n",
    "#trainingData['potentialEnergy'] = trainingData['eigenvalues'].apply(sum)\n",
    "\n",
    "y = trainingData.iloc[:, 0:1].values\n",
    "X = trainingData.iloc[:, 1:len(data['eigenvalues'][data.index[0]])+1].values\n",
    "\n",
    "print(X)\n",
    "print(y)\n",
    "#print(X[0])\n",
    "#print(y[0][0])\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))\n",
    "trainingData.head()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 71995.67001441  55590.34728149  32692.69301606  62456.21465719\n",
      "  98470.52847457 -65619.63134526 -62881.4731159  -46221.80541274]\n",
      "146482.5435698321\n",
      "(500, 8)\n",
      "(500,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#\"\"\"\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(500):#data.index:\n",
    "    randomNumbers = [random.uniform(-10**5, 10**5) for j in range(0,8)]\n",
    "    \n",
    "    X.append(randomNumbers)\n",
    "    y.append(sum(randomNumbers))\n",
    "    \n",
    "    #X.append(np.array(data['eigenvalues'][i]))\n",
    "    #y.append(data['potentialEnergy'][i])\n",
    "    #y.append(sum(data['eigenvalues'][i]))\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "print(X[0])\n",
    "print(y[0])\n",
    "#print(X[15000])\n",
    "#print(y[15000])\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results training input:\n",
      "[-9.74926246e+02 -1.71138840e+02  1.02664056e+01  1.32161675e+03\n",
      " -2.84247741e+04 -2.13529606e+03  1.93604202e+02  3.07744081e+04] -1.3987582533 \n",
      "\n",
      "[-9.02874894e+04 -1.70557804e+02  3.58596326e+02  9.04824895e+04\n",
      " -2.71294665e+07 -2.13372740e+03  2.65541581e+03  2.71299903e+07] -2.3088431453 \n",
      "\n",
      "[-8.04708929e+03 -5.85025108e+01  9.97511245e+01  8.09242459e+03\n",
      " -7.21865214e+05 -4.20263297e+02  4.94802032e+02  7.21940895e+05] -0.0027627558000000003 \n",
      "\n",
      "(25217, 8)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(25217,)\n",
      "<class 'numpy.ndarray'>\n",
      "Results validation input:\n",
      "[ -1985.80657979  -1215.76413536   -639.71208463  13909.87413813\n",
      " -82608.12701604 -37147.81602701 -11231.23677156 202965.39601243] 504.1268681175 \n",
      "\n",
      "[  -4338.46402169   -3622.94667359    2200.26464445   15829.73738918\n",
      " -280008.63450764 -216580.73424207  197607.09958967  370960.48535786] 296.0818227826 \n",
      "\n",
      "[ -1369.40691204  -1011.62192644   -726.98141366  13176.6015905\n",
      " -43076.6745508  -28014.77595935 -15094.13386425 158163.80057222] -1.6617118397000001 \n",
      "\n",
      "(6304, 8)\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(6304,)\n",
      "<class 'numpy.ndarray'>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "trainingData = data.sample(frac=0.8)\n",
    "validationData = data.drop(trainingData.index)\n",
    "\n",
    "inputData = []\n",
    "targetData = []\n",
    "for i in trainingData.index:\n",
    "    inputData.append(trainingData['eigenvalues'][i])#TODO remove sorted once fixed in data\n",
    "    #targetData.append(trainingData['potentialEnergy'][i])\n",
    "    targetData.append(sum(trainingData['eigenvalues'][i]))\n",
    "\n",
    "trainingInput = [[inputData], [targetData]]\n",
    "\n",
    "inputData = []\n",
    "targetData = []\n",
    "for i in validationData.index:\n",
    "    inputData.append(validationData['eigenvalues'][i])#TODO remove sorted once fixed in data\n",
    "    targetData.append(validationData['potentialEnergy'][i])\n",
    "\n",
    "validationInput = [[inputData], [targetData]]\n",
    "\"\"\"\n",
    "#\"\"\"\n",
    "trainingData = data.sample(frac=0.8)\n",
    "validationData = data.drop(trainingData.index)\n",
    "\n",
    "inputData = []\n",
    "targetData = []\n",
    "for i in trainingData.index:\n",
    "    inputData.append(np.array(trainingData['eigenvalues'][i]))\n",
    "    targetData.append(trainingData['potentialEnergy'][i])\n",
    "    #targetData.append(sum(trainingData['eigenvalues'][i]))\n",
    "\n",
    "trainingInput = {'input':np.array(inputData), 'target':np.array(targetData)}\n",
    "\n",
    "print('Results training input:')\n",
    "[print(trainingInput['input'][i], trainingInput['target'][i], '\\n') for i in range(0, 3)]\n",
    "print(np.shape(trainingInput['input']))\n",
    "print(type(trainingInput['input']))\n",
    "print(type(trainingInput['input'][0]))\n",
    "print(np.shape(trainingInput['target']))\n",
    "print(type(trainingInput['target']))\n",
    "\n",
    "inputData = []\n",
    "targetData = []\n",
    "for i in validationData.index:\n",
    "    inputData.append(np.array(validationData['eigenvalues'][i]))\n",
    "    targetData.append(validationData['potentialEnergy'][i])\n",
    "    #targetData.append([sum(validationData['eigenvalues'][i])])\n",
    "\n",
    "validationInput = {'input':np.array(inputData), 'target':np.array(targetData)}\n",
    "\n",
    "print('Results validation input:')\n",
    "[print(validationInput['input'][i], validationInput['target'][i], '\\n') for i in range(0, 3)]\n",
    "print(np.shape(validationInput['input']))\n",
    "print(type(validationInput['input']))\n",
    "print(type(validationInput['input'][0]))\n",
    "print(np.shape(validationInput['target']))\n",
    "print(type(validationInput['target']))\n",
    "\n",
    "validationPlots = [random.choice(validationData.index) for i in range(0, 11)]\n",
    "\n",
    "#\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "widthOfCell = 5.0\n",
    "numberOfSurroundingCells = 3\n",
    "R0 = 20\n",
    "x = np.linspace(0.1, 1.1, 60)\n",
    "\n",
    "testData = {'particleCoordinates': [],'widthOfCell':[] , 'numberOfSurroundingCells': [], 'potentialEnergy':[]}# Initialy use a dictionary because it's easier to append to than a dataframe.\n",
    "for i in x:\n",
    "    particles = [np.array([0.1, 0.1]), np.array([0.55, 0.9]), np.array([0.6, 0.665]), np.array([0.5, i])]\n",
    "    particles = [widthOfCell * coordinate for coordinate in particles]\n",
    "    otherSpace = generateSpace(particles, numberOfSurroundingCells, widthOfCell)# Generate the other space.\n",
    "    testData['particleCoordinates'].append(particles)\n",
    "    testData['widthOfCell'].append(widthOfCell)\n",
    "    testData['numberOfSurroundingCells'].append(numberOfSurroundingCells)\n",
    "    testData['potentialEnergy'].append(potentialEnergy(otherSpace, particles, potentialEnergyPerTrio) - offset)# Calculate the potential energy and save it to the data dictonairy.\n",
    "\n",
    "x = widthOfCell * x\n",
    "testData = prepareDatabseForMachineLearning(pd.DataFrame(testData), orderOfMatrix, R0, giveUpdates = False)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "ax1.plot(list(x), [testData['eigenvalues'][i] for i in range(len(testData['eigenvalues']))])\n",
    "ax1.legend()\n",
    "ax1.set_ylim(-1000, 1000)\n",
    "ax1.set_xlabel('Position of atom')\n",
    "ax1.set_ylabel('Eigenvalues')\n",
    "ax1.set_title('Eigenvalues for different positions.')\n",
    "plt.show()\n",
    "\n",
    "testData.head()\n",
    "testData.describe()\n",
    "\n",
    "y = []\n",
    "for i in range(len(x)):\n",
    "    y.append(testData['potentialEnergy'][i])\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "ax1.plot(list(x), y, label='Real potential energy')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Position of atom')\n",
    "ax1.set_ylabel('Potential energy')\n",
    "ax1.set_title('Predicted potential energy and real potential energy for different positions.')\n",
    "#ax1.set_ylim(-0.75 * offset , 1.5 * max(y[1]))\n",
    "plt.show()\n",
    "#\"\"\"\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "prefixName = 0\n",
    "saveFolder = 'testRun'\n",
    "lossMetric = 'MAE'#keras.losses.MeanAbsoluteError()#keras.losses.MeanSquaredError()\n",
    "#trainNetwork(model, trainingInput, 0.3, numberOfEpochs=100, batchSize=32, verboseSetting=1)\n",
    "#fitDetails = network.fit(trainingData[0], trainingData[1], shuffle=True, batch_size=batchSize, validation_split=validationSplit, epochs=numberOfEpochs, verbose=max(0,verboseSetting))\n",
    "\n",
    "def RSMPE(target,value):\n",
    "    return ((keras.backend.mean(((value-target)/target)**2))**0.5)*100\n",
    "\n",
    "def MAE(target,value):\n",
    "    return keras.backend.mean(abs(value-target))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 0: Linear network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_38 (Dense)             (None, 1)                 8         \n",
      "=================================================================\n",
      "Total params: 8\n",
      "Trainable params: 8\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Layer 0:  [array([[-0.11831999],\n",
      "       [-0.139961  ],\n",
      "       [ 0.19290996],\n",
      "       [ 0.68787444],\n",
      "       [-0.6723417 ],\n",
      "       [ 0.06833392],\n",
      "       [-0.47423705],\n",
      "       [-0.46447757]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "name = 'LinearNetwork'\n",
    "settings = [['Dense', 1, 'linear', False]]\n",
    "\n",
    "\n",
    "learningRate = 1*10**-4\n",
    "\n",
    "#model = makeDenseNetwork(inputSize, settings, learningRate, kernalInitializer='he_uniform')#, lossFunction=lossMetric)#eras.initializers.glorot_uniform(seed=None)\n",
    "model = Sequential()\n",
    "opt = adam(lr=learningRate)\n",
    "model.add(Dense(1, activation=\"linear\", use_bias=False, input_dim=8))\n",
    "model.compile(loss=MAE, optimizer=opt)\n",
    "\n",
    "model.summary()\n",
    "for i in range(len(model.layers)):\n",
    "    print('Layer ' + str(i) + ': ', model.layers[i].get_weights())\n",
    "    #print(model.evaluate(validationInput['input'], validationInput['target'], verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 350 samples, validate on 150 samples\n",
      "Epoch 1/10000\n",
      "350/350 [==============================] - 0s 202us/step - loss: 145746.2990 - val_loss: 162052.9831\n",
      "Epoch 2/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 145633.2571 - val_loss: 161930.7122\n",
      "Epoch 3/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 145520.5367 - val_loss: 161811.3911\n",
      "Epoch 4/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 145411.1937 - val_loss: 161690.2515\n",
      "Epoch 5/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 145302.1023 - val_loss: 161569.4701\n",
      "Epoch 6/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 145192.4886 - val_loss: 161450.4117\n",
      "Epoch 7/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 145081.5777 - val_loss: 161332.1102\n",
      "Epoch 8/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 144972.3376 - val_loss: 161213.9513\n",
      "Epoch 9/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 144864.0272 - val_loss: 161093.2539\n",
      "Epoch 10/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 144752.8961 - val_loss: 160975.9247\n",
      "Epoch 11/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 144644.4604 - val_loss: 160856.4268\n",
      "Epoch 12/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 144535.7287 - val_loss: 160736.5683\n",
      "Epoch 13/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 144425.8869 - val_loss: 160616.2434\n",
      "Epoch 14/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 144315.8038 - val_loss: 160497.9731\n",
      "Epoch 15/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 144206.7562 - val_loss: 160377.4633\n",
      "Epoch 16/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 144097.8974 - val_loss: 160257.0578\n",
      "Epoch 17/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 143987.1225 - val_loss: 160137.2720\n",
      "Epoch 18/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 143879.2700 - val_loss: 160016.2178\n",
      "Epoch 19/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 143767.3123 - val_loss: 159897.4246\n",
      "Epoch 20/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 143658.4968 - val_loss: 159778.4205\n",
      "Epoch 21/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 143549.7700 - val_loss: 159658.3569\n",
      "Epoch 22/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 143440.5047 - val_loss: 159537.9931\n",
      "Epoch 23/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 143329.6488 - val_loss: 159417.3966\n",
      "Epoch 24/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 143220.6217 - val_loss: 159297.9981\n",
      "Epoch 25/10000\n",
      "350/350 [==============================] - 0s 93us/step - loss: 143112.0221 - val_loss: 159178.1539\n",
      "Epoch 26/10000\n",
      "350/350 [==============================] - 0s 103us/step - loss: 143001.7617 - val_loss: 159058.3785\n",
      "Epoch 27/10000\n",
      "350/350 [==============================] - 0s 103us/step - loss: 142892.9708 - val_loss: 158939.1300\n",
      "Epoch 28/10000\n",
      "350/350 [==============================] - 0s 95us/step - loss: 142783.3376 - val_loss: 158820.2453\n",
      "Epoch 29/10000\n",
      "350/350 [==============================] - 0s 94us/step - loss: 142675.3098 - val_loss: 158700.1698\n",
      "Epoch 30/10000\n",
      "350/350 [==============================] - 0s 88us/step - loss: 142565.1116 - val_loss: 158580.7349\n",
      "Epoch 31/10000\n",
      "350/350 [==============================] - 0s 86us/step - loss: 142457.8698 - val_loss: 158459.7255\n",
      "Epoch 32/10000\n",
      "350/350 [==============================] - 0s 88us/step - loss: 142347.3688 - val_loss: 158342.4413\n",
      "Epoch 33/10000\n",
      "350/350 [==============================] - 0s 90us/step - loss: 142239.6921 - val_loss: 158222.8028\n",
      "Epoch 34/10000\n",
      "350/350 [==============================] - 0s 80us/step - loss: 142131.6498 - val_loss: 158103.1264\n",
      "Epoch 35/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 142023.2383 - val_loss: 157982.7801\n",
      "Epoch 36/10000\n",
      "350/350 [==============================] - 0s 83us/step - loss: 141915.0905 - val_loss: 157861.8120\n",
      "Epoch 37/10000\n",
      "350/350 [==============================] - 0s 83us/step - loss: 141803.3648 - val_loss: 157744.4868\n",
      "Epoch 38/10000\n",
      "350/350 [==============================] - 0s 86us/step - loss: 141697.3400 - val_loss: 157622.6154\n",
      "Epoch 39/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 141585.9998 - val_loss: 157505.2701\n",
      "Epoch 40/10000\n",
      "350/350 [==============================] - 0s 78us/step - loss: 141479.1695 - val_loss: 157384.3219\n",
      "Epoch 41/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 141371.6037 - val_loss: 157263.0546\n",
      "Epoch 42/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 141259.6281 - val_loss: 157145.2197\n",
      "Epoch 43/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 141152.1215 - val_loss: 157025.2021\n",
      "Epoch 44/10000\n",
      "350/350 [==============================] - 0s 83us/step - loss: 141044.0183 - val_loss: 156903.7970\n",
      "Epoch 45/10000\n",
      "350/350 [==============================] - 0s 81us/step - loss: 140934.7434 - val_loss: 156783.2339\n",
      "Epoch 46/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 140825.4777 - val_loss: 156662.3228\n",
      "Epoch 47/10000\n",
      "350/350 [==============================] - 0s 83us/step - loss: 140715.8871 - val_loss: 156542.9011\n",
      "Epoch 48/10000\n",
      "350/350 [==============================] - 0s 86us/step - loss: 140608.0539 - val_loss: 156421.4839\n",
      "Epoch 49/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 140498.8550 - val_loss: 156301.3734\n",
      "Epoch 50/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 140388.6871 - val_loss: 156181.7489\n",
      "Epoch 51/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 140279.1197 - val_loss: 156062.1021\n",
      "Epoch 52/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 140170.2843 - val_loss: 155942.8769\n",
      "Epoch 53/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 140063.2634 - val_loss: 155821.4468\n",
      "Epoch 54/10000\n",
      "350/350 [==============================] - 0s 80us/step - loss: 139952.8154 - val_loss: 155702.3661\n",
      "Epoch 55/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 139842.2082 - val_loss: 155584.4465\n",
      "Epoch 56/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 139735.9694 - val_loss: 155463.3471\n",
      "Epoch 57/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 139625.0738 - val_loss: 155344.6032\n",
      "Epoch 58/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 139516.7698 - val_loss: 155223.6502\n",
      "Epoch 59/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 139406.9915 - val_loss: 155104.4403\n",
      "Epoch 60/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 139297.9412 - val_loss: 154984.7539\n",
      "Epoch 61/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 139187.6896 - val_loss: 154866.8132\n",
      "Epoch 62/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 139079.2507 - val_loss: 154747.4836\n",
      "Epoch 63/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 138970.7455 - val_loss: 154627.2218\n",
      "Epoch 64/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 138859.7123 - val_loss: 154508.3367\n",
      "Epoch 65/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 138754.1653 - val_loss: 154386.6071\n",
      "Epoch 66/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 138641.9025 - val_loss: 154267.0481\n",
      "Epoch 67/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 138532.9990 - val_loss: 154147.2484\n",
      "Epoch 68/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 138422.9751 - val_loss: 154029.2103\n",
      "Epoch 69/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 138313.8163 - val_loss: 153909.8532\n",
      "Epoch 70/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 138206.6052 - val_loss: 153789.2953\n",
      "Epoch 71/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 138095.6204 - val_loss: 153669.6486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 137986.1867 - val_loss: 153550.1568\n",
      "Epoch 73/10000\n",
      "350/350 [==============================] - 0s 84us/step - loss: 137878.1505 - val_loss: 153430.0847\n",
      "Epoch 74/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 137766.3376 - val_loss: 153311.7384\n",
      "Epoch 75/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 137657.8385 - val_loss: 153192.4103\n",
      "Epoch 76/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 137550.0092 - val_loss: 153071.9351\n",
      "Epoch 77/10000\n",
      "350/350 [==============================] - 0s 80us/step - loss: 137438.7015 - val_loss: 152952.5333\n",
      "Epoch 78/10000\n",
      "350/350 [==============================] - 0s 80us/step - loss: 137330.9882 - val_loss: 152831.1221\n",
      "Epoch 79/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 137218.9498 - val_loss: 152713.4822\n",
      "Epoch 80/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 137111.5924 - val_loss: 152592.8248\n",
      "Epoch 81/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 137001.5158 - val_loss: 152472.2563\n",
      "Epoch 82/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 136891.0952 - val_loss: 152352.8213\n",
      "Epoch 83/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 136782.9162 - val_loss: 152232.7535\n",
      "Epoch 84/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 136671.6603 - val_loss: 152114.7696\n",
      "Epoch 85/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 136563.8510 - val_loss: 151993.9095\n",
      "Epoch 86/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 136452.6877 - val_loss: 151875.2679\n",
      "Epoch 87/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 136344.3223 - val_loss: 151754.9915\n",
      "Epoch 88/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 136234.9990 - val_loss: 151635.1053\n",
      "Epoch 89/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 136125.9153 - val_loss: 151515.1165\n",
      "Epoch 90/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 136015.0587 - val_loss: 151396.2081\n",
      "Epoch 91/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 135905.7597 - val_loss: 151276.6465\n",
      "Epoch 92/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 135798.3003 - val_loss: 151156.9101\n",
      "Epoch 93/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 135690.2232 - val_loss: 151036.0797\n",
      "Epoch 94/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 135577.1086 - val_loss: 150917.4234\n",
      "Epoch 95/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 135467.7383 - val_loss: 150798.7701\n",
      "Epoch 96/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 135360.0474 - val_loss: 150678.1116\n",
      "Epoch 97/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 135250.3270 - val_loss: 150559.4967\n",
      "Epoch 98/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 135142.6820 - val_loss: 150438.9895\n",
      "Epoch 99/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 135033.1679 - val_loss: 150319.2365\n",
      "Epoch 100/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 134922.3756 - val_loss: 150202.3596\n",
      "Epoch 101/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 134815.2668 - val_loss: 150084.5080\n",
      "Epoch 102/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 134708.2499 - val_loss: 149963.7199\n",
      "Epoch 103/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 134599.2763 - val_loss: 149844.7533\n",
      "Epoch 104/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 134489.6403 - val_loss: 149726.1836\n",
      "Epoch 105/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 134382.9596 - val_loss: 149607.7196\n",
      "Epoch 106/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 134274.1931 - val_loss: 149488.3929\n",
      "Epoch 107/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 134166.9429 - val_loss: 149368.5634\n",
      "Epoch 108/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 134057.8359 - val_loss: 149249.0046\n",
      "Epoch 109/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 133948.7338 - val_loss: 149131.3915\n",
      "Epoch 110/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 133841.6217 - val_loss: 149012.2661\n",
      "Epoch 111/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 133734.1955 - val_loss: 148890.2932\n",
      "Epoch 112/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 133624.8493 - val_loss: 148771.7264\n",
      "Epoch 113/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 133514.1438 - val_loss: 148655.2721\n",
      "Epoch 114/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 133409.5088 - val_loss: 148534.0136\n",
      "Epoch 115/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 133300.5235 - val_loss: 148414.5599\n",
      "Epoch 116/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 133192.4862 - val_loss: 148295.3981\n",
      "Epoch 117/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 133084.9284 - val_loss: 148175.3504\n",
      "Epoch 118/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 132974.3503 - val_loss: 148058.1267\n",
      "Epoch 119/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 132866.7971 - val_loss: 147940.9829\n",
      "Epoch 120/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 132759.2489 - val_loss: 147821.1462\n",
      "Epoch 121/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 132651.0024 - val_loss: 147702.6166\n",
      "Epoch 122/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 132544.3216 - val_loss: 147582.7734\n",
      "Epoch 123/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 132434.8022 - val_loss: 147463.4736\n",
      "Epoch 124/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 132327.1797 - val_loss: 147344.9515\n",
      "Epoch 125/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 132219.0989 - val_loss: 147225.5512\n",
      "Epoch 126/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 132109.8008 - val_loss: 147107.9969\n",
      "Epoch 127/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 132001.9834 - val_loss: 146990.0583\n",
      "Epoch 128/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 131895.7380 - val_loss: 146871.1166\n",
      "Epoch 129/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 131787.5602 - val_loss: 146751.2097\n",
      "Epoch 130/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 131680.6104 - val_loss: 146631.5214\n",
      "Epoch 131/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 131571.2487 - val_loss: 146514.8750\n",
      "Epoch 132/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 131466.0430 - val_loss: 146395.3271\n",
      "Epoch 133/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 131359.1892 - val_loss: 146276.0553\n",
      "Epoch 134/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 131249.3297 - val_loss: 146159.3800\n",
      "Epoch 135/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 131142.3092 - val_loss: 146041.9330\n",
      "Epoch 136/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 131035.9692 - val_loss: 145923.2782\n",
      "Epoch 137/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 130929.4647 - val_loss: 145804.7238\n",
      "Epoch 138/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 130821.4809 - val_loss: 145686.2168\n",
      "Epoch 139/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 130712.4451 - val_loss: 145570.1071\n",
      "Epoch 140/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 130607.7969 - val_loss: 145451.5549\n",
      "Epoch 141/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 130500.4017 - val_loss: 145332.2998\n",
      "Epoch 142/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 130391.7245 - val_loss: 145214.3603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 143/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 130286.9213 - val_loss: 145093.7336\n",
      "Epoch 144/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 130176.7809 - val_loss: 144976.8779\n",
      "Epoch 145/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 130071.6312 - val_loss: 144858.0922\n",
      "Epoch 146/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 129962.0866 - val_loss: 144741.7997\n",
      "Epoch 147/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 129854.3997 - val_loss: 144625.0096\n",
      "Epoch 148/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 129749.4434 - val_loss: 144508.3835\n",
      "Epoch 149/10000\n",
      "350/350 [==============================] - 0s 78us/step - loss: 129639.4461 - val_loss: 144392.1867\n",
      "Epoch 150/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 129534.6555 - val_loss: 144274.8182\n",
      "Epoch 151/10000\n",
      "350/350 [==============================] - 0s 78us/step - loss: 129428.1384 - val_loss: 144153.8730\n",
      "Epoch 152/10000\n",
      "350/350 [==============================] - 0s 85us/step - loss: 129319.2222 - val_loss: 144036.5229\n",
      "Epoch 153/10000\n",
      "350/350 [==============================] - 0s 83us/step - loss: 129210.4125 - val_loss: 143921.4205\n",
      "Epoch 154/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 129102.4847 - val_loss: 143806.0430\n",
      "Epoch 155/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 128997.0098 - val_loss: 143687.5895\n",
      "Epoch 156/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 128889.8210 - val_loss: 143569.1011\n",
      "Epoch 157/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 128783.5336 - val_loss: 143449.3033\n",
      "Epoch 158/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 128676.4722 - val_loss: 143329.9519\n",
      "Epoch 159/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 128567.3753 - val_loss: 143212.7279\n",
      "Epoch 160/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 128460.1176 - val_loss: 143096.9132\n",
      "Epoch 161/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 128353.3566 - val_loss: 142979.5900\n",
      "Epoch 162/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 128246.7138 - val_loss: 142863.5422\n",
      "Epoch 163/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 128143.3759 - val_loss: 142745.5399\n",
      "Epoch 164/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 128035.5372 - val_loss: 142629.3902\n",
      "Epoch 165/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 127929.9931 - val_loss: 142512.0750\n",
      "Epoch 166/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 127823.6626 - val_loss: 142396.0114\n",
      "Epoch 167/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 127719.6894 - val_loss: 142277.6061\n",
      "Epoch 168/10000\n",
      "350/350 [==============================] - 0s 81us/step - loss: 127611.4623 - val_loss: 142162.1505\n",
      "Epoch 169/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 127509.4906 - val_loss: 142043.6199\n",
      "Epoch 170/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 127400.6071 - val_loss: 141928.3365\n",
      "Epoch 171/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 127295.4238 - val_loss: 141811.8733\n",
      "Epoch 172/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 127190.2901 - val_loss: 141695.6655\n",
      "Epoch 173/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 127085.5279 - val_loss: 141577.7132\n",
      "Epoch 174/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 126979.3711 - val_loss: 141461.4601\n",
      "Epoch 175/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 126874.5352 - val_loss: 141343.5464\n",
      "Epoch 176/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 126766.3109 - val_loss: 141227.3986\n",
      "Epoch 177/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 126663.8557 - val_loss: 141109.6178\n",
      "Epoch 178/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 126556.7121 - val_loss: 140992.7886\n",
      "Epoch 179/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 126449.4715 - val_loss: 140878.9039\n",
      "Epoch 180/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 126345.8078 - val_loss: 140761.4536\n",
      "Epoch 181/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 126240.4874 - val_loss: 140643.2739\n",
      "Epoch 182/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 126133.9941 - val_loss: 140527.1169\n",
      "Epoch 183/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 126029.7756 - val_loss: 140409.7467\n",
      "Epoch 184/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 125923.0048 - val_loss: 140293.3018\n",
      "Epoch 185/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 125817.9963 - val_loss: 140176.8998\n",
      "Epoch 186/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 125713.5856 - val_loss: 140058.8398\n",
      "Epoch 187/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 125606.4389 - val_loss: 139943.6998\n",
      "Epoch 188/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 125501.4833 - val_loss: 139826.6786\n",
      "Epoch 189/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 125396.8529 - val_loss: 139708.9561\n",
      "Epoch 190/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 125289.9013 - val_loss: 139593.0088\n",
      "Epoch 191/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 125184.7639 - val_loss: 139477.4878\n",
      "Epoch 192/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 125079.3452 - val_loss: 139361.2016\n",
      "Epoch 193/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 124973.7596 - val_loss: 139245.2299\n",
      "Epoch 194/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 124869.9603 - val_loss: 139126.2899\n",
      "Epoch 195/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 124760.6148 - val_loss: 139012.5181\n",
      "Epoch 196/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 124659.0627 - val_loss: 138895.0454\n",
      "Epoch 197/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 124551.5528 - val_loss: 138778.0870\n",
      "Epoch 198/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 124447.6627 - val_loss: 138661.0281\n",
      "Epoch 199/10000\n",
      "350/350 [==============================] - 0s 86us/step - loss: 124341.0792 - val_loss: 138543.8055\n",
      "Epoch 200/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 124235.6237 - val_loss: 138428.1503\n",
      "Epoch 201/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 124129.1009 - val_loss: 138311.9951\n",
      "Epoch 202/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 124025.3692 - val_loss: 138192.9700\n",
      "Epoch 203/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 123918.1215 - val_loss: 138077.3167\n",
      "Epoch 204/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 123814.9445 - val_loss: 137960.6301\n",
      "Epoch 205/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 123708.0650 - val_loss: 137844.7068\n",
      "Epoch 206/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 123602.1656 - val_loss: 137729.4164\n",
      "Epoch 207/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 123497.3019 - val_loss: 137612.6853\n",
      "Epoch 208/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 123391.2619 - val_loss: 137496.8220\n",
      "Epoch 209/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 123287.5877 - val_loss: 137378.2830\n",
      "Epoch 210/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 123179.2378 - val_loss: 137264.7472\n",
      "Epoch 211/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 123076.3534 - val_loss: 137147.9665\n",
      "Epoch 212/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 122969.8966 - val_loss: 137032.0720\n",
      "Epoch 213/10000\n",
      "350/350 [==============================] - 0s 120us/step - loss: 122867.4446 - val_loss: 136913.0064\n",
      "Epoch 214/10000\n",
      "350/350 [==============================] - 0s 117us/step - loss: 122759.0388 - val_loss: 136797.4416\n",
      "Epoch 215/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 122654.9998 - val_loss: 136680.0421\n",
      "Epoch 216/10000\n",
      "350/350 [==============================] - 0s 106us/step - loss: 122548.7034 - val_loss: 136563.3178\n",
      "Epoch 217/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 122443.6984 - val_loss: 136447.7487\n",
      "Epoch 218/10000\n",
      "350/350 [==============================] - 0s 78us/step - loss: 122339.6253 - val_loss: 136329.6797\n",
      "Epoch 219/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 122232.0890 - val_loss: 136214.7698\n",
      "Epoch 220/10000\n",
      "350/350 [==============================] - 0s 80us/step - loss: 122128.0275 - val_loss: 136097.0666\n",
      "Epoch 221/10000\n",
      "350/350 [==============================] - 0s 80us/step - loss: 122021.9700 - val_loss: 135982.4695\n",
      "Epoch 222/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 121917.8842 - val_loss: 135864.1731\n",
      "Epoch 223/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 121810.8920 - val_loss: 135749.1100\n",
      "Epoch 224/10000\n",
      "350/350 [==============================] - 0s 85us/step - loss: 121706.5698 - val_loss: 135632.9530\n",
      "Epoch 225/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 121599.4939 - val_loss: 135518.1933\n",
      "Epoch 226/10000\n",
      "350/350 [==============================] - 0s 78us/step - loss: 121495.4509 - val_loss: 135401.6617\n",
      "Epoch 227/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 121390.6864 - val_loss: 135284.9472\n",
      "Epoch 228/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 121285.4862 - val_loss: 135167.0472\n",
      "Epoch 229/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 121179.0765 - val_loss: 135051.2705\n",
      "Epoch 230/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 121073.3936 - val_loss: 134934.4382\n",
      "Epoch 231/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 120967.3498 - val_loss: 134818.5231\n",
      "Epoch 232/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 120863.2740 - val_loss: 134701.5449\n",
      "Epoch 233/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 120757.7622 - val_loss: 134585.0669\n",
      "Epoch 234/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 120651.0621 - val_loss: 134469.0802\n",
      "Epoch 235/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 120545.0222 - val_loss: 134353.3866\n",
      "Epoch 236/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 120440.4844 - val_loss: 134237.0338\n",
      "Epoch 237/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 120336.4859 - val_loss: 134120.1035\n",
      "Epoch 238/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 120229.7043 - val_loss: 134003.1302\n",
      "Epoch 239/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 120122.5275 - val_loss: 133887.7334\n",
      "Epoch 240/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 120020.1209 - val_loss: 133769.1972\n",
      "Epoch 241/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 119915.5038 - val_loss: 133651.3121\n",
      "Epoch 242/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 119806.6253 - val_loss: 133535.9028\n",
      "Epoch 243/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 119701.2271 - val_loss: 133421.2231\n",
      "Epoch 244/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 119597.3960 - val_loss: 133304.3555\n",
      "Epoch 245/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 119491.3396 - val_loss: 133187.2213\n",
      "Epoch 246/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 119383.7394 - val_loss: 133074.4401\n",
      "Epoch 247/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 119281.0309 - val_loss: 132957.2655\n",
      "Epoch 248/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 119175.0570 - val_loss: 132840.0361\n",
      "Epoch 249/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 119070.5209 - val_loss: 132722.0335\n",
      "Epoch 250/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 118962.5378 - val_loss: 132607.1135\n",
      "Epoch 251/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 118859.9939 - val_loss: 132489.4382\n",
      "Epoch 252/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 118753.5172 - val_loss: 132372.4662\n",
      "Epoch 253/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 118648.6437 - val_loss: 132254.9713\n",
      "Epoch 254/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 118542.2984 - val_loss: 132139.3847\n",
      "Epoch 255/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 118437.0846 - val_loss: 132024.3801\n",
      "Epoch 256/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 118332.0149 - val_loss: 131907.8932\n",
      "Epoch 257/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 118226.1876 - val_loss: 131792.0167\n",
      "Epoch 258/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 118121.2054 - val_loss: 131676.0281\n",
      "Epoch 259/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 118018.3144 - val_loss: 131559.2482\n",
      "Epoch 260/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 117910.2988 - val_loss: 131446.7511\n",
      "Epoch 261/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 117809.7064 - val_loss: 131328.3614\n",
      "Epoch 262/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 117702.8127 - val_loss: 131213.5512\n",
      "Epoch 263/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 117600.1809 - val_loss: 131097.3278\n",
      "Epoch 264/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 117493.9557 - val_loss: 130981.6118\n",
      "Epoch 265/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 117390.2764 - val_loss: 130865.8746\n",
      "Epoch 266/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 117286.5161 - val_loss: 130749.6099\n",
      "Epoch 267/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 117180.9751 - val_loss: 130633.7964\n",
      "Epoch 268/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 117075.9680 - val_loss: 130518.1654\n",
      "Epoch 269/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 116974.1481 - val_loss: 130400.5254\n",
      "Epoch 270/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 116867.6264 - val_loss: 130285.1697\n",
      "Epoch 271/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 116763.3608 - val_loss: 130170.2432\n",
      "Epoch 272/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 116657.5486 - val_loss: 130055.0268\n",
      "Epoch 273/10000\n",
      "350/350 [==============================] - 0s 97us/step - loss: 116553.7781 - val_loss: 129940.6405\n",
      "Epoch 274/10000\n",
      "350/350 [==============================] - 0s 85us/step - loss: 116449.8844 - val_loss: 129824.6683\n",
      "Epoch 275/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 116344.7751 - val_loss: 129707.5351\n",
      "Epoch 276/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 116240.2255 - val_loss: 129592.0850\n",
      "Epoch 277/10000\n",
      "350/350 [==============================] - 0s 83us/step - loss: 116136.3778 - val_loss: 129474.6202\n",
      "Epoch 278/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 116030.1033 - val_loss: 129360.2316\n",
      "Epoch 279/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 115927.7498 - val_loss: 129243.4372\n",
      "Epoch 280/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 115821.0574 - val_loss: 129127.4786\n",
      "Epoch 281/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 115717.3336 - val_loss: 129012.2888\n",
      "Epoch 282/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 115613.1330 - val_loss: 128896.4979\n",
      "Epoch 283/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 66us/step - loss: 115511.1224 - val_loss: 128780.0295\n",
      "Epoch 284/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 115406.9867 - val_loss: 128664.7996\n",
      "Epoch 285/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 115302.3753 - val_loss: 128551.9081\n",
      "Epoch 286/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 115201.2665 - val_loss: 128435.6069\n",
      "Epoch 287/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 115096.9514 - val_loss: 128321.2354\n",
      "Epoch 288/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 114992.8183 - val_loss: 128206.8922\n",
      "Epoch 289/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 114891.2350 - val_loss: 128089.4317\n",
      "Epoch 290/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 114785.9213 - val_loss: 127975.1550\n",
      "Epoch 291/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 114683.4510 - val_loss: 127859.8115\n",
      "Epoch 292/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 114580.3738 - val_loss: 127743.8095\n",
      "Epoch 293/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 114475.8413 - val_loss: 127629.0984\n",
      "Epoch 294/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 114372.5807 - val_loss: 127514.0788\n",
      "Epoch 295/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 114270.6287 - val_loss: 127397.0802\n",
      "Epoch 296/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 114164.9347 - val_loss: 127284.8803\n",
      "Epoch 297/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 114062.0435 - val_loss: 127169.2578\n",
      "Epoch 298/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 113959.9600 - val_loss: 127052.7303\n",
      "Epoch 299/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 113855.1349 - val_loss: 126937.0916\n",
      "Epoch 300/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 113752.0565 - val_loss: 126822.2680\n",
      "Epoch 301/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 113649.7548 - val_loss: 126706.4764\n",
      "Epoch 302/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 113544.5065 - val_loss: 126593.7118\n",
      "Epoch 303/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 113442.3243 - val_loss: 126477.3480\n",
      "Epoch 304/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 113338.5443 - val_loss: 126362.8947\n",
      "Epoch 305/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 113236.7350 - val_loss: 126246.4388\n",
      "Epoch 306/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 113132.4905 - val_loss: 126132.3004\n",
      "Epoch 307/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 113028.8200 - val_loss: 126016.5855\n",
      "Epoch 308/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 112924.2970 - val_loss: 125903.1766\n",
      "Epoch 309/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 112822.3667 - val_loss: 125787.2499\n",
      "Epoch 310/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 112718.3423 - val_loss: 125673.3885\n",
      "Epoch 311/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 112616.4096 - val_loss: 125557.0384\n",
      "Epoch 312/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 112513.8554 - val_loss: 125441.3198\n",
      "Epoch 313/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 112408.9960 - val_loss: 125326.6113\n",
      "Epoch 314/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 112306.9458 - val_loss: 125212.1016\n",
      "Epoch 315/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 112205.7615 - val_loss: 125095.6537\n",
      "Epoch 316/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 112101.7199 - val_loss: 124979.8203\n",
      "Epoch 317/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 111998.1908 - val_loss: 124865.8220\n",
      "Epoch 318/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 111896.9097 - val_loss: 124749.9129\n",
      "Epoch 319/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 111791.2628 - val_loss: 124637.4037\n",
      "Epoch 320/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 111689.4995 - val_loss: 124523.3196\n",
      "Epoch 321/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 111588.3552 - val_loss: 124406.6812\n",
      "Epoch 322/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 111483.7992 - val_loss: 124291.6703\n",
      "Epoch 323/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 111381.8735 - val_loss: 124176.1968\n",
      "Epoch 324/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 111276.2112 - val_loss: 124063.0568\n",
      "Epoch 325/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 111176.6920 - val_loss: 123947.2055\n",
      "Epoch 326/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 111071.9196 - val_loss: 123831.2035\n",
      "Epoch 327/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 110970.4932 - val_loss: 123715.9035\n",
      "Epoch 328/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 110865.7016 - val_loss: 123601.8546\n",
      "Epoch 329/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 110764.0029 - val_loss: 123485.7346\n",
      "Epoch 330/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 110660.9271 - val_loss: 123368.9764\n",
      "Epoch 331/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 110556.8391 - val_loss: 123254.6534\n",
      "Epoch 332/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 110456.9474 - val_loss: 123136.5936\n",
      "Epoch 333/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 110349.9465 - val_loss: 123024.7896\n",
      "Epoch 334/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 110247.2358 - val_loss: 122910.7752\n",
      "Epoch 335/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 110145.1849 - val_loss: 122797.0779\n",
      "Epoch 336/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 110044.2101 - val_loss: 122679.4096\n",
      "Epoch 337/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 109939.3691 - val_loss: 122566.6596\n",
      "Epoch 338/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 109837.3879 - val_loss: 122450.3911\n",
      "Epoch 339/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 109733.1157 - val_loss: 122338.3030\n",
      "Epoch 340/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 109631.1864 - val_loss: 122223.9005\n",
      "Epoch 341/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 109527.4663 - val_loss: 122109.0667\n",
      "Epoch 342/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 109425.2223 - val_loss: 121993.3718\n",
      "Epoch 343/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 109320.7817 - val_loss: 121880.4748\n",
      "Epoch 344/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 109219.9293 - val_loss: 121763.9616\n",
      "Epoch 345/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 109115.4285 - val_loss: 121650.1848\n",
      "Epoch 346/10000\n",
      "350/350 [==============================] - 0s 81us/step - loss: 109012.6478 - val_loss: 121535.2531\n",
      "Epoch 347/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 108909.2367 - val_loss: 121423.7348\n",
      "Epoch 348/10000\n",
      "350/350 [==============================] - 0s 83us/step - loss: 108807.8665 - val_loss: 121307.5366\n",
      "Epoch 349/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 108703.9629 - val_loss: 121192.6250\n",
      "Epoch 350/10000\n",
      "350/350 [==============================] - 0s 81us/step - loss: 108602.2217 - val_loss: 121077.3211\n",
      "Epoch 351/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 108497.5629 - val_loss: 120965.8898\n",
      "Epoch 352/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 108395.2681 - val_loss: 120851.5936\n",
      "Epoch 353/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 108291.9500 - val_loss: 120738.0345\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 354/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 108191.0885 - val_loss: 120622.3032\n",
      "Epoch 355/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 108086.2743 - val_loss: 120509.2933\n",
      "Epoch 356/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 107985.2163 - val_loss: 120394.6036\n",
      "Epoch 357/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 107881.6577 - val_loss: 120281.9235\n",
      "Epoch 358/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 107780.0549 - val_loss: 120166.9088\n",
      "Epoch 359/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 107677.0324 - val_loss: 120054.0364\n",
      "Epoch 360/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 107575.6162 - val_loss: 119939.4812\n",
      "Epoch 361/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 107470.3271 - val_loss: 119828.2535\n",
      "Epoch 362/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 107370.6262 - val_loss: 119713.3963\n",
      "Epoch 363/10000\n",
      "350/350 [==============================] - 0s 78us/step - loss: 107267.4612 - val_loss: 119598.5311\n",
      "Epoch 364/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 107165.0967 - val_loss: 119484.8212\n",
      "Epoch 365/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 107064.6071 - val_loss: 119370.9695\n",
      "Epoch 366/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 106959.3214 - val_loss: 119259.5433\n",
      "Epoch 367/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 106860.9992 - val_loss: 119146.0586\n",
      "Epoch 368/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 106758.4869 - val_loss: 119030.3834\n",
      "Epoch 369/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 106655.6124 - val_loss: 118917.2337\n",
      "Epoch 370/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 106552.7733 - val_loss: 118805.3544\n",
      "Epoch 371/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 106452.1361 - val_loss: 118691.8901\n",
      "Epoch 372/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 106350.5898 - val_loss: 118578.3921\n",
      "Epoch 373/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 106248.4084 - val_loss: 118464.5071\n",
      "Epoch 374/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 106145.5134 - val_loss: 118352.3128\n",
      "Epoch 375/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 106046.6799 - val_loss: 118235.4071\n",
      "Epoch 376/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 105941.6085 - val_loss: 118124.1853\n",
      "Epoch 377/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 105841.5394 - val_loss: 118010.7150\n",
      "Epoch 378/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 105739.0280 - val_loss: 117897.1284\n",
      "Epoch 379/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 105637.0637 - val_loss: 117784.8266\n",
      "Epoch 380/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 105535.9466 - val_loss: 117670.1071\n",
      "Epoch 381/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 105433.8314 - val_loss: 117556.7430\n",
      "Epoch 382/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 105331.3019 - val_loss: 117444.2389\n",
      "Epoch 383/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 105229.1940 - val_loss: 117331.3783\n",
      "Epoch 384/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 105127.8531 - val_loss: 117219.7881\n",
      "Epoch 385/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 105026.7885 - val_loss: 117105.4471\n",
      "Epoch 386/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 104923.8048 - val_loss: 116992.9742\n",
      "Epoch 387/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 104824.5516 - val_loss: 116877.1139\n",
      "Epoch 388/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 104720.1755 - val_loss: 116764.0320\n",
      "Epoch 389/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 104618.4012 - val_loss: 116650.9054\n",
      "Epoch 390/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 104517.9583 - val_loss: 116537.4219\n",
      "Epoch 391/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 104415.6872 - val_loss: 116424.1245\n",
      "Epoch 392/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 104312.5612 - val_loss: 116310.1917\n",
      "Epoch 393/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 104210.8974 - val_loss: 116198.4061\n",
      "Epoch 394/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 104110.2608 - val_loss: 116084.0114\n",
      "Epoch 395/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 104008.0659 - val_loss: 115970.3205\n",
      "Epoch 396/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 103904.5619 - val_loss: 115859.3152\n",
      "Epoch 397/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 103804.6355 - val_loss: 115744.6774\n",
      "Epoch 398/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 103703.6621 - val_loss: 115629.3149\n",
      "Epoch 399/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 103600.9733 - val_loss: 115518.0437\n",
      "Epoch 400/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 103498.7892 - val_loss: 115403.2255\n",
      "Epoch 401/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 103397.4656 - val_loss: 115288.6796\n",
      "Epoch 402/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 103294.8829 - val_loss: 115176.3071\n",
      "Epoch 403/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 103192.2707 - val_loss: 115065.1254\n",
      "Epoch 404/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 103094.0575 - val_loss: 114948.0161\n",
      "Epoch 405/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 102989.0386 - val_loss: 114834.5261\n",
      "Epoch 406/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 102887.2043 - val_loss: 114722.2202\n",
      "Epoch 407/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 102787.1485 - val_loss: 114607.9053\n",
      "Epoch 408/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 102683.0067 - val_loss: 114494.9248\n",
      "Epoch 409/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 102581.5623 - val_loss: 114381.7711\n",
      "Epoch 410/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 102480.7302 - val_loss: 114267.6419\n",
      "Epoch 411/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 102379.4221 - val_loss: 114153.9261\n",
      "Epoch 412/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 102276.2941 - val_loss: 114041.5343\n",
      "Epoch 413/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 102174.5189 - val_loss: 113928.1704\n",
      "Epoch 414/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 102072.3900 - val_loss: 113815.2929\n",
      "Epoch 415/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 101972.7630 - val_loss: 113699.3572\n",
      "Epoch 416/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 101869.5634 - val_loss: 113584.6104\n",
      "Epoch 417/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 101767.6099 - val_loss: 113471.7042\n",
      "Epoch 418/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 101664.6086 - val_loss: 113359.9249\n",
      "Epoch 419/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 101564.9461 - val_loss: 113245.0175\n",
      "Epoch 420/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 101461.7705 - val_loss: 113133.4733\n",
      "Epoch 421/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 101361.3313 - val_loss: 113019.1625\n",
      "Epoch 422/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 101258.7571 - val_loss: 112906.7082\n",
      "Epoch 423/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 101156.6302 - val_loss: 112794.2978\n",
      "Epoch 424/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 101056.2667 - val_loss: 112680.1613\n",
      "Epoch 425/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 100955.3062 - val_loss: 112566.0349\n",
      "Epoch 426/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 100853.5965 - val_loss: 112451.8371\n",
      "Epoch 427/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 100750.1447 - val_loss: 112339.2505\n",
      "Epoch 428/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 100649.2158 - val_loss: 112226.0581\n",
      "Epoch 429/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 100547.7180 - val_loss: 112112.7826\n",
      "Epoch 430/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 100445.6067 - val_loss: 111998.5182\n",
      "Epoch 431/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 100343.7054 - val_loss: 111886.0722\n",
      "Epoch 432/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 100243.3446 - val_loss: 111771.9339\n",
      "Epoch 433/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 100139.1420 - val_loss: 111660.1812\n",
      "Epoch 434/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 100039.7837 - val_loss: 111545.3342\n",
      "Epoch 435/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 99936.9610 - val_loss: 111431.6721\n",
      "Epoch 436/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 99835.8504 - val_loss: 111317.3969\n",
      "Epoch 437/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 99732.8459 - val_loss: 111205.3708\n",
      "Epoch 438/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 99632.9842 - val_loss: 111090.1712\n",
      "Epoch 439/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 99528.4910 - val_loss: 110978.5216\n",
      "Epoch 440/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 99428.3645 - val_loss: 110864.3438\n",
      "Epoch 441/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 99327.6658 - val_loss: 110751.5936\n",
      "Epoch 442/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 99224.6251 - val_loss: 110637.6812\n",
      "Epoch 443/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 99122.1378 - val_loss: 110523.6931\n",
      "Epoch 444/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 99021.6974 - val_loss: 110409.6380\n",
      "Epoch 445/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 98920.1580 - val_loss: 110295.0847\n",
      "Epoch 446/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 98816.9139 - val_loss: 110181.0416\n",
      "Epoch 447/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 98714.7190 - val_loss: 110068.5755\n",
      "Epoch 448/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 98613.7722 - val_loss: 109953.7595\n",
      "Epoch 449/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 98511.9515 - val_loss: 109840.2859\n",
      "Epoch 450/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 98409.3066 - val_loss: 109728.3078\n",
      "Epoch 451/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 98308.8608 - val_loss: 109614.1945\n",
      "Epoch 452/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 98206.5642 - val_loss: 109500.7172\n",
      "Epoch 453/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 98103.9088 - val_loss: 109388.9072\n",
      "Epoch 454/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 98003.0951 - val_loss: 109275.8165\n",
      "Epoch 455/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 97901.8542 - val_loss: 109160.9804\n",
      "Epoch 456/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 97798.0808 - val_loss: 109048.7320\n",
      "Epoch 457/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 97697.9730 - val_loss: 108934.8836\n",
      "Epoch 458/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 97594.8730 - val_loss: 108824.0275\n",
      "Epoch 459/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 97494.3508 - val_loss: 108710.1952\n",
      "Epoch 460/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 97392.1447 - val_loss: 108596.8236\n",
      "Epoch 461/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 97289.7801 - val_loss: 108485.1789\n",
      "Epoch 462/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 97189.4434 - val_loss: 108370.8383\n",
      "Epoch 463/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 97086.4715 - val_loss: 108258.1029\n",
      "Epoch 464/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 96984.5726 - val_loss: 108146.3786\n",
      "Epoch 465/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 96884.2355 - val_loss: 108032.1667\n",
      "Epoch 466/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 96784.8610 - val_loss: 107917.1081\n",
      "Epoch 467/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 96682.0528 - val_loss: 107803.2525\n",
      "Epoch 468/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 96579.9141 - val_loss: 107690.9554\n",
      "Epoch 469/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 96478.8441 - val_loss: 107578.6257\n",
      "Epoch 470/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 96378.5783 - val_loss: 107465.5630\n",
      "Epoch 471/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 96278.6693 - val_loss: 107349.3224\n",
      "Epoch 472/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 96174.9560 - val_loss: 107236.8028\n",
      "Epoch 473/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 96074.6384 - val_loss: 107125.5236\n",
      "Epoch 474/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 95974.0781 - val_loss: 107011.2872\n",
      "Epoch 475/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 95872.5464 - val_loss: 106898.1252\n",
      "Epoch 476/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 95772.4479 - val_loss: 106784.2088\n",
      "Epoch 477/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 95669.1370 - val_loss: 106672.2749\n",
      "Epoch 478/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 95568.2321 - val_loss: 106559.6048\n",
      "Epoch 479/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 95468.3935 - val_loss: 106445.7771\n",
      "Epoch 480/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 95366.5567 - val_loss: 106332.0861\n",
      "Epoch 481/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 95265.3386 - val_loss: 106218.1440\n",
      "Epoch 482/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 95163.2863 - val_loss: 106105.9367\n",
      "Epoch 483/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 95062.6320 - val_loss: 105992.6196\n",
      "Epoch 484/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 94961.1736 - val_loss: 105880.8547\n",
      "Epoch 485/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 94861.6358 - val_loss: 105765.1060\n",
      "Epoch 486/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 94759.3593 - val_loss: 105652.0670\n",
      "Epoch 487/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 94656.9347 - val_loss: 105540.8257\n",
      "Epoch 488/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 94558.3695 - val_loss: 105425.0598\n",
      "Epoch 489/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 94455.7363 - val_loss: 105311.3579\n",
      "Epoch 490/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 94354.7556 - val_loss: 105197.7863\n",
      "Epoch 491/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 94253.0364 - val_loss: 105086.6464\n",
      "Epoch 492/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 94151.5735 - val_loss: 104973.5955\n",
      "Epoch 493/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 94050.5406 - val_loss: 104860.7539\n",
      "Epoch 494/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 93949.9413 - val_loss: 104748.5952\n",
      "Epoch 495/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 59us/step - loss: 93847.9974 - val_loss: 104635.4017\n",
      "Epoch 496/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 93744.3328 - val_loss: 104526.4357\n",
      "Epoch 497/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 93647.0040 - val_loss: 104411.6343\n",
      "Epoch 498/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 93543.6108 - val_loss: 104300.8811\n",
      "Epoch 499/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 93443.2149 - val_loss: 104187.8766\n",
      "Epoch 500/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 93342.2570 - val_loss: 104075.3138\n",
      "Epoch 501/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 93241.1159 - val_loss: 103962.0104\n",
      "Epoch 502/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 93139.6556 - val_loss: 103849.5853\n",
      "Epoch 503/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 93037.0070 - val_loss: 103737.8425\n",
      "Epoch 504/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 92937.3131 - val_loss: 103622.9312\n",
      "Epoch 505/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 92835.0531 - val_loss: 103509.9345\n",
      "Epoch 506/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 92736.0577 - val_loss: 103396.2865\n",
      "Epoch 507/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 92631.5173 - val_loss: 103286.9164\n",
      "Epoch 508/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 92531.3854 - val_loss: 103172.5667\n",
      "Epoch 509/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 92429.6268 - val_loss: 103061.1086\n",
      "Epoch 510/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 92329.1025 - val_loss: 102948.4501\n",
      "Epoch 511/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 92227.9818 - val_loss: 102836.0989\n",
      "Epoch 512/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 92124.7251 - val_loss: 102725.5098\n",
      "Epoch 513/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 92024.9670 - val_loss: 102614.9889\n",
      "Epoch 514/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 91923.5707 - val_loss: 102503.6852\n",
      "Epoch 515/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 91821.9535 - val_loss: 102390.9366\n",
      "Epoch 516/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 91720.2791 - val_loss: 102280.6579\n",
      "Epoch 517/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 91620.8975 - val_loss: 102167.7917\n",
      "Epoch 518/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 91519.1716 - val_loss: 102055.0299\n",
      "Epoch 519/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 91416.5358 - val_loss: 101943.9767\n",
      "Epoch 520/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 91316.2730 - val_loss: 101832.9669\n",
      "Epoch 521/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 91214.5885 - val_loss: 101721.4286\n",
      "Epoch 522/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 91113.9827 - val_loss: 101609.1304\n",
      "Epoch 523/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 91012.3023 - val_loss: 101496.9709\n",
      "Epoch 524/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 90910.9751 - val_loss: 101385.6505\n",
      "Epoch 525/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 90811.4998 - val_loss: 101272.5915\n",
      "Epoch 526/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 90707.0703 - val_loss: 101163.0312\n",
      "Epoch 527/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 90608.5858 - val_loss: 101049.6719\n",
      "Epoch 528/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 90504.2525 - val_loss: 100940.2230\n",
      "Epoch 529/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 90406.4856 - val_loss: 100827.3579\n",
      "Epoch 530/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 90304.1897 - val_loss: 100714.2446\n",
      "Epoch 531/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 90201.1833 - val_loss: 100604.2727\n",
      "Epoch 532/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 90102.5259 - val_loss: 100493.6211\n",
      "Epoch 533/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 90002.1191 - val_loss: 100380.1197\n",
      "Epoch 534/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 89900.1109 - val_loss: 100269.1189\n",
      "Epoch 535/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 89799.8308 - val_loss: 100158.2226\n",
      "Epoch 536/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 89698.9013 - val_loss: 100045.7165\n",
      "Epoch 537/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 89599.5407 - val_loss: 99932.9551\n",
      "Epoch 538/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 89497.5783 - val_loss: 99822.9471\n",
      "Epoch 539/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 89397.0450 - val_loss: 99711.5002\n",
      "Epoch 540/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 89297.7555 - val_loss: 99600.0499\n",
      "Epoch 541/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 89196.7214 - val_loss: 99489.7361\n",
      "Epoch 542/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 89095.7112 - val_loss: 99379.5488\n",
      "Epoch 543/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 88996.5123 - val_loss: 99267.1452\n",
      "Epoch 544/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 88896.1969 - val_loss: 99154.7635\n",
      "Epoch 545/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 88796.2264 - val_loss: 99042.4128\n",
      "Epoch 546/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 88695.0387 - val_loss: 98930.6239\n",
      "Epoch 547/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 88593.7958 - val_loss: 98819.3900\n",
      "Epoch 548/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 88495.2334 - val_loss: 98705.8620\n",
      "Epoch 549/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 88393.8053 - val_loss: 98596.2063\n",
      "Epoch 550/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 88293.6658 - val_loss: 98485.3846\n",
      "Epoch 551/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 88191.2667 - val_loss: 98377.4432\n",
      "Epoch 552/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 88093.6066 - val_loss: 98265.9314\n",
      "Epoch 553/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 87992.3899 - val_loss: 98155.2847\n",
      "Epoch 554/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 87891.2562 - val_loss: 98047.0389\n",
      "Epoch 555/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 87791.2173 - val_loss: 97937.3630\n",
      "Epoch 556/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 87691.8678 - val_loss: 97825.6039\n",
      "Epoch 557/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 87592.3597 - val_loss: 97714.1070\n",
      "Epoch 558/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 87490.0426 - val_loss: 97603.9034\n",
      "Epoch 559/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 87389.2085 - val_loss: 97495.1687\n",
      "Epoch 560/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 87289.7750 - val_loss: 97383.5172\n",
      "Epoch 561/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 87189.9507 - val_loss: 97273.4202\n",
      "Epoch 562/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 87088.6602 - val_loss: 97163.7162\n",
      "Epoch 563/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 86988.8500 - val_loss: 97052.1801\n",
      "Epoch 564/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 86887.5737 - val_loss: 96942.9454\n",
      "Epoch 565/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 86789.1357 - val_loss: 96831.7059\n",
      "Epoch 566/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 86687.2816 - val_loss: 96722.3953\n",
      "Epoch 567/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 86586.9273 - val_loss: 96612.9483\n",
      "Epoch 568/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 86487.3245 - val_loss: 96502.2444\n",
      "Epoch 569/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 86385.5665 - val_loss: 96394.2620\n",
      "Epoch 570/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 86286.2693 - val_loss: 96284.0103\n",
      "Epoch 571/10000\n",
      "350/350 [==============================] - 0s 91us/step - loss: 86186.2781 - val_loss: 96174.5521\n",
      "Epoch 572/10000\n",
      "350/350 [==============================] - 0s 111us/step - loss: 86085.5702 - val_loss: 96062.1822\n",
      "Epoch 573/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 85985.3794 - val_loss: 95952.1770\n",
      "Epoch 574/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 85884.8229 - val_loss: 95842.1428\n",
      "Epoch 575/10000\n",
      "350/350 [==============================] - 0s 87us/step - loss: 85785.6185 - val_loss: 95733.0093\n",
      "Epoch 576/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 85683.0796 - val_loss: 95623.8387\n",
      "Epoch 577/10000\n",
      "350/350 [==============================] - 0s 82us/step - loss: 85584.3773 - val_loss: 95514.5103\n",
      "Epoch 578/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 85486.1517 - val_loss: 95405.6689\n",
      "Epoch 579/10000\n",
      "350/350 [==============================] - 0s 91us/step - loss: 85385.4779 - val_loss: 95295.8353\n",
      "Epoch 580/10000\n",
      "350/350 [==============================] - 0s 80us/step - loss: 85287.2523 - val_loss: 95186.1636\n",
      "Epoch 581/10000\n",
      "350/350 [==============================] - 0s 99us/step - loss: 85187.4389 - val_loss: 95076.3522\n",
      "Epoch 582/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 85087.0709 - val_loss: 94967.9752\n",
      "Epoch 583/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 84987.9895 - val_loss: 94857.1911\n",
      "Epoch 584/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 84886.8798 - val_loss: 94749.7234\n",
      "Epoch 585/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 84789.6544 - val_loss: 94639.4413\n",
      "Epoch 586/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 84688.6151 - val_loss: 94531.8536\n",
      "Epoch 587/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 84590.9847 - val_loss: 94422.0585\n",
      "Epoch 588/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 84490.1214 - val_loss: 94314.1666\n",
      "Epoch 589/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 84393.6608 - val_loss: 94206.7469\n",
      "Epoch 590/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 84293.6872 - val_loss: 94096.7395\n",
      "Epoch 591/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 84192.8979 - val_loss: 93988.1001\n",
      "Epoch 592/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 84095.2373 - val_loss: 93880.6842\n",
      "Epoch 593/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 83995.7106 - val_loss: 93772.7188\n",
      "Epoch 594/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 83896.9070 - val_loss: 93662.7456\n",
      "Epoch 595/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 83798.3493 - val_loss: 93554.1864\n",
      "Epoch 596/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 83699.0721 - val_loss: 93446.4966\n",
      "Epoch 597/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 83600.6774 - val_loss: 93337.7707\n",
      "Epoch 598/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 83500.6769 - val_loss: 93228.6969\n",
      "Epoch 599/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 83403.2127 - val_loss: 93119.4196\n",
      "Epoch 600/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 83302.8292 - val_loss: 93010.2536\n",
      "Epoch 601/10000\n",
      "350/350 [==============================] - ETA: 0s - loss: 70259.375 - 0s 71us/step - loss: 83204.0853 - val_loss: 92901.0021\n",
      "Epoch 602/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 83105.6924 - val_loss: 92791.5928\n",
      "Epoch 603/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 83006.5912 - val_loss: 92682.2962\n",
      "Epoch 604/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 82905.7649 - val_loss: 92575.8678\n",
      "Epoch 605/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 82807.7392 - val_loss: 92466.7872\n",
      "Epoch 606/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 82709.3555 - val_loss: 92357.4000\n",
      "Epoch 607/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 82609.7227 - val_loss: 92247.6954\n",
      "Epoch 608/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 82510.3744 - val_loss: 92140.8178\n",
      "Epoch 609/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 82411.5141 - val_loss: 92033.9597\n",
      "Epoch 610/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 82314.9902 - val_loss: 91921.4446\n",
      "Epoch 611/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 82214.8801 - val_loss: 91812.8528\n",
      "Epoch 612/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 82113.7186 - val_loss: 91704.7755\n",
      "Epoch 613/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 82015.8752 - val_loss: 91596.2939\n",
      "Epoch 614/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 81917.4470 - val_loss: 91486.4568\n",
      "Epoch 615/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 81818.1098 - val_loss: 91378.2714\n",
      "Epoch 616/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 81717.9312 - val_loss: 91269.6126\n",
      "Epoch 617/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 81620.8465 - val_loss: 91159.9847\n",
      "Epoch 618/10000\n",
      "350/350 [==============================] - 0s 85us/step - loss: 81520.7100 - val_loss: 91051.0417\n",
      "Epoch 619/10000\n",
      "350/350 [==============================] - 0s 86us/step - loss: 81420.9586 - val_loss: 90943.7084\n",
      "Epoch 620/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 81322.6392 - val_loss: 90835.6320\n",
      "Epoch 621/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 81226.0654 - val_loss: 90723.6466\n",
      "Epoch 622/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 81126.3625 - val_loss: 90615.4697\n",
      "Epoch 623/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 81026.7990 - val_loss: 90507.7004\n",
      "Epoch 624/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 80927.8363 - val_loss: 90399.6629\n",
      "Epoch 625/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 80830.8354 - val_loss: 90289.6920\n",
      "Epoch 626/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 80731.1699 - val_loss: 90182.4205\n",
      "Epoch 627/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 80633.9917 - val_loss: 90070.7230\n",
      "Epoch 628/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 80533.0468 - val_loss: 89962.1884\n",
      "Epoch 629/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 80435.2765 - val_loss: 89855.3468\n",
      "Epoch 630/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 80335.9533 - val_loss: 89746.7445\n",
      "Epoch 631/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 80237.8676 - val_loss: 89639.3814\n",
      "Epoch 632/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 80141.4519 - val_loss: 89529.0378\n",
      "Epoch 633/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 80040.2292 - val_loss: 89421.4636\n",
      "Epoch 634/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 79942.5767 - val_loss: 89312.8795\n",
      "Epoch 635/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 79845.3371 - val_loss: 89200.6511\n",
      "Epoch 636/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 79744.4703 - val_loss: 89093.5565\n",
      "Epoch 637/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 79646.6752 - val_loss: 88986.1498\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 638/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 79549.4312 - val_loss: 88876.1547\n",
      "Epoch 639/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 79449.6901 - val_loss: 88766.0845\n",
      "Epoch 640/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 79349.0367 - val_loss: 88661.0032\n",
      "Epoch 641/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 79253.3133 - val_loss: 88549.9148\n",
      "Epoch 642/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 79154.4252 - val_loss: 88441.3482\n",
      "Epoch 643/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 79055.0481 - val_loss: 88332.2173\n",
      "Epoch 644/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 78957.7475 - val_loss: 88222.6003\n",
      "Epoch 645/10000\n",
      "350/350 [==============================] - 0s 88us/step - loss: 78859.4727 - val_loss: 88112.5780\n",
      "Epoch 646/10000\n",
      "350/350 [==============================] - 0s 78us/step - loss: 78759.2231 - val_loss: 88004.9951\n",
      "Epoch 647/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 78659.4217 - val_loss: 87899.4484\n",
      "Epoch 648/10000\n",
      "350/350 [==============================] - 0s 133us/step - loss: 78563.2956 - val_loss: 87790.8971\n",
      "Epoch 649/10000\n",
      "350/350 [==============================] - 0s 82us/step - loss: 78464.1140 - val_loss: 87680.3781\n",
      "Epoch 650/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 78365.9315 - val_loss: 87572.8285\n",
      "Epoch 651/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 78266.6433 - val_loss: 87462.5056\n",
      "Epoch 652/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 78167.7454 - val_loss: 87355.6420\n",
      "Epoch 653/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 78069.3547 - val_loss: 87246.5404\n",
      "Epoch 654/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 77970.2240 - val_loss: 87140.6836\n",
      "Epoch 655/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 77872.4693 - val_loss: 87030.8021\n",
      "Epoch 656/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 77775.5759 - val_loss: 86920.9675\n",
      "Epoch 657/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 77677.6717 - val_loss: 86811.6605\n",
      "Epoch 658/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 77578.3147 - val_loss: 86702.2375\n",
      "Epoch 659/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 77479.1336 - val_loss: 86594.1700\n",
      "Epoch 660/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 77382.3167 - val_loss: 86486.2571\n",
      "Epoch 661/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 77283.8820 - val_loss: 86377.2115\n",
      "Epoch 662/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 77185.9252 - val_loss: 86268.6078\n",
      "Epoch 663/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 77088.6290 - val_loss: 86160.1881\n",
      "Epoch 664/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 76988.8734 - val_loss: 86053.3945\n",
      "Epoch 665/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 76892.1888 - val_loss: 85945.1441\n",
      "Epoch 666/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 76792.1527 - val_loss: 85838.9763\n",
      "Epoch 667/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 76696.4765 - val_loss: 85730.0604\n",
      "Epoch 668/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 76597.1753 - val_loss: 85624.3298\n",
      "Epoch 669/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 76499.9911 - val_loss: 85515.4415\n",
      "Epoch 670/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 76402.6801 - val_loss: 85407.5063\n",
      "Epoch 671/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 76302.8194 - val_loss: 85301.6148\n",
      "Epoch 672/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 76205.7180 - val_loss: 85193.8358\n",
      "Epoch 673/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 76109.2622 - val_loss: 85085.8248\n",
      "Epoch 674/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 76010.4727 - val_loss: 84980.5811\n",
      "Epoch 675/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 75914.3299 - val_loss: 84872.2873\n",
      "Epoch 676/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 75814.0970 - val_loss: 84765.9689\n",
      "Epoch 677/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 75718.5803 - val_loss: 84657.8971\n",
      "Epoch 678/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 75619.9220 - val_loss: 84550.9068\n",
      "Epoch 679/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 75522.2423 - val_loss: 84442.6630\n",
      "Epoch 680/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 75424.1822 - val_loss: 84334.6683\n",
      "Epoch 681/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 75326.5302 - val_loss: 84228.2724\n",
      "Epoch 682/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 75229.5708 - val_loss: 84121.0521\n",
      "Epoch 683/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 75133.3968 - val_loss: 84011.6918\n",
      "Epoch 684/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 75033.3728 - val_loss: 83905.7040\n",
      "Epoch 685/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 74934.6653 - val_loss: 83799.5351\n",
      "Epoch 686/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 74839.6170 - val_loss: 83689.8916\n",
      "Epoch 687/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 74741.5313 - val_loss: 83581.1281\n",
      "Epoch 688/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 74644.2369 - val_loss: 83471.0067\n",
      "Epoch 689/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 74543.5404 - val_loss: 83368.0276\n",
      "Epoch 690/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 74447.7222 - val_loss: 83260.1198\n",
      "Epoch 691/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 74351.4934 - val_loss: 83150.7926\n",
      "Epoch 692/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 74252.0650 - val_loss: 83043.6166\n",
      "Epoch 693/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 74154.9332 - val_loss: 82937.8970\n",
      "Epoch 694/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 74057.3717 - val_loss: 82831.0592\n",
      "Epoch 695/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 73960.9029 - val_loss: 82723.3008\n",
      "Epoch 696/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 73864.3591 - val_loss: 82615.0818\n",
      "Epoch 697/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 73766.6309 - val_loss: 82506.5854\n",
      "Epoch 698/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 73669.5640 - val_loss: 82399.4738\n",
      "Epoch 699/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 73570.9863 - val_loss: 82293.5801\n",
      "Epoch 700/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 73475.2203 - val_loss: 82184.8014\n",
      "Epoch 701/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 73375.6273 - val_loss: 82080.2188\n",
      "Epoch 702/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 73280.1673 - val_loss: 81970.3941\n",
      "Epoch 703/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 73180.6882 - val_loss: 81865.2446\n",
      "Epoch 704/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 73085.0724 - val_loss: 81757.1438\n",
      "Epoch 705/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 72987.6289 - val_loss: 81647.5130\n",
      "Epoch 706/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 72891.0693 - val_loss: 81540.9116\n",
      "Epoch 707/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 72792.5549 - val_loss: 81432.2446\n",
      "Epoch 708/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 72693.8240 - val_loss: 81324.2702\n",
      "Epoch 709/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 72597.4196 - val_loss: 81218.0959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 710/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 72500.8736 - val_loss: 81109.8065\n",
      "Epoch 711/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 72402.5826 - val_loss: 81002.9370\n",
      "Epoch 712/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 72306.8366 - val_loss: 80895.3522\n",
      "Epoch 713/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 72207.9669 - val_loss: 80787.9022\n",
      "Epoch 714/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 72112.1508 - val_loss: 80681.5304\n",
      "Epoch 715/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 72015.0077 - val_loss: 80573.9801\n",
      "Epoch 716/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 71917.8820 - val_loss: 80466.3739\n",
      "Epoch 717/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 71820.6086 - val_loss: 80359.3302\n",
      "Epoch 718/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 71723.1298 - val_loss: 80252.7210\n",
      "Epoch 719/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 71627.5751 - val_loss: 80145.7801\n",
      "Epoch 720/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 71529.7901 - val_loss: 80039.4111\n",
      "Epoch 721/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 71432.9219 - val_loss: 79932.2639\n",
      "Epoch 722/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 71335.3752 - val_loss: 79828.3356\n",
      "Epoch 723/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 71239.3703 - val_loss: 79720.4913\n",
      "Epoch 724/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 71142.6107 - val_loss: 79613.5676\n",
      "Epoch 725/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 71045.9829 - val_loss: 79504.9811\n",
      "Epoch 726/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 70948.9167 - val_loss: 79399.0163\n",
      "Epoch 727/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 70853.3607 - val_loss: 79291.2307\n",
      "Epoch 728/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 70756.3070 - val_loss: 79183.3820\n",
      "Epoch 729/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 70659.2590 - val_loss: 79074.9214\n",
      "Epoch 730/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 70562.0371 - val_loss: 78968.3101\n",
      "Epoch 731/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 70466.5001 - val_loss: 78859.7150\n",
      "Epoch 732/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 70368.3681 - val_loss: 78754.2183\n",
      "Epoch 733/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 70272.1693 - val_loss: 78647.0422\n",
      "Epoch 734/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 70173.7870 - val_loss: 78541.5067\n",
      "Epoch 735/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 70079.9536 - val_loss: 78432.0033\n",
      "Epoch 736/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 69980.6390 - val_loss: 78326.8138\n",
      "Epoch 737/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 69885.1317 - val_loss: 78220.8518\n",
      "Epoch 738/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 69787.7245 - val_loss: 78115.1789\n",
      "Epoch 739/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 69691.9123 - val_loss: 78006.4292\n",
      "Epoch 740/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 69595.4524 - val_loss: 77898.4847\n",
      "Epoch 741/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 69497.4375 - val_loss: 77790.1941\n",
      "Epoch 742/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 69400.2520 - val_loss: 77684.9473\n",
      "Epoch 743/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 69303.4877 - val_loss: 77577.0669\n",
      "Epoch 744/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 69206.7261 - val_loss: 77469.1948\n",
      "Epoch 745/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 69110.1523 - val_loss: 77361.2298\n",
      "Epoch 746/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 69012.6224 - val_loss: 77255.1466\n",
      "Epoch 747/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 68916.8506 - val_loss: 77146.7086\n",
      "Epoch 748/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 68819.5414 - val_loss: 77040.5716\n",
      "Epoch 749/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 68722.3157 - val_loss: 76934.6408\n",
      "Epoch 750/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 68626.1837 - val_loss: 76824.8624\n",
      "Epoch 751/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 68529.5392 - val_loss: 76718.4401\n",
      "Epoch 752/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 68433.7131 - val_loss: 76608.9114\n",
      "Epoch 753/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 68333.8287 - val_loss: 76503.6561\n",
      "Epoch 754/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 68238.7341 - val_loss: 76396.2756\n",
      "Epoch 755/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 68142.7955 - val_loss: 76289.1884\n",
      "Epoch 756/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 68045.9812 - val_loss: 76183.2201\n",
      "Epoch 757/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 67951.5533 - val_loss: 76077.8491\n",
      "Epoch 758/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 67853.2957 - val_loss: 75970.0948\n",
      "Epoch 759/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 67757.0560 - val_loss: 75863.4922\n",
      "Epoch 760/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 67662.3075 - val_loss: 75753.8469\n",
      "Epoch 761/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 67565.4518 - val_loss: 75646.4419\n",
      "Epoch 762/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 67469.5844 - val_loss: 75538.9859\n",
      "Epoch 763/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 67373.0102 - val_loss: 75432.9519\n",
      "Epoch 764/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 67277.4986 - val_loss: 75326.6584\n",
      "Epoch 765/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 67180.4551 - val_loss: 75219.3422\n",
      "Epoch 766/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 67084.3580 - val_loss: 75111.2141\n",
      "Epoch 767/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 66988.1419 - val_loss: 75003.8508\n",
      "Epoch 768/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 66891.8407 - val_loss: 74897.4116\n",
      "Epoch 769/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 66795.8554 - val_loss: 74790.4069\n",
      "Epoch 770/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 66700.7565 - val_loss: 74680.4097\n",
      "Epoch 771/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 66604.0801 - val_loss: 74572.0585\n",
      "Epoch 772/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 66508.1942 - val_loss: 74464.5216\n",
      "Epoch 773/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 66412.0660 - val_loss: 74356.9924\n",
      "Epoch 774/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 66315.5876 - val_loss: 74249.9741\n",
      "Epoch 775/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 66218.9534 - val_loss: 74144.4951\n",
      "Epoch 776/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 66123.8522 - val_loss: 74037.4011\n",
      "Epoch 777/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 66029.2509 - val_loss: 73928.3318\n",
      "Epoch 778/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 65932.1291 - val_loss: 73821.6474\n",
      "Epoch 779/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 65835.6838 - val_loss: 73716.4649\n",
      "Epoch 780/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 65742.2247 - val_loss: 73606.4050\n",
      "Epoch 781/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 65642.8429 - val_loss: 73501.9619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 782/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 65550.0733 - val_loss: 73393.5181\n",
      "Epoch 783/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 65452.8051 - val_loss: 73287.6532\n",
      "Epoch 784/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 65356.2447 - val_loss: 73180.9009\n",
      "Epoch 785/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 65261.1104 - val_loss: 73072.7952\n",
      "Epoch 786/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 65166.5027 - val_loss: 72965.0934\n",
      "Epoch 787/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 65069.6106 - val_loss: 72859.9264\n",
      "Epoch 788/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 64975.4608 - val_loss: 72752.6856\n",
      "Epoch 789/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 64877.9101 - val_loss: 72645.9542\n",
      "Epoch 790/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 64784.7196 - val_loss: 72537.5052\n",
      "Epoch 791/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 64686.0866 - val_loss: 72432.7777\n",
      "Epoch 792/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 64592.1700 - val_loss: 72327.1225\n",
      "Epoch 793/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 64498.4265 - val_loss: 72219.9603\n",
      "Epoch 794/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 64400.6148 - val_loss: 72115.2335\n",
      "Epoch 795/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 64307.2199 - val_loss: 72007.9444\n",
      "Epoch 796/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 64210.7579 - val_loss: 71899.4824\n",
      "Epoch 797/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 64116.2940 - val_loss: 71796.1506\n",
      "Epoch 798/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 64021.2845 - val_loss: 71685.6951\n",
      "Epoch 799/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 63923.0829 - val_loss: 71582.7026\n",
      "Epoch 800/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 63829.2176 - val_loss: 71475.8985\n",
      "Epoch 801/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 63734.7296 - val_loss: 71366.5778\n",
      "Epoch 802/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 63638.4165 - val_loss: 71260.2185\n",
      "Epoch 803/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 63542.5938 - val_loss: 71153.6142\n",
      "Epoch 804/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 63447.7249 - val_loss: 71047.8635\n",
      "Epoch 805/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 63351.6091 - val_loss: 70943.8165\n",
      "Epoch 806/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 63256.1596 - val_loss: 70836.2241\n",
      "Epoch 807/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 63161.1700 - val_loss: 70729.3051\n",
      "Epoch 808/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 63065.4187 - val_loss: 70621.8819\n",
      "Epoch 809/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 62968.8599 - val_loss: 70516.6599\n",
      "Epoch 810/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 62875.2192 - val_loss: 70409.4223\n",
      "Epoch 811/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 62779.7192 - val_loss: 70302.1060\n",
      "Epoch 812/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 62684.3458 - val_loss: 70194.6224\n",
      "Epoch 813/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 62587.6958 - val_loss: 70088.0991\n",
      "Epoch 814/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 62493.1593 - val_loss: 69980.5708\n",
      "Epoch 815/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 62396.9130 - val_loss: 69875.9133\n",
      "Epoch 816/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 62302.3441 - val_loss: 69771.1794\n",
      "Epoch 817/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 62206.9092 - val_loss: 69663.4503\n",
      "Epoch 818/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 62111.8396 - val_loss: 69557.9716\n",
      "Epoch 819/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 62015.5565 - val_loss: 69452.5340\n",
      "Epoch 820/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 61922.3960 - val_loss: 69346.5967\n",
      "Epoch 821/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 61826.2269 - val_loss: 69240.8584\n",
      "Epoch 822/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 61731.8119 - val_loss: 69135.2902\n",
      "Epoch 823/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 61637.1042 - val_loss: 69029.8203\n",
      "Epoch 824/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 61542.0584 - val_loss: 68923.5309\n",
      "Epoch 825/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 61445.8969 - val_loss: 68817.4777\n",
      "Epoch 826/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 61354.0763 - val_loss: 68708.6058\n",
      "Epoch 827/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 61258.7251 - val_loss: 68600.8526\n",
      "Epoch 828/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 61162.0360 - val_loss: 68496.2318\n",
      "Epoch 829/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 61067.6686 - val_loss: 68391.4401\n",
      "Epoch 830/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 60974.4644 - val_loss: 68282.7300\n",
      "Epoch 831/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 60876.6189 - val_loss: 68179.9815\n",
      "Epoch 832/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 60784.2112 - val_loss: 68070.5469\n",
      "Epoch 833/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 60686.7176 - val_loss: 67966.0984\n",
      "Epoch 834/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 60593.7313 - val_loss: 67858.9866\n",
      "Epoch 835/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 60499.0440 - val_loss: 67751.8618\n",
      "Epoch 836/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 60403.3444 - val_loss: 67646.2883\n",
      "Epoch 837/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 60307.9431 - val_loss: 67541.5107\n",
      "Epoch 838/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 60215.6327 - val_loss: 67434.3083\n",
      "Epoch 839/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 60118.7342 - val_loss: 67328.0560\n",
      "Epoch 840/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 60023.3872 - val_loss: 67223.8826\n",
      "Epoch 841/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 59928.9421 - val_loss: 67117.7768\n",
      "Epoch 842/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 59834.7611 - val_loss: 67011.0024\n",
      "Epoch 843/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 59739.6957 - val_loss: 66904.4319\n",
      "Epoch 844/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 59643.5537 - val_loss: 66800.1657\n",
      "Epoch 845/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 59551.7973 - val_loss: 66690.6667\n",
      "Epoch 846/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 59455.5602 - val_loss: 66586.2117\n",
      "Epoch 847/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 59360.1096 - val_loss: 66480.7185\n",
      "Epoch 848/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 59267.3408 - val_loss: 66372.9973\n",
      "Epoch 849/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 59170.3204 - val_loss: 66268.1849\n",
      "Epoch 850/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 59076.2832 - val_loss: 66161.1767\n",
      "Epoch 851/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 58981.5972 - val_loss: 66055.6028\n",
      "Epoch 852/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 58887.0904 - val_loss: 65950.3659\n",
      "Epoch 853/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 58791.0179 - val_loss: 65846.6385\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 854/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 58697.7162 - val_loss: 65740.4768\n",
      "Epoch 855/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 58603.8496 - val_loss: 65632.5749\n",
      "Epoch 856/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 58507.9346 - val_loss: 65526.2477\n",
      "Epoch 857/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 58413.3445 - val_loss: 65423.7981\n",
      "Epoch 858/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 58318.1981 - val_loss: 65316.4614\n",
      "Epoch 859/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 58226.2579 - val_loss: 65207.3993\n",
      "Epoch 860/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 58129.1348 - val_loss: 65101.0310\n",
      "Epoch 861/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 58034.1944 - val_loss: 64995.7207\n",
      "Epoch 862/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 57939.8625 - val_loss: 64890.9684\n",
      "Epoch 863/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 57845.3008 - val_loss: 64785.5327\n",
      "Epoch 864/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 57751.4290 - val_loss: 64679.2523\n",
      "Epoch 865/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 57656.5085 - val_loss: 64573.1308\n",
      "Epoch 866/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 57563.1853 - val_loss: 64464.6792\n",
      "Epoch 867/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 57467.0995 - val_loss: 64359.3867\n",
      "Epoch 868/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 57373.2837 - val_loss: 64255.6651\n",
      "Epoch 869/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 57278.6063 - val_loss: 64151.4585\n",
      "Epoch 870/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 57185.0060 - val_loss: 64044.3734\n",
      "Epoch 871/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 57090.2515 - val_loss: 63937.9472\n",
      "Epoch 872/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 56994.3210 - val_loss: 63833.2556\n",
      "Epoch 873/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 56901.6159 - val_loss: 63726.2974\n",
      "Epoch 874/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 56808.3489 - val_loss: 63619.4966\n",
      "Epoch 875/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 56714.1577 - val_loss: 63510.8236\n",
      "Epoch 876/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 56619.1792 - val_loss: 63404.6983\n",
      "Epoch 877/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 56525.1679 - val_loss: 63300.2316\n",
      "Epoch 878/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 56430.2108 - val_loss: 63195.3631\n",
      "Epoch 879/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 56336.4070 - val_loss: 63090.7508\n",
      "Epoch 880/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 56245.2750 - val_loss: 62981.4069\n",
      "Epoch 881/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 56149.9925 - val_loss: 62875.8010\n",
      "Epoch 882/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 56055.4479 - val_loss: 62768.8952\n",
      "Epoch 883/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 55962.4134 - val_loss: 62661.5700\n",
      "Epoch 884/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 55868.4527 - val_loss: 62555.5700\n",
      "Epoch 885/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 55772.7221 - val_loss: 62450.1273\n",
      "Epoch 886/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 55680.7972 - val_loss: 62343.9331\n",
      "Epoch 887/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 55586.4622 - val_loss: 62237.7441\n",
      "Epoch 888/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 55492.1152 - val_loss: 62132.2577\n",
      "Epoch 889/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 55398.3361 - val_loss: 62024.7410\n",
      "Epoch 890/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 55304.4553 - val_loss: 61919.0507\n",
      "Epoch 891/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 55210.1156 - val_loss: 61813.3173\n",
      "Epoch 892/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 55115.9115 - val_loss: 61707.5197\n",
      "Epoch 893/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 55022.7396 - val_loss: 61601.4516\n",
      "Epoch 894/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 54928.9573 - val_loss: 61495.6078\n",
      "Epoch 895/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 54835.9254 - val_loss: 61389.2440\n",
      "Epoch 896/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 54741.9542 - val_loss: 61281.8391\n",
      "Epoch 897/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 54647.0977 - val_loss: 61175.0407\n",
      "Epoch 898/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 54553.5307 - val_loss: 61070.8068\n",
      "Epoch 899/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 54461.2661 - val_loss: 60963.5966\n",
      "Epoch 900/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 54365.2842 - val_loss: 60858.5181\n",
      "Epoch 901/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 54272.5935 - val_loss: 60754.2465\n",
      "Epoch 902/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 54179.1743 - val_loss: 60645.4833\n",
      "Epoch 903/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 54084.7092 - val_loss: 60539.4594\n",
      "Epoch 904/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 53991.9860 - val_loss: 60431.4349\n",
      "Epoch 905/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 53897.8972 - val_loss: 60325.2760\n",
      "Epoch 906/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 53804.7973 - val_loss: 60216.2761\n",
      "Epoch 907/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 53708.8296 - val_loss: 60113.6926\n",
      "Epoch 908/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 53615.0777 - val_loss: 60008.9710\n",
      "Epoch 909/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 53524.1533 - val_loss: 59900.3866\n",
      "Epoch 910/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 53428.2717 - val_loss: 59793.5823\n",
      "Epoch 911/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 53335.2969 - val_loss: 59688.2847\n",
      "Epoch 912/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 53243.4084 - val_loss: 59582.1966\n",
      "Epoch 913/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 53147.8812 - val_loss: 59477.2433\n",
      "Epoch 914/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 53056.1921 - val_loss: 59370.0874\n",
      "Epoch 915/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 52961.8368 - val_loss: 59265.7683\n",
      "Epoch 916/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 52869.5316 - val_loss: 59161.1793\n",
      "Epoch 917/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 52775.0581 - val_loss: 59055.9578\n",
      "Epoch 918/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 52684.1384 - val_loss: 58948.8576\n",
      "Epoch 919/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 52587.1497 - val_loss: 58844.7633\n",
      "Epoch 920/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 52495.9583 - val_loss: 58739.2894\n",
      "Epoch 921/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 52403.0254 - val_loss: 58632.3048\n",
      "Epoch 922/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 52309.1505 - val_loss: 58527.7631\n",
      "Epoch 923/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 52217.4138 - val_loss: 58420.6965\n",
      "Epoch 924/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 52122.1231 - val_loss: 58315.9867\n",
      "Epoch 925/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 52030.8137 - val_loss: 58210.3833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 926/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 51937.6139 - val_loss: 58104.4877\n",
      "Epoch 927/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 51844.3823 - val_loss: 57997.7477\n",
      "Epoch 928/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 51749.0865 - val_loss: 57894.1727\n",
      "Epoch 929/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 51659.1965 - val_loss: 57787.7859\n",
      "Epoch 930/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 51567.1066 - val_loss: 57678.7444\n",
      "Epoch 931/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 51471.5784 - val_loss: 57573.3297\n",
      "Epoch 932/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 51377.8779 - val_loss: 57469.5273\n",
      "Epoch 933/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 51285.6469 - val_loss: 57364.7659\n",
      "Epoch 934/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 51192.8943 - val_loss: 57259.0744\n",
      "Epoch 935/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 51099.9468 - val_loss: 57152.2909\n",
      "Epoch 936/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 51005.4650 - val_loss: 57048.9852\n",
      "Epoch 937/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 50914.2645 - val_loss: 56944.4374\n",
      "Epoch 938/10000\n",
      "350/350 [==============================] - 0s 111us/step - loss: 50821.1799 - val_loss: 56836.9703\n",
      "Epoch 939/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 50728.3378 - val_loss: 56729.7276\n",
      "Epoch 940/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 50633.3886 - val_loss: 56625.9351\n",
      "Epoch 941/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 50540.5499 - val_loss: 56521.9266\n",
      "Epoch 942/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 50447.4495 - val_loss: 56417.4225\n",
      "Epoch 943/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 50355.2875 - val_loss: 56312.7158\n",
      "Epoch 944/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 50261.8618 - val_loss: 56206.2244\n",
      "Epoch 945/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 50167.5309 - val_loss: 56102.6518\n",
      "Epoch 946/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 50076.1677 - val_loss: 55996.8483\n",
      "Epoch 947/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 49985.2574 - val_loss: 55888.4284\n",
      "Epoch 948/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 49888.4825 - val_loss: 55787.1207\n",
      "Epoch 949/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 49798.5839 - val_loss: 55680.3589\n",
      "Epoch 950/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 49704.2718 - val_loss: 55577.4457\n",
      "Epoch 951/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 49611.8162 - val_loss: 55472.5026\n",
      "Epoch 952/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 49518.2446 - val_loss: 55368.8714\n",
      "Epoch 953/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 49428.2394 - val_loss: 55264.3009\n",
      "Epoch 954/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 49334.1315 - val_loss: 55159.0384\n",
      "Epoch 955/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 49241.0351 - val_loss: 55055.9891\n",
      "Epoch 956/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 49149.4338 - val_loss: 54949.3682\n",
      "Epoch 957/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 49054.1687 - val_loss: 54845.4823\n",
      "Epoch 958/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 48962.5904 - val_loss: 54739.9375\n",
      "Epoch 959/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 48870.4894 - val_loss: 54635.6393\n",
      "Epoch 960/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 48778.1523 - val_loss: 54529.3333\n",
      "Epoch 961/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 48684.7104 - val_loss: 54425.6617\n",
      "Epoch 962/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 48592.2333 - val_loss: 54323.0068\n",
      "Epoch 963/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 48499.8496 - val_loss: 54218.1950\n",
      "Epoch 964/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 48407.4257 - val_loss: 54114.5199\n",
      "Epoch 965/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 48314.0657 - val_loss: 54010.9884\n",
      "Epoch 966/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 48224.7733 - val_loss: 53903.8769\n",
      "Epoch 967/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 48129.6728 - val_loss: 53800.0207\n",
      "Epoch 968/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 48038.1828 - val_loss: 53697.7761\n",
      "Epoch 969/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 47945.7524 - val_loss: 53591.0379\n",
      "Epoch 970/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 47852.5435 - val_loss: 53486.6202\n",
      "Epoch 971/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 47760.9020 - val_loss: 53380.6197\n",
      "Epoch 972/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 47667.0632 - val_loss: 53278.8293\n",
      "Epoch 973/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 47576.1435 - val_loss: 53172.4864\n",
      "Epoch 974/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 47482.9093 - val_loss: 53067.3523\n",
      "Epoch 975/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 47390.5824 - val_loss: 52963.4257\n",
      "Epoch 976/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 47296.8266 - val_loss: 52859.8527\n",
      "Epoch 977/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 47206.3771 - val_loss: 52756.1181\n",
      "Epoch 978/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 47112.9956 - val_loss: 52652.3041\n",
      "Epoch 979/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 47021.7486 - val_loss: 52545.4069\n",
      "Epoch 980/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 46928.0081 - val_loss: 52440.6767\n",
      "Epoch 981/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 46835.4140 - val_loss: 52335.1135\n",
      "Epoch 982/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 46742.4782 - val_loss: 52232.8315\n",
      "Epoch 983/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 46651.0123 - val_loss: 52127.1334\n",
      "Epoch 984/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 46559.9074 - val_loss: 52021.2147\n",
      "Epoch 985/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 46464.1906 - val_loss: 51920.6124\n",
      "Epoch 986/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 46373.6122 - val_loss: 51817.0785\n",
      "Epoch 987/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 46282.7253 - val_loss: 51709.8680\n",
      "Epoch 988/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 46188.6684 - val_loss: 51606.7353\n",
      "Epoch 989/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 46096.9718 - val_loss: 51503.4690\n",
      "Epoch 990/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 46003.2174 - val_loss: 51399.2759\n",
      "Epoch 991/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 45912.4035 - val_loss: 51294.3353\n",
      "Epoch 992/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 45818.9787 - val_loss: 51191.4668\n",
      "Epoch 993/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 45727.6350 - val_loss: 51085.7992\n",
      "Epoch 994/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 45635.1863 - val_loss: 50981.4870\n",
      "Epoch 995/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 45543.4867 - val_loss: 50877.2126\n",
      "Epoch 996/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 45450.0240 - val_loss: 50773.4326\n",
      "Epoch 997/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 45359.0304 - val_loss: 50667.0547\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 998/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 45266.7784 - val_loss: 50562.1606\n",
      "Epoch 999/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 45172.8763 - val_loss: 50458.7513\n",
      "Epoch 1000/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 45081.5922 - val_loss: 50355.6442\n",
      "Epoch 1001/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 44989.8153 - val_loss: 50252.4419\n",
      "Epoch 1002/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 44898.5867 - val_loss: 50148.5284\n",
      "Epoch 1003/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 44806.8304 - val_loss: 50043.2369\n",
      "Epoch 1004/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 44713.7597 - val_loss: 49938.2491\n",
      "Epoch 1005/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 44623.4788 - val_loss: 49833.0974\n",
      "Epoch 1006/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 44529.8417 - val_loss: 49732.0352\n",
      "Epoch 1007/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 44441.9602 - val_loss: 49622.9081\n",
      "Epoch 1008/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 44344.6229 - val_loss: 49522.0648\n",
      "Epoch 1009/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 44254.8108 - val_loss: 49417.7216\n",
      "Epoch 1010/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 44164.4019 - val_loss: 49310.5256\n",
      "Epoch 1011/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 44070.9873 - val_loss: 49207.8941\n",
      "Epoch 1012/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 43978.8488 - val_loss: 49105.6539\n",
      "Epoch 1013/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 43888.0706 - val_loss: 49000.4348\n",
      "Epoch 1014/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 43795.6642 - val_loss: 48895.2977\n",
      "Epoch 1015/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 43703.9938 - val_loss: 48791.9969\n",
      "Epoch 1016/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 43612.2751 - val_loss: 48687.4861\n",
      "Epoch 1017/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 43519.9239 - val_loss: 48584.0769\n",
      "Epoch 1018/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 43429.6093 - val_loss: 48475.4792\n",
      "Epoch 1019/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 43337.1527 - val_loss: 48371.1244\n",
      "Epoch 1020/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 43244.5623 - val_loss: 48266.5365\n",
      "Epoch 1021/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 43153.2258 - val_loss: 48161.9678\n",
      "Epoch 1022/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 43062.1757 - val_loss: 48058.6499\n",
      "Epoch 1023/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 42969.6197 - val_loss: 47954.7077\n",
      "Epoch 1024/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 42880.5881 - val_loss: 47848.4732\n",
      "Epoch 1025/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 42787.8206 - val_loss: 47742.4343\n",
      "Epoch 1026/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 42696.2249 - val_loss: 47639.0614\n",
      "Epoch 1027/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 42604.9494 - val_loss: 47533.8410\n",
      "Epoch 1028/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 42513.1811 - val_loss: 47432.7078\n",
      "Epoch 1029/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 42422.5007 - val_loss: 47328.1816\n",
      "Epoch 1030/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 42331.2732 - val_loss: 47222.9686\n",
      "Epoch 1031/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 42240.6625 - val_loss: 47117.3439\n",
      "Epoch 1032/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 42150.4600 - val_loss: 47009.5916\n",
      "Epoch 1033/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 42056.2203 - val_loss: 46907.3174\n",
      "Epoch 1034/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 41966.8862 - val_loss: 46801.2836\n",
      "Epoch 1035/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 41873.8107 - val_loss: 46699.9944\n",
      "Epoch 1036/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 41785.0906 - val_loss: 46592.7816\n",
      "Epoch 1037/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 41690.6474 - val_loss: 46490.2379\n",
      "Epoch 1038/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 41600.9062 - val_loss: 46384.0323\n",
      "Epoch 1039/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 41509.3107 - val_loss: 46279.3712\n",
      "Epoch 1040/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 41418.2939 - val_loss: 46174.6802\n",
      "Epoch 1041/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 41326.7075 - val_loss: 46070.0101\n",
      "Epoch 1042/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 41235.4551 - val_loss: 45966.2781\n",
      "Epoch 1043/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 41143.6033 - val_loss: 45861.9169\n",
      "Epoch 1044/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 41051.8301 - val_loss: 45757.4802\n",
      "Epoch 1045/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 40962.3058 - val_loss: 45654.8096\n",
      "Epoch 1046/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 40872.0779 - val_loss: 45548.8985\n",
      "Epoch 1047/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 40780.3006 - val_loss: 45444.5310\n",
      "Epoch 1048/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 40690.0604 - val_loss: 45342.5102\n",
      "Epoch 1049/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 40598.8046 - val_loss: 45238.5998\n",
      "Epoch 1050/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 40508.6422 - val_loss: 45134.2069\n",
      "Epoch 1051/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 40418.8611 - val_loss: 45031.3195\n",
      "Epoch 1052/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 40326.5070 - val_loss: 44929.5297\n",
      "Epoch 1053/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 40237.9643 - val_loss: 44825.2423\n",
      "Epoch 1054/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 40146.6833 - val_loss: 44722.4219\n",
      "Epoch 1055/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 40055.9556 - val_loss: 44622.3965\n",
      "Epoch 1056/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 39966.3516 - val_loss: 44519.0006\n",
      "Epoch 1057/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 39877.0961 - val_loss: 44416.9833\n",
      "Epoch 1058/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 39787.1342 - val_loss: 44312.3574\n",
      "Epoch 1059/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 39698.0255 - val_loss: 44208.6699\n",
      "Epoch 1060/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 39605.7230 - val_loss: 44105.8818\n",
      "Epoch 1061/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 39515.9767 - val_loss: 44005.0982\n",
      "Epoch 1062/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 39427.4101 - val_loss: 43900.3011\n",
      "Epoch 1063/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 39336.8091 - val_loss: 43797.0041\n",
      "Epoch 1064/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 39246.1329 - val_loss: 43693.6659\n",
      "Epoch 1065/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 39154.8490 - val_loss: 43593.9407\n",
      "Epoch 1066/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 39066.8606 - val_loss: 43491.2698\n",
      "Epoch 1067/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 38976.2015 - val_loss: 43388.4211\n",
      "Epoch 1068/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 38886.0744 - val_loss: 43284.7026\n",
      "Epoch 1069/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 38794.6925 - val_loss: 43183.9699\n",
      "Epoch 1070/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 38707.8150 - val_loss: 43078.0090\n",
      "Epoch 1071/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 38615.8279 - val_loss: 42976.1109\n",
      "Epoch 1072/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 38526.6289 - val_loss: 42872.7402\n",
      "Epoch 1073/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 38435.6831 - val_loss: 42771.8239\n",
      "Epoch 1074/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 38346.3277 - val_loss: 42667.9823\n",
      "Epoch 1075/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 38256.3956 - val_loss: 42563.9643\n",
      "Epoch 1076/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 38165.0300 - val_loss: 42462.6582\n",
      "Epoch 1077/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 38075.8924 - val_loss: 42359.4823\n",
      "Epoch 1078/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 37986.8284 - val_loss: 42255.3282\n",
      "Epoch 1079/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 37895.8457 - val_loss: 42152.4698\n",
      "Epoch 1080/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 37804.8115 - val_loss: 42050.4736\n",
      "Epoch 1081/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 37716.4894 - val_loss: 41947.3518\n",
      "Epoch 1082/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 37624.8636 - val_loss: 41846.5620\n",
      "Epoch 1083/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 37538.0286 - val_loss: 41741.4233\n",
      "Epoch 1084/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 37445.4118 - val_loss: 41640.2842\n",
      "Epoch 1085/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 37355.3651 - val_loss: 41538.7109\n",
      "Epoch 1086/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 37266.2662 - val_loss: 41433.2078\n",
      "Epoch 1087/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 37175.9850 - val_loss: 41329.8886\n",
      "Epoch 1088/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 37087.5859 - val_loss: 41225.1508\n",
      "Epoch 1089/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 36995.3447 - val_loss: 41124.9407\n",
      "Epoch 1090/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 36906.7387 - val_loss: 41024.3203\n",
      "Epoch 1091/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 36817.2567 - val_loss: 40921.8319\n",
      "Epoch 1092/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 36729.2450 - val_loss: 40818.2711\n",
      "Epoch 1093/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 36638.4038 - val_loss: 40716.5865\n",
      "Epoch 1094/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 36550.3268 - val_loss: 40612.6958\n",
      "Epoch 1095/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 36459.1454 - val_loss: 40512.1114\n",
      "Epoch 1096/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 36370.4497 - val_loss: 40410.6951\n",
      "Epoch 1097/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 36281.6103 - val_loss: 40309.6460\n",
      "Epoch 1098/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 36193.5113 - val_loss: 40205.0885\n",
      "Epoch 1099/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 36103.1536 - val_loss: 40105.6027\n",
      "Epoch 1100/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 36011.5851 - val_loss: 40005.0979\n",
      "Epoch 1101/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 35925.8273 - val_loss: 39900.1169\n",
      "Epoch 1102/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 35834.4048 - val_loss: 39799.9453\n",
      "Epoch 1103/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 35744.0878 - val_loss: 39699.6626\n",
      "Epoch 1104/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 35657.2045 - val_loss: 39595.7566\n",
      "Epoch 1105/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 35566.8809 - val_loss: 39492.9665\n",
      "Epoch 1106/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 35476.9164 - val_loss: 39394.3405\n",
      "Epoch 1107/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 35389.1372 - val_loss: 39292.5041\n",
      "Epoch 1108/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 35299.1583 - val_loss: 39193.1690\n",
      "Epoch 1109/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 35210.2046 - val_loss: 39093.2186\n",
      "Epoch 1110/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 35121.4487 - val_loss: 38992.4794\n",
      "Epoch 1111/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 35033.2519 - val_loss: 38890.2109\n",
      "Epoch 1112/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 34943.5967 - val_loss: 38789.6647\n",
      "Epoch 1113/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 34854.8965 - val_loss: 38689.0211\n",
      "Epoch 1114/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 34766.5563 - val_loss: 38587.3309\n",
      "Epoch 1115/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 34678.4131 - val_loss: 38485.6540\n",
      "Epoch 1116/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 34588.7015 - val_loss: 38386.0253\n",
      "Epoch 1117/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 34499.1007 - val_loss: 38288.5350\n",
      "Epoch 1118/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 34412.6951 - val_loss: 38188.3453\n",
      "Epoch 1119/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 34324.3083 - val_loss: 38087.4444\n",
      "Epoch 1120/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 34235.5502 - val_loss: 37986.8460\n",
      "Epoch 1121/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 34147.7085 - val_loss: 37886.3856\n",
      "Epoch 1122/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 34057.8630 - val_loss: 37786.7878\n",
      "Epoch 1123/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 33969.6496 - val_loss: 37688.1922\n",
      "Epoch 1124/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 33881.9176 - val_loss: 37588.3123\n",
      "Epoch 1125/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 33792.6080 - val_loss: 37489.8243\n",
      "Epoch 1126/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 33704.9567 - val_loss: 37390.1115\n",
      "Epoch 1127/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 33617.1293 - val_loss: 37288.7309\n",
      "Epoch 1128/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 33527.9756 - val_loss: 37189.1674\n",
      "Epoch 1129/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 33438.9658 - val_loss: 37089.9622\n",
      "Epoch 1130/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 33350.2392 - val_loss: 36990.2681\n",
      "Epoch 1131/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 33263.1704 - val_loss: 36889.0002\n",
      "Epoch 1132/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 33174.8215 - val_loss: 36787.5702\n",
      "Epoch 1133/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 33085.5320 - val_loss: 36688.7740\n",
      "Epoch 1134/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 32996.8759 - val_loss: 36589.6884\n",
      "Epoch 1135/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 32909.0895 - val_loss: 36490.5481\n",
      "Epoch 1136/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 32821.1828 - val_loss: 36392.6894\n",
      "Epoch 1137/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 32733.2871 - val_loss: 36292.7318\n",
      "Epoch 1138/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 32644.5160 - val_loss: 36192.3906\n",
      "Epoch 1139/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 32555.9898 - val_loss: 36094.0911\n",
      "Epoch 1140/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 67us/step - loss: 32468.5905 - val_loss: 35993.0976\n",
      "Epoch 1141/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 32380.3918 - val_loss: 35894.9408\n",
      "Epoch 1142/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 32292.7271 - val_loss: 35796.6367\n",
      "Epoch 1143/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 32205.1304 - val_loss: 35696.1322\n",
      "Epoch 1144/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 32116.2184 - val_loss: 35598.2909\n",
      "Epoch 1145/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 32029.7453 - val_loss: 35497.7535\n",
      "Epoch 1146/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 31941.2754 - val_loss: 35399.3031\n",
      "Epoch 1147/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 31855.4604 - val_loss: 35297.7757\n",
      "Epoch 1148/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 31764.3939 - val_loss: 35202.1203\n",
      "Epoch 1149/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 31677.9234 - val_loss: 35102.3066\n",
      "Epoch 1150/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 31591.2140 - val_loss: 35003.9819\n",
      "Epoch 1151/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 31505.0297 - val_loss: 34906.0594\n",
      "Epoch 1152/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 31415.8251 - val_loss: 34809.6043\n",
      "Epoch 1153/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 31328.9278 - val_loss: 34711.5776\n",
      "Epoch 1154/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 31243.4231 - val_loss: 34610.2726\n",
      "Epoch 1155/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 31155.9313 - val_loss: 34511.5058\n",
      "Epoch 1156/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 31067.8876 - val_loss: 34414.3992\n",
      "Epoch 1157/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 30979.6959 - val_loss: 34317.5430\n",
      "Epoch 1158/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 30893.5860 - val_loss: 34220.5079\n",
      "Epoch 1159/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 30806.4866 - val_loss: 34122.5928\n",
      "Epoch 1160/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 30719.9511 - val_loss: 34023.7938\n",
      "Epoch 1161/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 30630.3793 - val_loss: 33926.7703\n",
      "Epoch 1162/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 30545.3011 - val_loss: 33831.4266\n",
      "Epoch 1163/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 30456.7017 - val_loss: 33732.9087\n",
      "Epoch 1164/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 30371.3873 - val_loss: 33633.4912\n",
      "Epoch 1165/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 30283.9276 - val_loss: 33536.0270\n",
      "Epoch 1166/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 30195.7007 - val_loss: 33438.3697\n",
      "Epoch 1167/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 30108.7373 - val_loss: 33340.5262\n",
      "Epoch 1168/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 30022.2298 - val_loss: 33242.6284\n",
      "Epoch 1169/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 29934.5516 - val_loss: 33146.6875\n",
      "Epoch 1170/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 29847.9801 - val_loss: 33050.7178\n",
      "Epoch 1171/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 29762.3916 - val_loss: 32954.8825\n",
      "Epoch 1172/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 29675.1170 - val_loss: 32857.0034\n",
      "Epoch 1173/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 29586.1831 - val_loss: 32762.2293\n",
      "Epoch 1174/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 29501.5294 - val_loss: 32666.1820\n",
      "Epoch 1175/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 29415.7838 - val_loss: 32564.7085\n",
      "Epoch 1176/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 29327.0601 - val_loss: 32469.1830\n",
      "Epoch 1177/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 29241.3037 - val_loss: 32371.5316\n",
      "Epoch 1178/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 29152.9373 - val_loss: 32275.6787\n",
      "Epoch 1179/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 29067.9132 - val_loss: 32176.5533\n",
      "Epoch 1180/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 28979.5749 - val_loss: 32080.9471\n",
      "Epoch 1181/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 28892.5572 - val_loss: 31985.0874\n",
      "Epoch 1182/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 28807.0710 - val_loss: 31887.0734\n",
      "Epoch 1183/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 28721.7770 - val_loss: 31789.9779\n",
      "Epoch 1184/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 28634.7185 - val_loss: 31693.3404\n",
      "Epoch 1185/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 28545.9868 - val_loss: 31598.0608\n",
      "Epoch 1186/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 28461.0340 - val_loss: 31501.8420\n",
      "Epoch 1187/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 28375.2423 - val_loss: 31404.2521\n",
      "Epoch 1188/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 28288.8004 - val_loss: 31307.2990\n",
      "Epoch 1189/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 28201.1286 - val_loss: 31211.7754\n",
      "Epoch 1190/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 28116.3678 - val_loss: 31113.1346\n",
      "Epoch 1191/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 28029.4000 - val_loss: 31015.7921\n",
      "Epoch 1192/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 27944.1216 - val_loss: 30918.4443\n",
      "Epoch 1193/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 27856.9093 - val_loss: 30821.4491\n",
      "Epoch 1194/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 27770.4665 - val_loss: 30726.9238\n",
      "Epoch 1195/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 27684.3037 - val_loss: 30631.8732\n",
      "Epoch 1196/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 27598.6265 - val_loss: 30533.7901\n",
      "Epoch 1197/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 27511.7823 - val_loss: 30437.7140\n",
      "Epoch 1198/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 27424.2405 - val_loss: 30342.9661\n",
      "Epoch 1199/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 27341.5623 - val_loss: 30243.4113\n",
      "Epoch 1200/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 27252.7129 - val_loss: 30148.0724\n",
      "Epoch 1201/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 27167.0783 - val_loss: 30051.4043\n",
      "Epoch 1202/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 27081.2885 - val_loss: 29955.6876\n",
      "Epoch 1203/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 26995.3749 - val_loss: 29860.1013\n",
      "Epoch 1204/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 26908.1682 - val_loss: 29767.0441\n",
      "Epoch 1205/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 26823.6168 - val_loss: 29668.2825\n",
      "Epoch 1206/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 26736.5108 - val_loss: 29574.0280\n",
      "Epoch 1207/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 26650.9557 - val_loss: 29476.8605\n",
      "Epoch 1208/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 26565.7109 - val_loss: 29379.6920\n",
      "Epoch 1209/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 26478.7690 - val_loss: 29282.3416\n",
      "Epoch 1210/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 26392.7875 - val_loss: 29186.9215\n",
      "Epoch 1211/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 26305.7696 - val_loss: 29093.9112\n",
      "Epoch 1212/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 26221.1513 - val_loss: 28996.2405\n",
      "Epoch 1213/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 26135.7138 - val_loss: 28900.5574\n",
      "Epoch 1214/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 26046.8731 - val_loss: 28805.3097\n",
      "Epoch 1215/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 25962.0819 - val_loss: 28709.0370\n",
      "Epoch 1216/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 25876.6034 - val_loss: 28610.7259\n",
      "Epoch 1217/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 25789.5873 - val_loss: 28515.5618\n",
      "Epoch 1218/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 25703.8356 - val_loss: 28420.6588\n",
      "Epoch 1219/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 25617.5013 - val_loss: 28324.9251\n",
      "Epoch 1220/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 25533.4551 - val_loss: 28225.9489\n",
      "Epoch 1221/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 25443.8881 - val_loss: 28134.0388\n",
      "Epoch 1222/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 25360.8288 - val_loss: 28035.4555\n",
      "Epoch 1223/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 25273.4760 - val_loss: 27939.7224\n",
      "Epoch 1224/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 25187.6002 - val_loss: 27846.1009\n",
      "Epoch 1225/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 25101.6653 - val_loss: 27749.4963\n",
      "Epoch 1226/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 25015.2623 - val_loss: 27655.0437\n",
      "Epoch 1227/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 24931.1942 - val_loss: 27561.7271\n",
      "Epoch 1228/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 24845.6667 - val_loss: 27463.4035\n",
      "Epoch 1229/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 24758.9927 - val_loss: 27368.0237\n",
      "Epoch 1230/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 24673.6519 - val_loss: 27272.4059\n",
      "Epoch 1231/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 24588.3298 - val_loss: 27176.1934\n",
      "Epoch 1232/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 24502.6601 - val_loss: 27082.9967\n",
      "Epoch 1233/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 24415.0485 - val_loss: 26991.2729\n",
      "Epoch 1234/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 24331.8107 - val_loss: 26893.5592\n",
      "Epoch 1235/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 24245.2902 - val_loss: 26799.6391\n",
      "Epoch 1236/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 24160.5659 - val_loss: 26703.7467\n",
      "Epoch 1237/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 24074.2529 - val_loss: 26608.0872\n",
      "Epoch 1238/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 23988.8673 - val_loss: 26513.2389\n",
      "Epoch 1239/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 23904.2196 - val_loss: 26417.6809\n",
      "Epoch 1240/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 23817.1574 - val_loss: 26323.4282\n",
      "Epoch 1241/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 23732.3050 - val_loss: 26229.0746\n",
      "Epoch 1242/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 23647.0871 - val_loss: 26132.9699\n",
      "Epoch 1243/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 23561.0482 - val_loss: 26038.5092\n",
      "Epoch 1244/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 23474.8328 - val_loss: 25943.0920\n",
      "Epoch 1245/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 23389.4448 - val_loss: 25848.7576\n",
      "Epoch 1246/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 23304.8634 - val_loss: 25752.3947\n",
      "Epoch 1247/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 23220.1662 - val_loss: 25657.7441\n",
      "Epoch 1248/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 23134.9426 - val_loss: 25561.1239\n",
      "Epoch 1249/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 23047.7101 - val_loss: 25470.3484\n",
      "Epoch 1250/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 22962.8150 - val_loss: 25375.2691\n",
      "Epoch 1251/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 22879.0358 - val_loss: 25279.2353\n",
      "Epoch 1252/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 22794.4826 - val_loss: 25183.6658\n",
      "Epoch 1253/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 22707.9122 - val_loss: 25089.1366\n",
      "Epoch 1254/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 22623.0435 - val_loss: 24993.8891\n",
      "Epoch 1255/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 22537.9919 - val_loss: 24897.9393\n",
      "Epoch 1256/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 22453.5412 - val_loss: 24801.3153\n",
      "Epoch 1257/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 22367.5499 - val_loss: 24710.6225\n",
      "Epoch 1258/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 22283.6558 - val_loss: 24615.1595\n",
      "Epoch 1259/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 22197.7313 - val_loss: 24519.6007\n",
      "Epoch 1260/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 22112.4605 - val_loss: 24423.9287\n",
      "Epoch 1261/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 22028.9930 - val_loss: 24328.6609\n",
      "Epoch 1262/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 21942.4666 - val_loss: 24236.8357\n",
      "Epoch 1263/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 21859.7331 - val_loss: 24140.7039\n",
      "Epoch 1264/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 21774.3704 - val_loss: 24046.5330\n",
      "Epoch 1265/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 21689.7068 - val_loss: 23951.9279\n",
      "Epoch 1266/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 21606.4640 - val_loss: 23854.8967\n",
      "Epoch 1267/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 21523.1568 - val_loss: 23757.7676\n",
      "Epoch 1268/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 21433.5441 - val_loss: 23666.9238\n",
      "Epoch 1269/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 21351.7853 - val_loss: 23574.2364\n",
      "Epoch 1270/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 21268.4823 - val_loss: 23477.0908\n",
      "Epoch 1271/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 21184.1310 - val_loss: 23383.8621\n",
      "Epoch 1272/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 21099.8791 - val_loss: 23285.6003\n",
      "Epoch 1273/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 21014.2203 - val_loss: 23191.3438\n",
      "Epoch 1274/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 20930.0590 - val_loss: 23096.9003\n",
      "Epoch 1275/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 20845.5134 - val_loss: 23003.7617\n",
      "Epoch 1276/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 20761.2931 - val_loss: 22907.5029\n",
      "Epoch 1277/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 20676.0767 - val_loss: 22813.1016\n",
      "Epoch 1278/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 20592.9354 - val_loss: 22717.7309\n",
      "Epoch 1279/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 20508.0973 - val_loss: 22621.2647\n",
      "Epoch 1280/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 20423.4244 - val_loss: 22526.6595\n",
      "Epoch 1281/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 20338.1312 - val_loss: 22433.1885\n",
      "Epoch 1282/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 67us/step - loss: 20255.0418 - val_loss: 22337.2712\n",
      "Epoch 1283/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 20171.1177 - val_loss: 22240.6876\n",
      "Epoch 1284/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 20085.7097 - val_loss: 22148.2299\n",
      "Epoch 1285/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 20001.0151 - val_loss: 22054.3912\n",
      "Epoch 1286/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 19916.7683 - val_loss: 21960.0892\n",
      "Epoch 1287/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 19833.6191 - val_loss: 21864.8368\n",
      "Epoch 1288/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 19752.0337 - val_loss: 21767.5505\n",
      "Epoch 1289/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 19663.8969 - val_loss: 21676.4019\n",
      "Epoch 1290/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 19582.4172 - val_loss: 21579.1621\n",
      "Epoch 1291/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 19496.5526 - val_loss: 21486.2909\n",
      "Epoch 1292/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 19415.2175 - val_loss: 21389.3605\n",
      "Epoch 1293/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 19328.7369 - val_loss: 21298.4112\n",
      "Epoch 1294/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 19246.3307 - val_loss: 21202.4492\n",
      "Epoch 1295/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 19161.6860 - val_loss: 21108.7938\n",
      "Epoch 1296/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 19078.6291 - val_loss: 21012.9772\n",
      "Epoch 1297/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 18994.7108 - val_loss: 20919.1614\n",
      "Epoch 1298/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 18909.1320 - val_loss: 20827.0517\n",
      "Epoch 1299/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 18825.9851 - val_loss: 20733.3953\n",
      "Epoch 1300/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 18740.9067 - val_loss: 20639.7079\n",
      "Epoch 1301/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 18659.6815 - val_loss: 20543.3667\n",
      "Epoch 1302/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 18572.6098 - val_loss: 20451.7593\n",
      "Epoch 1303/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 18491.6363 - val_loss: 20355.7703\n",
      "Epoch 1304/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 18406.3834 - val_loss: 20261.8947\n",
      "Epoch 1305/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 18324.5327 - val_loss: 20164.9544\n",
      "Epoch 1306/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 18238.5219 - val_loss: 20072.7866\n",
      "Epoch 1307/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 18153.3683 - val_loss: 19981.6827\n",
      "Epoch 1308/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 18070.9677 - val_loss: 19886.2114\n",
      "Epoch 1309/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 17987.3136 - val_loss: 19791.2782\n",
      "Epoch 1310/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 17903.2902 - val_loss: 19696.4154\n",
      "Epoch 1311/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 17819.8523 - val_loss: 19601.6236\n",
      "Epoch 1312/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 17735.2253 - val_loss: 19508.0241\n",
      "Epoch 1313/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 17652.7850 - val_loss: 19414.8373\n",
      "Epoch 1314/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 17567.4749 - val_loss: 19319.4567\n",
      "Epoch 1315/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 17483.5828 - val_loss: 19227.3790\n",
      "Epoch 1316/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 17400.3959 - val_loss: 19132.3915\n",
      "Epoch 1317/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 17315.4978 - val_loss: 19039.9443\n",
      "Epoch 1318/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 17233.8077 - val_loss: 18942.7193\n",
      "Epoch 1319/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 17148.9408 - val_loss: 18848.7652\n",
      "Epoch 1320/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 17063.7517 - val_loss: 18755.9347\n",
      "Epoch 1321/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 16982.0222 - val_loss: 18661.5891\n",
      "Epoch 1322/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 16897.3856 - val_loss: 18568.3393\n",
      "Epoch 1323/10000\n",
      "350/350 [==============================] - 0s 80us/step - loss: 16814.0964 - val_loss: 18475.1918\n",
      "Epoch 1324/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 16729.0913 - val_loss: 18383.1950\n",
      "Epoch 1325/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 16647.2541 - val_loss: 18288.8941\n",
      "Epoch 1326/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 16562.1364 - val_loss: 18196.6614\n",
      "Epoch 1327/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 16479.6987 - val_loss: 18103.0470\n",
      "Epoch 1328/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 16396.3434 - val_loss: 18007.1334\n",
      "Epoch 1329/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 16312.2674 - val_loss: 17913.4705\n",
      "Epoch 1330/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 16227.9527 - val_loss: 17820.6375\n",
      "Epoch 1331/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 16144.8183 - val_loss: 17727.3696\n",
      "Epoch 1332/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 16061.2639 - val_loss: 17634.5505\n",
      "Epoch 1333/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 15977.5279 - val_loss: 17540.8410\n",
      "Epoch 1334/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 15894.3550 - val_loss: 17445.5412\n",
      "Epoch 1335/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 15810.8630 - val_loss: 17352.3576\n",
      "Epoch 1336/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 15726.7668 - val_loss: 17258.5407\n",
      "Epoch 1337/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 15643.5272 - val_loss: 17164.0324\n",
      "Epoch 1338/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 15560.1549 - val_loss: 17070.7947\n",
      "Epoch 1339/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 15476.4848 - val_loss: 16977.6663\n",
      "Epoch 1340/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 15392.9560 - val_loss: 16886.1368\n",
      "Epoch 1341/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 15309.3328 - val_loss: 16792.6427\n",
      "Epoch 1342/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 15225.8984 - val_loss: 16698.5915\n",
      "Epoch 1343/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 15141.6170 - val_loss: 16605.9145\n",
      "Epoch 1344/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 15058.9770 - val_loss: 16512.9754\n",
      "Epoch 1345/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 14975.9872 - val_loss: 16419.5469\n",
      "Epoch 1346/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 14891.8143 - val_loss: 16327.3332\n",
      "Epoch 1347/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 14809.7322 - val_loss: 16234.1568\n",
      "Epoch 1348/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 14724.9150 - val_loss: 16144.2988\n",
      "Epoch 1349/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 14643.6395 - val_loss: 16051.5110\n",
      "Epoch 1350/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 14562.5976 - val_loss: 15956.9658\n",
      "Epoch 1351/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 14477.1937 - val_loss: 15864.7336\n",
      "Epoch 1352/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 14395.7860 - val_loss: 15770.3706\n",
      "Epoch 1353/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 14312.5713 - val_loss: 15679.5139\n",
      "Epoch 1354/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 14230.0134 - val_loss: 15586.5079\n",
      "Epoch 1355/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 14146.7126 - val_loss: 15494.6598\n",
      "Epoch 1356/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 14064.0196 - val_loss: 15403.9270\n",
      "Epoch 1357/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 13982.3868 - val_loss: 15309.9212\n",
      "Epoch 1358/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 13899.8040 - val_loss: 15216.2305\n",
      "Epoch 1359/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 13817.1723 - val_loss: 15124.7216\n",
      "Epoch 1360/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 13733.6900 - val_loss: 15031.7223\n",
      "Epoch 1361/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 13652.3507 - val_loss: 14940.1101\n",
      "Epoch 1362/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 13567.7938 - val_loss: 14850.6700\n",
      "Epoch 1363/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 13488.9215 - val_loss: 14757.9307\n",
      "Epoch 1364/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 13405.8088 - val_loss: 14664.7696\n",
      "Epoch 1365/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 13323.2571 - val_loss: 14574.5304\n",
      "Epoch 1366/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 13241.5665 - val_loss: 14481.0847\n",
      "Epoch 1367/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 13159.4153 - val_loss: 14388.5447\n",
      "Epoch 1368/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 13078.5286 - val_loss: 14295.8136\n",
      "Epoch 1369/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 12994.7189 - val_loss: 14205.7894\n",
      "Epoch 1370/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 12914.5671 - val_loss: 14115.4285\n",
      "Epoch 1371/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 12831.0306 - val_loss: 14025.1173\n",
      "Epoch 1372/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 12750.7451 - val_loss: 13932.6579\n",
      "Epoch 1373/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 12669.0660 - val_loss: 13839.6025\n",
      "Epoch 1374/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 12588.0905 - val_loss: 13749.0194\n",
      "Epoch 1375/10000\n",
      "350/350 [==============================] - 0s 78us/step - loss: 12506.1535 - val_loss: 13657.5427\n",
      "Epoch 1376/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 12423.4177 - val_loss: 13570.5681\n",
      "Epoch 1377/10000\n",
      "350/350 [==============================] - 0s 76us/step - loss: 12344.5238 - val_loss: 13479.5646\n",
      "Epoch 1378/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 12261.3251 - val_loss: 13391.2183\n",
      "Epoch 1379/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 12182.2573 - val_loss: 13302.1787\n",
      "Epoch 1380/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 12101.6599 - val_loss: 13209.7089\n",
      "Epoch 1381/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 12019.3205 - val_loss: 13121.2202\n",
      "Epoch 1382/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 11938.6752 - val_loss: 13031.2794\n",
      "Epoch 1383/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 11857.5329 - val_loss: 12942.9981\n",
      "Epoch 1384/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 11778.4914 - val_loss: 12852.8064\n",
      "Epoch 1385/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 11696.1063 - val_loss: 12765.5580\n",
      "Epoch 1386/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 11616.8421 - val_loss: 12676.0233\n",
      "Epoch 1387/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 11535.2821 - val_loss: 12587.2121\n",
      "Epoch 1388/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 11454.7686 - val_loss: 12497.6369\n",
      "Epoch 1389/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 11375.5499 - val_loss: 12410.2304\n",
      "Epoch 1390/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 11294.1140 - val_loss: 12320.2883\n",
      "Epoch 1391/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 11214.8628 - val_loss: 12229.5423\n",
      "Epoch 1392/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 11132.6185 - val_loss: 12141.6600\n",
      "Epoch 1393/10000\n",
      "350/350 [==============================] - 0s 84us/step - loss: 11053.0362 - val_loss: 12055.1678\n",
      "Epoch 1394/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 10973.8173 - val_loss: 11964.7618\n",
      "Epoch 1395/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 10891.4935 - val_loss: 11875.0564\n",
      "Epoch 1396/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 10811.7872 - val_loss: 11784.1512\n",
      "Epoch 1397/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 10730.6358 - val_loss: 11696.0212\n",
      "Epoch 1398/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 10651.5449 - val_loss: 11608.4173\n",
      "Epoch 1399/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 10570.0869 - val_loss: 11519.2183\n",
      "Epoch 1400/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 10490.3621 - val_loss: 11430.4997\n",
      "Epoch 1401/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 10410.4349 - val_loss: 11340.2334\n",
      "Epoch 1402/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 10330.3187 - val_loss: 11253.0415\n",
      "Epoch 1403/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 10250.5393 - val_loss: 11166.0250\n",
      "Epoch 1404/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 10171.6129 - val_loss: 11074.7314\n",
      "Epoch 1405/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 10091.2033 - val_loss: 10989.5789\n",
      "Epoch 1406/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 10012.1341 - val_loss: 10898.9147\n",
      "Epoch 1407/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 9931.9869 - val_loss: 10812.3605\n",
      "Epoch 1408/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 9850.9199 - val_loss: 10726.7304\n",
      "Epoch 1409/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 9772.2769 - val_loss: 10636.8579\n",
      "Epoch 1410/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 9694.1651 - val_loss: 10548.7100\n",
      "Epoch 1411/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 9613.7148 - val_loss: 10461.0026\n",
      "Epoch 1412/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 9534.6457 - val_loss: 10372.0425\n",
      "Epoch 1413/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 9455.0949 - val_loss: 10282.9304\n",
      "Epoch 1414/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 9374.0922 - val_loss: 10195.7841\n",
      "Epoch 1415/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 9295.2440 - val_loss: 10110.4275\n",
      "Epoch 1416/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 9215.9186 - val_loss: 10024.3491\n",
      "Epoch 1417/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 9135.7944 - val_loss: 9938.7522\n",
      "Epoch 1418/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 9057.8398 - val_loss: 9850.1573\n",
      "Epoch 1419/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 8977.6939 - val_loss: 9764.7917\n",
      "Epoch 1420/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 8897.7172 - val_loss: 9677.0753\n",
      "Epoch 1421/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 8818.6517 - val_loss: 9588.6781\n",
      "Epoch 1422/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 8740.7310 - val_loss: 9501.4902\n",
      "Epoch 1423/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 8660.7207 - val_loss: 9415.1040\n",
      "Epoch 1424/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 63us/step - loss: 8581.1234 - val_loss: 9328.4695\n",
      "Epoch 1425/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 8502.0673 - val_loss: 9241.7569\n",
      "Epoch 1426/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 8423.0332 - val_loss: 9155.1114\n",
      "Epoch 1427/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 8344.8467 - val_loss: 9068.3193\n",
      "Epoch 1428/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 8265.3805 - val_loss: 8984.8762\n",
      "Epoch 1429/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 8186.0418 - val_loss: 8899.6929\n",
      "Epoch 1430/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 8107.0135 - val_loss: 8813.1661\n",
      "Epoch 1431/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 8028.4315 - val_loss: 8724.7752\n",
      "Epoch 1432/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 7949.9029 - val_loss: 8638.8486\n",
      "Epoch 1433/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 7870.7493 - val_loss: 8554.2765\n",
      "Epoch 1434/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 7792.7543 - val_loss: 8465.5004\n",
      "Epoch 1435/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 7711.6374 - val_loss: 8381.3717\n",
      "Epoch 1436/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 7635.9277 - val_loss: 8294.5588\n",
      "Epoch 1437/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 7557.0879 - val_loss: 8210.2708\n",
      "Epoch 1438/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 7480.0713 - val_loss: 8122.4068\n",
      "Epoch 1439/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 7402.0395 - val_loss: 8034.3937\n",
      "Epoch 1440/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 7323.4506 - val_loss: 7949.0903\n",
      "Epoch 1441/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 7245.8830 - val_loss: 7864.3078\n",
      "Epoch 1442/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 7168.6068 - val_loss: 7779.7567\n",
      "Epoch 1443/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 7090.0798 - val_loss: 7692.7918\n",
      "Epoch 1444/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 7013.8309 - val_loss: 7605.1990\n",
      "Epoch 1445/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 6935.6925 - val_loss: 7520.8829\n",
      "Epoch 1446/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 6858.5447 - val_loss: 7434.0763\n",
      "Epoch 1447/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 6779.3599 - val_loss: 7350.0573\n",
      "Epoch 1448/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 6703.0203 - val_loss: 7265.1629\n",
      "Epoch 1449/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 6624.8007 - val_loss: 7184.3139\n",
      "Epoch 1450/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 6549.3745 - val_loss: 7096.5177\n",
      "Epoch 1451/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 6469.8436 - val_loss: 7010.8235\n",
      "Epoch 1452/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 6393.2069 - val_loss: 6928.8045\n",
      "Epoch 1453/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 6316.3254 - val_loss: 6841.2332\n",
      "Epoch 1454/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 6239.0975 - val_loss: 6755.6688\n",
      "Epoch 1455/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 6160.6097 - val_loss: 6674.1469\n",
      "Epoch 1456/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 6084.5839 - val_loss: 6587.3844\n",
      "Epoch 1457/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 6005.5680 - val_loss: 6503.9795\n",
      "Epoch 1458/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 5929.9902 - val_loss: 6417.9240\n",
      "Epoch 1459/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 5852.1045 - val_loss: 6333.4373\n",
      "Epoch 1460/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 5775.5223 - val_loss: 6247.3227\n",
      "Epoch 1461/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 5698.0063 - val_loss: 6162.5158\n",
      "Epoch 1462/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 5620.3352 - val_loss: 6079.4824\n",
      "Epoch 1463/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 5544.6952 - val_loss: 5993.6940\n",
      "Epoch 1464/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 5467.7885 - val_loss: 5908.0110\n",
      "Epoch 1465/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 5390.1556 - val_loss: 5825.1035\n",
      "Epoch 1466/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 5312.3016 - val_loss: 5743.2004\n",
      "Epoch 1467/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 5236.2553 - val_loss: 5657.9518\n",
      "Epoch 1468/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 5159.9776 - val_loss: 5575.2874\n",
      "Epoch 1469/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 5083.9777 - val_loss: 5492.7194\n",
      "Epoch 1470/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 5006.7982 - val_loss: 5410.2669\n",
      "Epoch 1471/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 4933.6274 - val_loss: 5325.9304\n",
      "Epoch 1472/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 4854.0340 - val_loss: 5245.0971\n",
      "Epoch 1473/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 4780.1715 - val_loss: 5159.9768\n",
      "Epoch 1474/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 4703.4299 - val_loss: 5079.1352\n",
      "Epoch 1475/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 4627.8251 - val_loss: 4998.4769\n",
      "Epoch 1476/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 4552.3092 - val_loss: 4917.7150\n",
      "Epoch 1477/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 4477.2129 - val_loss: 4838.7519\n",
      "Epoch 1478/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 4401.5335 - val_loss: 4757.2175\n",
      "Epoch 1479/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 4326.9229 - val_loss: 4676.0223\n",
      "Epoch 1480/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 4249.9364 - val_loss: 4597.1816\n",
      "Epoch 1481/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 4176.1589 - val_loss: 4515.5163\n",
      "Epoch 1482/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 4101.7287 - val_loss: 4432.4019\n",
      "Epoch 1483/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 4025.4613 - val_loss: 4350.8369\n",
      "Epoch 1484/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3950.2066 - val_loss: 4273.5431\n",
      "Epoch 1485/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 3876.2191 - val_loss: 4191.7478\n",
      "Epoch 1486/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3800.8985 - val_loss: 4112.8830\n",
      "Epoch 1487/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3726.5915 - val_loss: 4031.7105\n",
      "Epoch 1488/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 3650.8123 - val_loss: 3951.4369\n",
      "Epoch 1489/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3577.3452 - val_loss: 3872.1783\n",
      "Epoch 1490/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3503.5668 - val_loss: 3793.8599\n",
      "Epoch 1491/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3430.2898 - val_loss: 3717.2840\n",
      "Epoch 1492/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3357.6323 - val_loss: 3639.1218\n",
      "Epoch 1493/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3284.8343 - val_loss: 3556.5224\n",
      "Epoch 1494/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3210.8332 - val_loss: 3478.3328\n",
      "Epoch 1495/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3138.9273 - val_loss: 3399.8739\n",
      "Epoch 1496/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3065.5162 - val_loss: 3323.1965\n",
      "Epoch 1497/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2993.7304 - val_loss: 3244.5986\n",
      "Epoch 1498/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2923.0552 - val_loss: 3165.1902\n",
      "Epoch 1499/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2849.3606 - val_loss: 3086.4842\n",
      "Epoch 1500/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2778.9621 - val_loss: 3007.4555\n",
      "Epoch 1501/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2707.7066 - val_loss: 2928.0110\n",
      "Epoch 1502/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2635.5912 - val_loss: 2851.1725\n",
      "Epoch 1503/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2563.2352 - val_loss: 2774.9031\n",
      "Epoch 1504/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2493.8676 - val_loss: 2694.8964\n",
      "Epoch 1505/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2423.2133 - val_loss: 2617.5191\n",
      "Epoch 1506/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2351.7120 - val_loss: 2538.6980\n",
      "Epoch 1507/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2280.1620 - val_loss: 2463.6424\n",
      "Epoch 1508/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2209.4660 - val_loss: 2387.1331\n",
      "Epoch 1509/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2139.7993 - val_loss: 2307.9909\n",
      "Epoch 1510/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2069.4277 - val_loss: 2230.5474\n",
      "Epoch 1511/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1999.4904 - val_loss: 2152.0052\n",
      "Epoch 1512/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 1927.8166 - val_loss: 2076.0091\n",
      "Epoch 1513/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1858.7518 - val_loss: 1999.0705\n",
      "Epoch 1514/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1790.2517 - val_loss: 1920.1020\n",
      "Epoch 1515/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1717.8179 - val_loss: 1845.3537\n",
      "Epoch 1516/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1649.0711 - val_loss: 1768.2890\n",
      "Epoch 1517/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1579.8183 - val_loss: 1692.1598\n",
      "Epoch 1518/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1510.8000 - val_loss: 1614.6284\n",
      "Epoch 1519/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1441.0136 - val_loss: 1537.5032\n",
      "Epoch 1520/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1372.8847 - val_loss: 1462.2049\n",
      "Epoch 1521/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 1304.8724 - val_loss: 1383.5862\n",
      "Epoch 1522/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1233.8829 - val_loss: 1310.6949\n",
      "Epoch 1523/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1167.0040 - val_loss: 1236.3426\n",
      "Epoch 1524/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1098.3444 - val_loss: 1162.7385\n",
      "Epoch 1525/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1030.2035 - val_loss: 1085.6576\n",
      "Epoch 1526/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 963.9398 - val_loss: 1008.2239\n",
      "Epoch 1527/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 893.5763 - val_loss: 935.4213\n",
      "Epoch 1528/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 827.3686 - val_loss: 861.5282\n",
      "Epoch 1529/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 758.1434 - val_loss: 788.3832\n",
      "Epoch 1530/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 691.9948 - val_loss: 712.6712\n",
      "Epoch 1531/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 623.6324 - val_loss: 640.1798\n",
      "Epoch 1532/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 556.8735 - val_loss: 564.4545\n",
      "Epoch 1533/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 489.0507 - val_loss: 493.8227\n",
      "Epoch 1534/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 423.1098 - val_loss: 420.7697\n",
      "Epoch 1535/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 355.2832 - val_loss: 347.2944\n",
      "Epoch 1536/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 289.6040 - val_loss: 272.6066\n",
      "Epoch 1537/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 222.3678 - val_loss: 200.7754\n",
      "Epoch 1538/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 156.3463 - val_loss: 130.7130\n",
      "Epoch 1539/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 91.4233 - val_loss: 57.7060\n",
      "Epoch 1540/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 27.4600 - val_loss: 9.3259\n",
      "Epoch 1541/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 15.7641 - val_loss: 10.3927\n",
      "Epoch 1542/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 6.7947 - val_loss: 7.2497\n",
      "Epoch 1543/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 4.5958 - val_loss: 3.4160\n",
      "Epoch 1544/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.5221 - val_loss: 4.6047\n",
      "Epoch 1545/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 4.1628 - val_loss: 3.1586\n",
      "Epoch 1546/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.0258 - val_loss: 3.2070\n",
      "Epoch 1547/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 3.2907 - val_loss: 3.1480\n",
      "Epoch 1548/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9684 - val_loss: 3.7317\n",
      "Epoch 1549/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4528 - val_loss: 3.6330\n",
      "Epoch 1550/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.9850 - val_loss: 3.6904\n",
      "Epoch 1551/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.0876 - val_loss: 2.9401\n",
      "Epoch 1552/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.3137 - val_loss: 3.1780\n",
      "Epoch 1553/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.6782 - val_loss: 2.4884\n",
      "Epoch 1554/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8886 - val_loss: 4.0471\n",
      "Epoch 1555/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.8010 - val_loss: 4.8131\n",
      "Epoch 1556/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.8892 - val_loss: 4.1883\n",
      "Epoch 1557/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7770 - val_loss: 3.4048\n",
      "Epoch 1558/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9837 - val_loss: 4.6310\n",
      "Epoch 1559/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6802 - val_loss: 2.0034\n",
      "Epoch 1560/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8770 - val_loss: 2.1716\n",
      "Epoch 1561/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3061 - val_loss: 2.5302\n",
      "Epoch 1562/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.6782 - val_loss: 4.2535\n",
      "Epoch 1563/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 3.6865 - val_loss: 2.9932\n",
      "Epoch 1564/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 3.4810 - val_loss: 4.2605\n",
      "Epoch 1565/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.9073 - val_loss: 2.7706\n",
      "Epoch 1566/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.8071 - val_loss: 2.3490\n",
      "Epoch 1567/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 1.9286 - val_loss: 2.2059\n",
      "Epoch 1568/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.4691 - val_loss: 2.1766\n",
      "Epoch 1569/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.0892 - val_loss: 1.9128\n",
      "Epoch 1570/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 74us/step - loss: 2.4999 - val_loss: 2.5754\n",
      "Epoch 1571/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.9121 - val_loss: 2.5985\n",
      "Epoch 1572/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9222 - val_loss: 1.7723\n",
      "Epoch 1573/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0618 - val_loss: 2.1905\n",
      "Epoch 1574/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9571 - val_loss: 2.3400\n",
      "Epoch 1575/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2923 - val_loss: 2.9317\n",
      "Epoch 1576/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7028 - val_loss: 1.9216\n",
      "Epoch 1577/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.4257 - val_loss: 1.5479\n",
      "Epoch 1578/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.8993 - val_loss: 1.9580\n",
      "Epoch 1579/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.1175 - val_loss: 2.2176\n",
      "Epoch 1580/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4688 - val_loss: 2.9971\n",
      "Epoch 1581/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.5535 - val_loss: 5.1019\n",
      "Epoch 1582/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.9146 - val_loss: 2.7017\n",
      "Epoch 1583/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.4641 - val_loss: 2.2564\n",
      "Epoch 1584/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7723 - val_loss: 3.3979\n",
      "Epoch 1585/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.1287 - val_loss: 4.0712\n",
      "Epoch 1586/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.6974 - val_loss: 4.1657\n",
      "Epoch 1587/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.1049 - val_loss: 3.0594\n",
      "Epoch 1588/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.1268 - val_loss: 2.7411\n",
      "Epoch 1589/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.9071 - val_loss: 2.4045\n",
      "Epoch 1590/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.1846 - val_loss: 2.9403\n",
      "Epoch 1591/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.7305 - val_loss: 2.7540\n",
      "Epoch 1592/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3730 - val_loss: 2.3405\n",
      "Epoch 1593/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4096 - val_loss: 2.1633\n",
      "Epoch 1594/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.6218 - val_loss: 2.9239\n",
      "Epoch 1595/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.7116 - val_loss: 2.9950\n",
      "Epoch 1596/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.8561 - val_loss: 1.7656\n",
      "Epoch 1597/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0106 - val_loss: 3.4712\n",
      "Epoch 1598/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9972 - val_loss: 2.4943\n",
      "Epoch 1599/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8872 - val_loss: 2.0859\n",
      "Epoch 1600/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2417 - val_loss: 2.5320\n",
      "Epoch 1601/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.3846 - val_loss: 2.6308\n",
      "Epoch 1602/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8976 - val_loss: 3.7709\n",
      "Epoch 1603/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.7893 - val_loss: 2.1085\n",
      "Epoch 1604/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6093 - val_loss: 2.9724\n",
      "Epoch 1605/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3967 - val_loss: 1.6483\n",
      "Epoch 1606/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1763 - val_loss: 2.7394\n",
      "Epoch 1607/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8139 - val_loss: 3.7927\n",
      "Epoch 1608/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.6594 - val_loss: 2.5730\n",
      "Epoch 1609/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.0528 - val_loss: 2.6601\n",
      "Epoch 1610/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8021 - val_loss: 2.8864\n",
      "Epoch 1611/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9578 - val_loss: 2.0232\n",
      "Epoch 1612/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9153 - val_loss: 3.1254\n",
      "Epoch 1613/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.6428 - val_loss: 2.8675\n",
      "Epoch 1614/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.1129 - val_loss: 2.7660\n",
      "Epoch 1615/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3494 - val_loss: 2.4781\n",
      "Epoch 1616/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2963 - val_loss: 1.9075\n",
      "Epoch 1617/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6807 - val_loss: 2.7701\n",
      "Epoch 1618/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0120 - val_loss: 2.4604\n",
      "Epoch 1619/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.1213 - val_loss: 2.8424\n",
      "Epoch 1620/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1001 - val_loss: 2.8929\n",
      "Epoch 1621/10000\n",
      "350/350 [==============================] - ETA: 0s - loss: 2.136 - 0s 62us/step - loss: 2.4281 - val_loss: 2.0141\n",
      "Epoch 1622/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5437 - val_loss: 4.0978\n",
      "Epoch 1623/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.8485 - val_loss: 5.8726\n",
      "Epoch 1624/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 4.6822 - val_loss: 4.2678\n",
      "Epoch 1625/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 3.6071 - val_loss: 2.7659\n",
      "Epoch 1626/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1833 - val_loss: 3.5984\n",
      "Epoch 1627/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.5503 - val_loss: 3.5037\n",
      "Epoch 1628/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6847 - val_loss: 2.8131\n",
      "Epoch 1629/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8118 - val_loss: 3.9954\n",
      "Epoch 1630/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 3.0874 - val_loss: 2.2655\n",
      "Epoch 1631/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.9489 - val_loss: 4.2277\n",
      "Epoch 1632/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.6016 - val_loss: 2.5434\n",
      "Epoch 1633/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2693 - val_loss: 2.3327\n",
      "Epoch 1634/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.2466 - val_loss: 3.4800\n",
      "Epoch 1635/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.1862 - val_loss: 2.1218\n",
      "Epoch 1636/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0127 - val_loss: 1.6795\n",
      "Epoch 1637/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5198 - val_loss: 1.8754\n",
      "Epoch 1638/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.6134 - val_loss: 2.1378\n",
      "Epoch 1639/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9193 - val_loss: 3.2663\n",
      "Epoch 1640/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.7753 - val_loss: 4.2567\n",
      "Epoch 1641/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 4.8340 - val_loss: 3.7847\n",
      "Epoch 1642/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8651 - val_loss: 3.5962\n",
      "Epoch 1643/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0597 - val_loss: 1.9721\n",
      "Epoch 1644/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.0330 - val_loss: 1.7441\n",
      "Epoch 1645/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2880 - val_loss: 1.9995\n",
      "Epoch 1646/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.6715 - val_loss: 3.1752\n",
      "Epoch 1647/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6879 - val_loss: 2.5714\n",
      "Epoch 1648/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 2.4817 - val_loss: 1.9670\n",
      "Epoch 1649/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.3258 - val_loss: 3.1026\n",
      "Epoch 1650/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.0081 - val_loss: 3.2911\n",
      "Epoch 1651/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4419 - val_loss: 1.9824\n",
      "Epoch 1652/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0332 - val_loss: 1.7607\n",
      "Epoch 1653/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1969 - val_loss: 3.3787\n",
      "Epoch 1654/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8383 - val_loss: 2.9147\n",
      "Epoch 1655/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0937 - val_loss: 0.9955\n",
      "Epoch 1656/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.5525 - val_loss: 1.2949\n",
      "Epoch 1657/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.2812 - val_loss: 2.1075\n",
      "Epoch 1658/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2962 - val_loss: 3.2508\n",
      "Epoch 1659/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2208 - val_loss: 2.0610\n",
      "Epoch 1660/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9097 - val_loss: 1.4754\n",
      "Epoch 1661/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8178 - val_loss: 2.1767\n",
      "Epoch 1662/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 2.0014 - val_loss: 3.2749\n",
      "Epoch 1663/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3829 - val_loss: 1.8349\n",
      "Epoch 1664/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0190 - val_loss: 1.5649\n",
      "Epoch 1665/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8054 - val_loss: 2.9366\n",
      "Epoch 1666/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3489 - val_loss: 1.8764\n",
      "Epoch 1667/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6552 - val_loss: 2.1833\n",
      "Epoch 1668/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5936 - val_loss: 2.3841\n",
      "Epoch 1669/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6384 - val_loss: 2.9873\n",
      "Epoch 1670/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9071 - val_loss: 2.9633\n",
      "Epoch 1671/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3907 - val_loss: 2.4381\n",
      "Epoch 1672/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1281 - val_loss: 2.8387\n",
      "Epoch 1673/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 3.0726 - val_loss: 3.6678\n",
      "Epoch 1674/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0697 - val_loss: 2.2870\n",
      "Epoch 1675/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.5387 - val_loss: 2.0583\n",
      "Epoch 1676/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2394 - val_loss: 2.3498\n",
      "Epoch 1677/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7583 - val_loss: 2.3944\n",
      "Epoch 1678/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5785 - val_loss: 3.3597\n",
      "Epoch 1679/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5716 - val_loss: 2.0221\n",
      "Epoch 1680/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2523 - val_loss: 2.0706\n",
      "Epoch 1681/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4464 - val_loss: 1.5999\n",
      "Epoch 1682/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.4554 - val_loss: 1.5270\n",
      "Epoch 1683/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.1749 - val_loss: 1.6181\n",
      "Epoch 1684/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4016 - val_loss: 3.3482\n",
      "Epoch 1685/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.3843 - val_loss: 1.8683\n",
      "Epoch 1686/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.6172 - val_loss: 2.5927\n",
      "Epoch 1687/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8943 - val_loss: 2.8240\n",
      "Epoch 1688/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3421 - val_loss: 2.2196\n",
      "Epoch 1689/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.7351 - val_loss: 3.3838\n",
      "Epoch 1690/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7342 - val_loss: 2.8164\n",
      "Epoch 1691/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.9368 - val_loss: 2.4905\n",
      "Epoch 1692/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.0675 - val_loss: 2.2574\n",
      "Epoch 1693/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.9535 - val_loss: 2.7424\n",
      "Epoch 1694/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4905 - val_loss: 1.6202\n",
      "Epoch 1695/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.8270 - val_loss: 2.4218\n",
      "Epoch 1696/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9863 - val_loss: 2.2021\n",
      "Epoch 1697/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8340 - val_loss: 2.1651\n",
      "Epoch 1698/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1106 - val_loss: 2.2062\n",
      "Epoch 1699/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8512 - val_loss: 2.4985\n",
      "Epoch 1700/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4326 - val_loss: 2.8622\n",
      "Epoch 1701/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3221 - val_loss: 2.6045\n",
      "Epoch 1702/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2155 - val_loss: 2.2324\n",
      "Epoch 1703/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1904 - val_loss: 1.6588\n",
      "Epoch 1704/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.2127 - val_loss: 4.2357\n",
      "Epoch 1705/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.6498 - val_loss: 4.2037\n",
      "Epoch 1706/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 3.5529 - val_loss: 4.6701\n",
      "Epoch 1707/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.3720 - val_loss: 2.8264\n",
      "Epoch 1708/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4456 - val_loss: 1.6678\n",
      "Epoch 1709/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.2184 - val_loss: 1.3561\n",
      "Epoch 1710/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3191 - val_loss: 2.5225\n",
      "Epoch 1711/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2970 - val_loss: 2.4391\n",
      "Epoch 1712/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5747 - val_loss: 3.0823\n",
      "Epoch 1713/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7883 - val_loss: 3.1729\n",
      "Epoch 1714/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3115 - val_loss: 1.8054\n",
      "Epoch 1715/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.9156 - val_loss: 2.7501\n",
      "Epoch 1716/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.9600 - val_loss: 1.4087\n",
      "Epoch 1717/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.7893 - val_loss: 2.0489\n",
      "Epoch 1718/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3401 - val_loss: 1.9829\n",
      "Epoch 1719/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9899 - val_loss: 1.2716\n",
      "Epoch 1720/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8685 - val_loss: 2.6541\n",
      "Epoch 1721/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.7391 - val_loss: 1.7709\n",
      "Epoch 1722/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 65us/step - loss: 1.7425 - val_loss: 2.2069\n",
      "Epoch 1723/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5306 - val_loss: 2.6980\n",
      "Epoch 1724/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4365 - val_loss: 1.1604\n",
      "Epoch 1725/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.3496 - val_loss: 1.1108\n",
      "Epoch 1726/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8600 - val_loss: 1.9305\n",
      "Epoch 1727/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9047 - val_loss: 1.7706\n",
      "Epoch 1728/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6178 - val_loss: 3.1251\n",
      "Epoch 1729/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0439 - val_loss: 2.1354\n",
      "Epoch 1730/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.6580 - val_loss: 1.8009\n",
      "Epoch 1731/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.1600 - val_loss: 0.9070\n",
      "Epoch 1732/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.7697 - val_loss: 1.5372\n",
      "Epoch 1733/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.4483 - val_loss: 1.3362\n",
      "Epoch 1734/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.4046 - val_loss: 2.0385\n",
      "Epoch 1735/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.6576 - val_loss: 1.8465\n",
      "Epoch 1736/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.1941 - val_loss: 2.5467\n",
      "Epoch 1737/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 2.6338 - val_loss: 1.4973\n",
      "Epoch 1738/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.1255 - val_loss: 3.6821\n",
      "Epoch 1739/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.7636 - val_loss: 2.1827\n",
      "Epoch 1740/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8431 - val_loss: 2.8173\n",
      "Epoch 1741/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8012 - val_loss: 1.1697\n",
      "Epoch 1742/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.3829 - val_loss: 2.4197\n",
      "Epoch 1743/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4877 - val_loss: 2.9863\n",
      "Epoch 1744/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.2450 - val_loss: 1.6898\n",
      "Epoch 1745/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.5360 - val_loss: 2.0205\n",
      "Epoch 1746/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5309 - val_loss: 1.4746\n",
      "Epoch 1747/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2748 - val_loss: 2.5165\n",
      "Epoch 1748/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5718 - val_loss: 2.5417\n",
      "Epoch 1749/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4099 - val_loss: 2.1715\n",
      "Epoch 1750/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.6164 - val_loss: 1.8745\n",
      "Epoch 1751/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7061 - val_loss: 1.4807\n",
      "Epoch 1752/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9628 - val_loss: 2.4226\n",
      "Epoch 1753/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4274 - val_loss: 1.6337\n",
      "Epoch 1754/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1937 - val_loss: 3.2221\n",
      "Epoch 1755/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5387 - val_loss: 2.5217\n",
      "Epoch 1756/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5335 - val_loss: 2.3307\n",
      "Epoch 1757/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5693 - val_loss: 2.6657\n",
      "Epoch 1758/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9120 - val_loss: 3.2364\n",
      "Epoch 1759/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4676 - val_loss: 1.5821\n",
      "Epoch 1760/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.0616 - val_loss: 1.9432\n",
      "Epoch 1761/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8071 - val_loss: 2.6581\n",
      "Epoch 1762/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0415 - val_loss: 1.5603\n",
      "Epoch 1763/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.4327 - val_loss: 1.1807\n",
      "Epoch 1764/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.6004 - val_loss: 1.7067\n",
      "Epoch 1765/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7618 - val_loss: 2.3244\n",
      "Epoch 1766/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1297 - val_loss: 1.8817\n",
      "Epoch 1767/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1124 - val_loss: 1.2083\n",
      "Epoch 1768/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.5736 - val_loss: 1.5528\n",
      "Epoch 1769/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0436 - val_loss: 0.8499\n",
      "Epoch 1770/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8559 - val_loss: 1.9452\n",
      "Epoch 1771/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2610 - val_loss: 1.9225\n",
      "Epoch 1772/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1164 - val_loss: 1.7016\n",
      "Epoch 1773/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.3605 - val_loss: 2.0433\n",
      "Epoch 1774/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3282 - val_loss: 2.5324\n",
      "Epoch 1775/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3036 - val_loss: 2.1404\n",
      "Epoch 1776/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9008 - val_loss: 2.3157\n",
      "Epoch 1777/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2005 - val_loss: 3.0543\n",
      "Epoch 1778/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.7577 - val_loss: 2.7591\n",
      "Epoch 1779/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.3734 - val_loss: 2.9192\n",
      "Epoch 1780/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.8559 - val_loss: 3.2289\n",
      "Epoch 1781/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.7279 - val_loss: 2.8766\n",
      "Epoch 1782/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0854 - val_loss: 2.0382\n",
      "Epoch 1783/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1994 - val_loss: 2.2469\n",
      "Epoch 1784/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3148 - val_loss: 2.6695\n",
      "Epoch 1785/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4472 - val_loss: 1.7248\n",
      "Epoch 1786/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2095 - val_loss: 2.5094\n",
      "Epoch 1787/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 3.5573 - val_loss: 3.4928\n",
      "Epoch 1788/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.4683 - val_loss: 3.6683\n",
      "Epoch 1789/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.8518 - val_loss: 3.2558\n",
      "Epoch 1790/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.7543 - val_loss: 2.4669\n",
      "Epoch 1791/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5923 - val_loss: 3.6957\n",
      "Epoch 1792/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9713 - val_loss: 3.5118\n",
      "Epoch 1793/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.8817 - val_loss: 2.5784\n",
      "Epoch 1794/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2283 - val_loss: 3.3166\n",
      "Epoch 1795/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3067 - val_loss: 3.0738\n",
      "Epoch 1796/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7718 - val_loss: 2.3304\n",
      "Epoch 1797/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9222 - val_loss: 1.4472\n",
      "Epoch 1798/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.3942 - val_loss: 1.4297\n",
      "Epoch 1799/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.4719 - val_loss: 3.6799\n",
      "Epoch 1800/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4034 - val_loss: 1.6729\n",
      "Epoch 1801/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9966 - val_loss: 2.3849\n",
      "Epoch 1802/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.1562 - val_loss: 2.2105\n",
      "Epoch 1803/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3864 - val_loss: 1.9671\n",
      "Epoch 1804/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.7464 - val_loss: 2.1789\n",
      "Epoch 1805/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3243 - val_loss: 2.2767\n",
      "Epoch 1806/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.7891 - val_loss: 4.3367\n",
      "Epoch 1807/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 4.3110 - val_loss: 4.0273\n",
      "Epoch 1808/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.5067 - val_loss: 2.3080\n",
      "Epoch 1809/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9568 - val_loss: 3.6014\n",
      "Epoch 1810/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.2812 - val_loss: 2.9903\n",
      "Epoch 1811/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.6846 - val_loss: 1.7187\n",
      "Epoch 1812/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2494 - val_loss: 1.8692\n",
      "Epoch 1813/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8165 - val_loss: 1.3263\n",
      "Epoch 1814/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 1.5306 - val_loss: 2.0074\n",
      "Epoch 1815/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.3533 - val_loss: 3.1553\n",
      "Epoch 1816/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.9624 - val_loss: 2.6120\n",
      "Epoch 1817/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2753 - val_loss: 2.3723\n",
      "Epoch 1818/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3228 - val_loss: 2.4267\n",
      "Epoch 1819/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2544 - val_loss: 1.5178\n",
      "Epoch 1820/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6300 - val_loss: 2.1269\n",
      "Epoch 1821/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.4221 - val_loss: 1.8177\n",
      "Epoch 1822/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0832 - val_loss: 2.3140\n",
      "Epoch 1823/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2538 - val_loss: 2.4074\n",
      "Epoch 1824/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4003 - val_loss: 3.1685\n",
      "Epoch 1825/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6514 - val_loss: 2.3823\n",
      "Epoch 1826/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2864 - val_loss: 2.4999\n",
      "Epoch 1827/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.3374 - val_loss: 2.0963\n",
      "Epoch 1828/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.7332 - val_loss: 2.9888\n",
      "Epoch 1829/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7309 - val_loss: 3.0718\n",
      "Epoch 1830/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0460 - val_loss: 0.7671\n",
      "Epoch 1831/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.5375 - val_loss: 2.0610\n",
      "Epoch 1832/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3880 - val_loss: 2.7959\n",
      "Epoch 1833/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.4792 - val_loss: 3.5510\n",
      "Epoch 1834/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.2059 - val_loss: 3.5765\n",
      "Epoch 1835/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5554 - val_loss: 3.2215\n",
      "Epoch 1836/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6923 - val_loss: 2.0544\n",
      "Epoch 1837/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.0542 - val_loss: 3.4907\n",
      "Epoch 1838/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 3.4435 - val_loss: 2.7946\n",
      "Epoch 1839/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.3024 - val_loss: 4.5185\n",
      "Epoch 1840/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2292 - val_loss: 3.5499\n",
      "Epoch 1841/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.6899 - val_loss: 3.4289\n",
      "Epoch 1842/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.1150 - val_loss: 2.6813\n",
      "Epoch 1843/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5216 - val_loss: 2.7937\n",
      "Epoch 1844/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0215 - val_loss: 1.9614\n",
      "Epoch 1845/10000\n",
      "350/350 [==============================] - ETA: 0s - loss: 2.110 - 0s 68us/step - loss: 2.0724 - val_loss: 2.9669\n",
      "Epoch 1846/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9264 - val_loss: 2.6978\n",
      "Epoch 1847/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.1093 - val_loss: 3.1027\n",
      "Epoch 1848/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6876 - val_loss: 3.3310\n",
      "Epoch 1849/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.1531 - val_loss: 3.0694\n",
      "Epoch 1850/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.7145 - val_loss: 2.8573\n",
      "Epoch 1851/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3243 - val_loss: 0.7816\n",
      "Epoch 1852/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.4785 - val_loss: 1.2244\n",
      "Epoch 1853/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8582 - val_loss: 2.3990\n",
      "Epoch 1854/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7701 - val_loss: 1.5515\n",
      "Epoch 1855/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.7270 - val_loss: 1.2540\n",
      "Epoch 1856/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5221 - val_loss: 2.0457\n",
      "Epoch 1857/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8566 - val_loss: 1.7959\n",
      "Epoch 1858/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6821 - val_loss: 1.7624\n",
      "Epoch 1859/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 1.7856 - val_loss: 1.3211\n",
      "Epoch 1860/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6598 - val_loss: 1.0626\n",
      "Epoch 1861/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.6819 - val_loss: 1.9414\n",
      "Epoch 1862/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3930 - val_loss: 3.1377\n",
      "Epoch 1863/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8622 - val_loss: 2.2997\n",
      "Epoch 1864/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9300 - val_loss: 2.2049\n",
      "Epoch 1865/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7729 - val_loss: 1.9901\n",
      "Epoch 1866/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4879 - val_loss: 3.3677\n",
      "Epoch 1867/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.6377 - val_loss: 3.6022\n",
      "Epoch 1868/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.3060 - val_loss: 3.5241\n",
      "Epoch 1869/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.7033 - val_loss: 2.8317\n",
      "Epoch 1870/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.1117 - val_loss: 2.8029\n",
      "Epoch 1871/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.7081 - val_loss: 2.7363\n",
      "Epoch 1872/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2966 - val_loss: 1.8533\n",
      "Epoch 1873/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9024 - val_loss: 2.6375\n",
      "Epoch 1874/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 65us/step - loss: 2.3115 - val_loss: 2.9944\n",
      "Epoch 1875/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9236 - val_loss: 2.1419\n",
      "Epoch 1876/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5480 - val_loss: 2.7140\n",
      "Epoch 1877/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.7076 - val_loss: 2.8334\n",
      "Epoch 1878/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.0591 - val_loss: 2.2258\n",
      "Epoch 1879/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.4980 - val_loss: 1.5720\n",
      "Epoch 1880/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2363 - val_loss: 3.7733\n",
      "Epoch 1881/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2100 - val_loss: 3.7138\n",
      "Epoch 1882/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7915 - val_loss: 2.2215\n",
      "Epoch 1883/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 2.0704 - val_loss: 1.9976\n",
      "Epoch 1884/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4179 - val_loss: 2.7999\n",
      "Epoch 1885/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1803 - val_loss: 2.7785\n",
      "Epoch 1886/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2966 - val_loss: 3.2570\n",
      "Epoch 1887/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.0005 - val_loss: 3.1589\n",
      "Epoch 1888/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.9338 - val_loss: 2.4020\n",
      "Epoch 1889/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3270 - val_loss: 1.6831\n",
      "Epoch 1890/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.0228 - val_loss: 2.0362\n",
      "Epoch 1891/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.5943 - val_loss: 2.0357\n",
      "Epoch 1892/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.1930 - val_loss: 1.2634\n",
      "Epoch 1893/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6658 - val_loss: 1.5793\n",
      "Epoch 1894/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9114 - val_loss: 1.7293\n",
      "Epoch 1895/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.3271 - val_loss: 1.2527\n",
      "Epoch 1896/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 1.3676 - val_loss: 1.8115\n",
      "Epoch 1897/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.7445 - val_loss: 2.5309\n",
      "Epoch 1898/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9853 - val_loss: 2.5837\n",
      "Epoch 1899/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8962 - val_loss: 1.5452\n",
      "Epoch 1900/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7814 - val_loss: 1.5351\n",
      "Epoch 1901/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.9296 - val_loss: 2.1124\n",
      "Epoch 1902/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5688 - val_loss: 2.6952\n",
      "Epoch 1903/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3356 - val_loss: 3.5341\n",
      "Epoch 1904/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4319 - val_loss: 1.2027\n",
      "Epoch 1905/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1201 - val_loss: 2.2762\n",
      "Epoch 1906/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5843 - val_loss: 2.1348\n",
      "Epoch 1907/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.0154 - val_loss: 3.2140\n",
      "Epoch 1908/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5017 - val_loss: 1.4338\n",
      "Epoch 1909/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.5493 - val_loss: 1.9408\n",
      "Epoch 1910/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9033 - val_loss: 2.2431\n",
      "Epoch 1911/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8572 - val_loss: 2.2950\n",
      "Epoch 1912/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.1617 - val_loss: 1.7400\n",
      "Epoch 1913/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7631 - val_loss: 1.4323\n",
      "Epoch 1914/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5914 - val_loss: 3.3565\n",
      "Epoch 1915/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8607 - val_loss: 2.3811\n",
      "Epoch 1916/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8030 - val_loss: 3.9519\n",
      "Epoch 1917/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.8173 - val_loss: 2.9782\n",
      "Epoch 1918/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5256 - val_loss: 1.7096\n",
      "Epoch 1919/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6991 - val_loss: 3.9179\n",
      "Epoch 1920/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 3.8332 - val_loss: 2.6831\n",
      "Epoch 1921/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0131 - val_loss: 4.0944\n",
      "Epoch 1922/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5502 - val_loss: 3.1706\n",
      "Epoch 1923/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8466 - val_loss: 2.9838\n",
      "Epoch 1924/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8297 - val_loss: 1.8259\n",
      "Epoch 1925/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6212 - val_loss: 1.8140\n",
      "Epoch 1926/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.5232 - val_loss: 1.7561\n",
      "Epoch 1927/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.4074 - val_loss: 1.7737\n",
      "Epoch 1928/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.6597 - val_loss: 2.3519\n",
      "Epoch 1929/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.3051 - val_loss: 2.2052\n",
      "Epoch 1930/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0965 - val_loss: 2.7634\n",
      "Epoch 1931/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3173 - val_loss: 1.9323\n",
      "Epoch 1932/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4488 - val_loss: 1.9726\n",
      "Epoch 1933/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3605 - val_loss: 2.4281\n",
      "Epoch 1934/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.8004 - val_loss: 3.4413\n",
      "Epoch 1935/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.6721 - val_loss: 2.6611\n",
      "Epoch 1936/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9227 - val_loss: 3.0080\n",
      "Epoch 1937/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8320 - val_loss: 1.9962\n",
      "Epoch 1938/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5974 - val_loss: 2.0667\n",
      "Epoch 1939/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.6429 - val_loss: 0.9987\n",
      "Epoch 1940/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.4101 - val_loss: 0.9866\n",
      "Epoch 1941/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.1455 - val_loss: 1.3733\n",
      "Epoch 1942/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.4495 - val_loss: 1.6723\n",
      "Epoch 1943/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2974 - val_loss: 2.2085\n",
      "Epoch 1944/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9960 - val_loss: 3.0229\n",
      "Epoch 1945/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6271 - val_loss: 2.1006\n",
      "Epoch 1946/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3132 - val_loss: 1.7324\n",
      "Epoch 1947/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2957 - val_loss: 3.1074\n",
      "Epoch 1948/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.5570 - val_loss: 2.4738\n",
      "Epoch 1949/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.6645 - val_loss: 2.4813\n",
      "Epoch 1950/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8369 - val_loss: 1.1699\n",
      "Epoch 1951/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.6069 - val_loss: 2.7723\n",
      "Epoch 1952/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5059 - val_loss: 2.3609\n",
      "Epoch 1953/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4506 - val_loss: 1.8329\n",
      "Epoch 1954/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9765 - val_loss: 2.6107\n",
      "Epoch 1955/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1795 - val_loss: 2.2411\n",
      "Epoch 1956/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0990 - val_loss: 3.3061\n",
      "Epoch 1957/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.1927 - val_loss: 3.0111\n",
      "Epoch 1958/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3770 - val_loss: 3.1641\n",
      "Epoch 1959/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8716 - val_loss: 0.7024\n",
      "Epoch 1960/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8162 - val_loss: 2.0526\n",
      "Epoch 1961/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.0892 - val_loss: 2.8808\n",
      "Epoch 1962/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2737 - val_loss: 2.8726\n",
      "Epoch 1963/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8372 - val_loss: 3.1453\n",
      "Epoch 1964/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.3625 - val_loss: 2.8973\n",
      "Epoch 1965/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5581 - val_loss: 2.7600\n",
      "Epoch 1966/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9407 - val_loss: 2.2718\n",
      "Epoch 1967/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1892 - val_loss: 2.1647\n",
      "Epoch 1968/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1908 - val_loss: 2.9841\n",
      "Epoch 1969/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6157 - val_loss: 3.5604\n",
      "Epoch 1970/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.9143 - val_loss: 2.8942\n",
      "Epoch 1971/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5042 - val_loss: 3.6932\n",
      "Epoch 1972/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6670 - val_loss: 2.1442\n",
      "Epoch 1973/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9027 - val_loss: 1.4007\n",
      "Epoch 1974/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2080 - val_loss: 2.7638\n",
      "Epoch 1975/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.4776 - val_loss: 2.3956\n",
      "Epoch 1976/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 4.1530 - val_loss: 2.7271\n",
      "Epoch 1977/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.7594 - val_loss: 1.9494\n",
      "Epoch 1978/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.0708 - val_loss: 2.2944\n",
      "Epoch 1979/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2167 - val_loss: 2.7644\n",
      "Epoch 1980/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9321 - val_loss: 2.4904\n",
      "Epoch 1981/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.3373 - val_loss: 4.3754\n",
      "Epoch 1982/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.3315 - val_loss: 2.6006\n",
      "Epoch 1983/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4195 - val_loss: 2.6782\n",
      "Epoch 1984/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5104 - val_loss: 1.8945\n",
      "Epoch 1985/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4224 - val_loss: 3.3529\n",
      "Epoch 1986/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.6296 - val_loss: 3.0601\n",
      "Epoch 1987/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6423 - val_loss: 1.9468\n",
      "Epoch 1988/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.5745 - val_loss: 1.8576\n",
      "Epoch 1989/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1647 - val_loss: 2.1724\n",
      "Epoch 1990/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9341 - val_loss: 1.7933\n",
      "Epoch 1991/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7174 - val_loss: 1.6840\n",
      "Epoch 1992/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1803 - val_loss: 3.1614\n",
      "Epoch 1993/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.5330 - val_loss: 2.6617\n",
      "Epoch 1994/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.7836 - val_loss: 3.0741\n",
      "Epoch 1995/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9041 - val_loss: 3.4645\n",
      "Epoch 1996/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.3581 - val_loss: 1.7592\n",
      "Epoch 1997/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.5566 - val_loss: 1.4624\n",
      "Epoch 1998/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.7558 - val_loss: 2.3486\n",
      "Epoch 1999/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5471 - val_loss: 1.9835\n",
      "Epoch 2000/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0267 - val_loss: 2.0612\n",
      "Epoch 2001/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2447 - val_loss: 1.7439\n",
      "Epoch 2002/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0898 - val_loss: 2.2097\n",
      "Epoch 2003/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0500 - val_loss: 1.7772\n",
      "Epoch 2004/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8533 - val_loss: 1.7482\n",
      "Epoch 2005/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.5818 - val_loss: 1.1761\n",
      "Epoch 2006/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.4116 - val_loss: 1.8540\n",
      "Epoch 2007/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9864 - val_loss: 2.3142\n",
      "Epoch 2008/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3333 - val_loss: 2.7502\n",
      "Epoch 2009/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2861 - val_loss: 1.5341\n",
      "Epoch 2010/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.1106 - val_loss: 4.9767\n",
      "Epoch 2011/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8378 - val_loss: 3.3277\n",
      "Epoch 2012/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7941 - val_loss: 3.1910\n",
      "Epoch 2013/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.7437 - val_loss: 3.4128\n",
      "Epoch 2014/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.5855 - val_loss: 3.2011\n",
      "Epoch 2015/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7698 - val_loss: 2.3846\n",
      "Epoch 2016/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2669 - val_loss: 1.5865\n",
      "Epoch 2017/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7704 - val_loss: 1.7197\n",
      "Epoch 2018/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6495 - val_loss: 2.7651\n",
      "Epoch 2019/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.6354 - val_loss: 1.0154\n",
      "Epoch 2020/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9392 - val_loss: 1.2721\n",
      "Epoch 2021/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.9642 - val_loss: 2.9764\n",
      "Epoch 2022/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 3.0898 - val_loss: 2.8727\n",
      "Epoch 2023/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.7745 - val_loss: 2.5557\n",
      "Epoch 2024/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6549 - val_loss: 2.0799\n",
      "Epoch 2025/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.4398 - val_loss: 2.2240\n",
      "Epoch 2026/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 63us/step - loss: 1.6397 - val_loss: 1.3369\n",
      "Epoch 2027/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.6192 - val_loss: 2.1650\n",
      "Epoch 2028/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0456 - val_loss: 2.6827\n",
      "Epoch 2029/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.8997 - val_loss: 1.8519\n",
      "Epoch 2030/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7763 - val_loss: 2.7228\n",
      "Epoch 2031/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.5171 - val_loss: 2.2610\n",
      "Epoch 2032/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1994 - val_loss: 2.0832\n",
      "Epoch 2033/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6429 - val_loss: 2.1422\n",
      "Epoch 2034/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.7620 - val_loss: 1.6712\n",
      "Epoch 2035/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4779 - val_loss: 2.8727\n",
      "Epoch 2036/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8142 - val_loss: 1.5615\n",
      "Epoch 2037/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.9856 - val_loss: 1.7707\n",
      "Epoch 2038/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5013 - val_loss: 1.8124\n",
      "Epoch 2039/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9891 - val_loss: 2.9841\n",
      "Epoch 2040/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.6286 - val_loss: 2.0887\n",
      "Epoch 2041/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.1401 - val_loss: 2.2417\n",
      "Epoch 2042/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0920 - val_loss: 2.2788\n",
      "Epoch 2043/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3666 - val_loss: 3.1310\n",
      "Epoch 2044/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.9950 - val_loss: 2.8770\n",
      "Epoch 2045/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0283 - val_loss: 3.6156\n",
      "Epoch 2046/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.1459 - val_loss: 4.1638\n",
      "Epoch 2047/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.6114 - val_loss: 3.3902\n",
      "Epoch 2048/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3713 - val_loss: 2.4047\n",
      "Epoch 2049/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0174 - val_loss: 2.3788\n",
      "Epoch 2050/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3320 - val_loss: 1.6781\n",
      "Epoch 2051/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7788 - val_loss: 2.0950\n",
      "Epoch 2052/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.7770 - val_loss: 1.2888\n",
      "Epoch 2053/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.6869 - val_loss: 1.6720\n",
      "Epoch 2054/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9012 - val_loss: 2.0750\n",
      "Epoch 2055/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.6877 - val_loss: 1.9462\n",
      "Epoch 2056/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6227 - val_loss: 3.3670\n",
      "Epoch 2057/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.1184 - val_loss: 3.0121\n",
      "Epoch 2058/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9384 - val_loss: 1.2854\n",
      "Epoch 2059/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.8500 - val_loss: 2.8715\n",
      "Epoch 2060/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8948 - val_loss: 1.5723\n",
      "Epoch 2061/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7280 - val_loss: 2.0280\n",
      "Epoch 2062/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.5109 - val_loss: 1.4919\n",
      "Epoch 2063/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.4497 - val_loss: 1.9465\n",
      "Epoch 2064/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1.7924 - val_loss: 1.8074\n",
      "Epoch 2065/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8854 - val_loss: 1.2253\n",
      "Epoch 2066/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9520 - val_loss: 2.1812\n",
      "Epoch 2067/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1646 - val_loss: 1.9646\n",
      "Epoch 2068/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7815 - val_loss: 1.9121\n",
      "Epoch 2069/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5507 - val_loss: 2.9153\n",
      "Epoch 2070/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.8801 - val_loss: 1.4685\n",
      "Epoch 2071/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.9306 - val_loss: 1.5761\n",
      "Epoch 2072/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3284 - val_loss: 2.5524\n",
      "Epoch 2073/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5030 - val_loss: 2.9947\n",
      "Epoch 2074/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.9955 - val_loss: 3.6856\n",
      "Epoch 2075/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.2487 - val_loss: 3.2582\n",
      "Epoch 2076/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.7123 - val_loss: 3.1430\n",
      "Epoch 2077/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2332 - val_loss: 2.3139\n",
      "Epoch 2078/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9347 - val_loss: 1.4164\n",
      "Epoch 2079/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6117 - val_loss: 2.8411\n",
      "Epoch 2080/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.2135 - val_loss: 1.5931\n",
      "Epoch 2081/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5777 - val_loss: 1.5115\n",
      "Epoch 2082/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5777 - val_loss: 1.9923\n",
      "Epoch 2083/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.5472 - val_loss: 2.0151\n",
      "Epoch 2084/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0693 - val_loss: 2.0825\n",
      "Epoch 2085/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0272 - val_loss: 2.4910\n",
      "Epoch 2086/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.9772 - val_loss: 3.3927\n",
      "Epoch 2087/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2155 - val_loss: 2.0974\n",
      "Epoch 2088/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1877 - val_loss: 2.8088\n",
      "Epoch 2089/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.8124 - val_loss: 2.4463\n",
      "Epoch 2090/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1250 - val_loss: 1.8094\n",
      "Epoch 2091/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8079 - val_loss: 1.9603\n",
      "Epoch 2092/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1129 - val_loss: 2.8290\n",
      "Epoch 2093/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4563 - val_loss: 2.5229\n",
      "Epoch 2094/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.0329 - val_loss: 3.7091\n",
      "Epoch 2095/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.4010 - val_loss: 3.0020\n",
      "Epoch 2096/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.4399 - val_loss: 2.9904\n",
      "Epoch 2097/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.0997 - val_loss: 2.6683\n",
      "Epoch 2098/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.9377 - val_loss: 1.8034\n",
      "Epoch 2099/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8773 - val_loss: 1.8452\n",
      "Epoch 2100/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1709 - val_loss: 1.5510\n",
      "Epoch 2101/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.9174 - val_loss: 2.0271\n",
      "Epoch 2102/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5953 - val_loss: 2.7253\n",
      "Epoch 2103/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5949 - val_loss: 3.1113\n",
      "Epoch 2104/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4344 - val_loss: 2.1432\n",
      "Epoch 2105/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4030 - val_loss: 2.3664\n",
      "Epoch 2106/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5390 - val_loss: 3.0183\n",
      "Epoch 2107/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0778 - val_loss: 1.0437\n",
      "Epoch 2108/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.7566 - val_loss: 1.8490\n",
      "Epoch 2109/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3156 - val_loss: 2.5805\n",
      "Epoch 2110/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.3000 - val_loss: 3.2489\n",
      "Epoch 2111/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.7733 - val_loss: 3.3355\n",
      "Epoch 2112/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5624 - val_loss: 1.0312\n",
      "Epoch 2113/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.5521 - val_loss: 1.8320\n",
      "Epoch 2114/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7939 - val_loss: 1.7858\n",
      "Epoch 2115/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8060 - val_loss: 2.3545\n",
      "Epoch 2116/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3637 - val_loss: 1.7145\n",
      "Epoch 2117/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2774 - val_loss: 3.3228\n",
      "Epoch 2118/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6828 - val_loss: 3.1535\n",
      "Epoch 2119/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1041 - val_loss: 1.5937\n",
      "Epoch 2120/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.6056 - val_loss: 1.6240\n",
      "Epoch 2121/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8733 - val_loss: 1.4433\n",
      "Epoch 2122/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2102 - val_loss: 1.6414\n",
      "Epoch 2123/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2954 - val_loss: 2.8051\n",
      "Epoch 2124/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2112 - val_loss: 2.0918\n",
      "Epoch 2125/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3320 - val_loss: 1.8334\n",
      "Epoch 2126/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1882 - val_loss: 2.4202\n",
      "Epoch 2127/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9865 - val_loss: 3.0682\n",
      "Epoch 2128/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.6895 - val_loss: 1.6769\n",
      "Epoch 2129/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7923 - val_loss: 2.7688\n",
      "Epoch 2130/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4264 - val_loss: 3.6743\n",
      "Epoch 2131/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.9571 - val_loss: 1.7629\n",
      "Epoch 2132/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0691 - val_loss: 2.4051\n",
      "Epoch 2133/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.0595 - val_loss: 1.3242\n",
      "Epoch 2134/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4331 - val_loss: 1.7086\n",
      "Epoch 2135/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4832 - val_loss: 2.3301\n",
      "Epoch 2136/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.5101 - val_loss: 2.4168\n",
      "Epoch 2137/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5926 - val_loss: 2.2465\n",
      "Epoch 2138/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8639 - val_loss: 2.2706\n",
      "Epoch 2139/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8998 - val_loss: 2.7914\n",
      "Epoch 2140/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.6120 - val_loss: 2.6058\n",
      "Epoch 2141/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0331 - val_loss: 2.0476\n",
      "Epoch 2142/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3401 - val_loss: 1.6671\n",
      "Epoch 2143/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.6017 - val_loss: 1.8560\n",
      "Epoch 2144/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.1130 - val_loss: 1.8109\n",
      "Epoch 2145/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9439 - val_loss: 3.5442\n",
      "Epoch 2146/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9278 - val_loss: 4.3552\n",
      "Epoch 2147/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3197 - val_loss: 3.0760\n",
      "Epoch 2148/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.3871 - val_loss: 2.7042\n",
      "Epoch 2149/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1931 - val_loss: 1.8700\n",
      "Epoch 2150/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 2.4497 - val_loss: 2.6140\n",
      "Epoch 2151/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7593 - val_loss: 1.9392\n",
      "Epoch 2152/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9839 - val_loss: 1.6504\n",
      "Epoch 2153/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.8502 - val_loss: 2.0172\n",
      "Epoch 2154/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.3581 - val_loss: 1.7647\n",
      "Epoch 2155/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8055 - val_loss: 2.7025\n",
      "Epoch 2156/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0992 - val_loss: 1.3566\n",
      "Epoch 2157/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7891 - val_loss: 1.4340\n",
      "Epoch 2158/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.5878 - val_loss: 1.5963\n",
      "Epoch 2159/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8770 - val_loss: 2.1691\n",
      "Epoch 2160/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2600 - val_loss: 2.2055\n",
      "Epoch 2161/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.1754 - val_loss: 2.4689\n",
      "Epoch 2162/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.7791 - val_loss: 2.3235\n",
      "Epoch 2163/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0397 - val_loss: 1.4424\n",
      "Epoch 2164/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9098 - val_loss: 1.7614\n",
      "Epoch 2165/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6591 - val_loss: 3.6698\n",
      "Epoch 2166/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.2644 - val_loss: 3.5911\n",
      "Epoch 2167/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.2143 - val_loss: 3.8971\n",
      "Epoch 2168/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.9338 - val_loss: 2.5201\n",
      "Epoch 2169/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.3944 - val_loss: 3.5617\n",
      "Epoch 2170/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.6000 - val_loss: 3.4105\n",
      "Epoch 2171/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7092 - val_loss: 2.7781\n",
      "Epoch 2172/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5618 - val_loss: 2.6499\n",
      "Epoch 2173/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.6717 - val_loss: 2.4092\n",
      "Epoch 2174/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8086 - val_loss: 1.0955\n",
      "Epoch 2175/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.4532 - val_loss: 2.0912\n",
      "Epoch 2176/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3699 - val_loss: 2.4228\n",
      "Epoch 2177/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5091 - val_loss: 2.4741\n",
      "Epoch 2178/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 66us/step - loss: 2.0132 - val_loss: 2.3194\n",
      "Epoch 2179/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2505 - val_loss: 2.1049\n",
      "Epoch 2180/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0527 - val_loss: 1.4053\n",
      "Epoch 2181/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.7716 - val_loss: 3.0842\n",
      "Epoch 2182/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6638 - val_loss: 3.6778\n",
      "Epoch 2183/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.9045 - val_loss: 4.4242\n",
      "Epoch 2184/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.4928 - val_loss: 2.9423\n",
      "Epoch 2185/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.8620 - val_loss: 4.1759\n",
      "Epoch 2186/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9974 - val_loss: 3.3538\n",
      "Epoch 2187/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.3804 - val_loss: 2.6189\n",
      "Epoch 2188/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8863 - val_loss: 1.6555\n",
      "Epoch 2189/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2832 - val_loss: 2.1212\n",
      "Epoch 2190/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.3875 - val_loss: 2.9467\n",
      "Epoch 2191/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0323 - val_loss: 2.6363\n",
      "Epoch 2192/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2611 - val_loss: 2.5421\n",
      "Epoch 2193/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5172 - val_loss: 2.0945\n",
      "Epoch 2194/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.4403 - val_loss: 1.9931\n",
      "Epoch 2195/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.7215 - val_loss: 2.0065\n",
      "Epoch 2196/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0311 - val_loss: 3.0002\n",
      "Epoch 2197/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1218 - val_loss: 1.7022\n",
      "Epoch 2198/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.7481 - val_loss: 1.6096\n",
      "Epoch 2199/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.4075 - val_loss: 1.9630\n",
      "Epoch 2200/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2181 - val_loss: 1.5536\n",
      "Epoch 2201/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9417 - val_loss: 2.0852\n",
      "Epoch 2202/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2952 - val_loss: 1.5119\n",
      "Epoch 2203/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2851 - val_loss: 1.8409\n",
      "Epoch 2204/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8891 - val_loss: 3.1709\n",
      "Epoch 2205/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.6533 - val_loss: 2.4451\n",
      "Epoch 2206/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8399 - val_loss: 2.7160\n",
      "Epoch 2207/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5073 - val_loss: 2.4985\n",
      "Epoch 2208/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3011 - val_loss: 2.3400\n",
      "Epoch 2209/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3018 - val_loss: 2.6212\n",
      "Epoch 2210/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8340 - val_loss: 2.1631\n",
      "Epoch 2211/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2046 - val_loss: 2.4113\n",
      "Epoch 2212/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.4082 - val_loss: 1.8531\n",
      "Epoch 2213/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0708 - val_loss: 2.0046\n",
      "Epoch 2214/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3569 - val_loss: 2.1641\n",
      "Epoch 2215/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.6282 - val_loss: 1.7155\n",
      "Epoch 2216/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0652 - val_loss: 2.1213\n",
      "Epoch 2217/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9957 - val_loss: 1.1160\n",
      "Epoch 2218/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.8123 - val_loss: 1.9340\n",
      "Epoch 2219/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0518 - val_loss: 2.1232\n",
      "Epoch 2220/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6712 - val_loss: 3.6320\n",
      "Epoch 2221/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.4512 - val_loss: 3.6084\n",
      "Epoch 2222/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.1832 - val_loss: 3.8310\n",
      "Epoch 2223/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.6652 - val_loss: 3.5505\n",
      "Epoch 2224/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0344 - val_loss: 1.2155\n",
      "Epoch 2225/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.6119 - val_loss: 3.0872\n",
      "Epoch 2226/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3738 - val_loss: 1.7190\n",
      "Epoch 2227/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8470 - val_loss: 2.2694\n",
      "Epoch 2228/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1702 - val_loss: 2.4910\n",
      "Epoch 2229/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4221 - val_loss: 1.9029\n",
      "Epoch 2230/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 2.0848 - val_loss: 2.4376\n",
      "Epoch 2231/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9967 - val_loss: 1.6969\n",
      "Epoch 2232/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0618 - val_loss: 2.2427\n",
      "Epoch 2233/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0681 - val_loss: 4.5107\n",
      "Epoch 2234/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.5501 - val_loss: 3.1290\n",
      "Epoch 2235/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.0990 - val_loss: 3.1162\n",
      "Epoch 2236/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.2845 - val_loss: 4.3033\n",
      "Epoch 2237/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.3403 - val_loss: 1.9776\n",
      "Epoch 2238/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1695 - val_loss: 2.0156\n",
      "Epoch 2239/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8169 - val_loss: 2.0196\n",
      "Epoch 2240/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3183 - val_loss: 2.4375\n",
      "Epoch 2241/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5535 - val_loss: 3.2076\n",
      "Epoch 2242/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.6551 - val_loss: 2.3311\n",
      "Epoch 2243/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2589 - val_loss: 2.9896\n",
      "Epoch 2244/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.1932 - val_loss: 1.7201\n",
      "Epoch 2245/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.2499 - val_loss: 2.7913\n",
      "Epoch 2246/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7110 - val_loss: 4.1440\n",
      "Epoch 2247/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8850 - val_loss: 2.8786\n",
      "Epoch 2248/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1857 - val_loss: 2.0913\n",
      "Epoch 2249/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7923 - val_loss: 3.5403\n",
      "Epoch 2250/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9092 - val_loss: 2.5752\n",
      "Epoch 2251/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8756 - val_loss: 3.5235\n",
      "Epoch 2252/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.9592 - val_loss: 4.2059\n",
      "Epoch 2253/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6337 - val_loss: 2.9714\n",
      "Epoch 2254/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.3149 - val_loss: 1.9662\n",
      "Epoch 2255/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6984 - val_loss: 2.8672\n",
      "Epoch 2256/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5737 - val_loss: 2.8935\n",
      "Epoch 2257/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4685 - val_loss: 2.6529\n",
      "Epoch 2258/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.0776 - val_loss: 4.4327\n",
      "Epoch 2259/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.6220 - val_loss: 4.8077\n",
      "Epoch 2260/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.0596 - val_loss: 2.2986\n",
      "Epoch 2261/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 2.1914 - val_loss: 2.1489\n",
      "Epoch 2262/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5395 - val_loss: 1.6296\n",
      "Epoch 2263/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8004 - val_loss: 2.0616\n",
      "Epoch 2264/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8808 - val_loss: 2.5302\n",
      "Epoch 2265/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.7967 - val_loss: 1.7773\n",
      "Epoch 2266/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5330 - val_loss: 3.5215\n",
      "Epoch 2267/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.1312 - val_loss: 2.1838\n",
      "Epoch 2268/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4357 - val_loss: 2.4481\n",
      "Epoch 2269/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.5836 - val_loss: 2.6297\n",
      "Epoch 2270/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.6929 - val_loss: 1.4383\n",
      "Epoch 2271/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.6406 - val_loss: 1.6407\n",
      "Epoch 2272/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3602 - val_loss: 2.8369\n",
      "Epoch 2273/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.5893 - val_loss: 2.2737\n",
      "Epoch 2274/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.9586 - val_loss: 1.9604\n",
      "Epoch 2275/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3617 - val_loss: 2.4270\n",
      "Epoch 2276/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.0978 - val_loss: 1.2058\n",
      "Epoch 2277/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.9014 - val_loss: 1.8048\n",
      "Epoch 2278/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2362 - val_loss: 1.6843\n",
      "Epoch 2279/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8098 - val_loss: 2.0364\n",
      "Epoch 2280/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3370 - val_loss: 2.9564\n",
      "Epoch 2281/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2695 - val_loss: 2.2405\n",
      "Epoch 2282/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8890 - val_loss: 3.0782\n",
      "Epoch 2283/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.1746 - val_loss: 2.0328\n",
      "Epoch 2284/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0640 - val_loss: 1.0970\n",
      "Epoch 2285/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7020 - val_loss: 1.8042\n",
      "Epoch 2286/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9412 - val_loss: 1.9887\n",
      "Epoch 2287/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9838 - val_loss: 2.0838\n",
      "Epoch 2288/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8164 - val_loss: 2.3742\n",
      "Epoch 2289/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.6838 - val_loss: 1.2740\n",
      "Epoch 2290/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8035 - val_loss: 2.3172\n",
      "Epoch 2291/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.5066 - val_loss: 2.3494\n",
      "Epoch 2292/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5206 - val_loss: 1.9084\n",
      "Epoch 2293/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8894 - val_loss: 3.6206\n",
      "Epoch 2294/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.6492 - val_loss: 1.7826\n",
      "Epoch 2295/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.7603 - val_loss: 2.8603\n",
      "Epoch 2296/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5343 - val_loss: 2.7393\n",
      "Epoch 2297/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.0858 - val_loss: 2.1431\n",
      "Epoch 2298/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6839 - val_loss: 2.3118\n",
      "Epoch 2299/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0784 - val_loss: 1.4456\n",
      "Epoch 2300/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9531 - val_loss: 3.0662\n",
      "Epoch 2301/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0690 - val_loss: 1.5053\n",
      "Epoch 2302/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.5995 - val_loss: 1.8354\n",
      "Epoch 2303/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3711 - val_loss: 1.8621\n",
      "Epoch 2304/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9428 - val_loss: 1.7809\n",
      "Epoch 2305/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.5636 - val_loss: 1.8416\n",
      "Epoch 2306/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.5210 - val_loss: 2.8613\n",
      "Epoch 2307/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5363 - val_loss: 2.8230\n",
      "Epoch 2308/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5553 - val_loss: 2.6045\n",
      "Epoch 2309/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4646 - val_loss: 2.6484\n",
      "Epoch 2310/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8853 - val_loss: 1.0392\n",
      "Epoch 2311/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9449 - val_loss: 2.2049\n",
      "Epoch 2312/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1111 - val_loss: 2.2007\n",
      "Epoch 2313/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4578 - val_loss: 2.9784\n",
      "Epoch 2314/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.2999 - val_loss: 3.7557\n",
      "Epoch 2315/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.7313 - val_loss: 2.3864\n",
      "Epoch 2316/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0827 - val_loss: 1.9867\n",
      "Epoch 2317/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9953 - val_loss: 1.7773\n",
      "Epoch 2318/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1808 - val_loss: 1.5624\n",
      "Epoch 2319/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9567 - val_loss: 1.7340\n",
      "Epoch 2320/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7701 - val_loss: 1.8770\n",
      "Epoch 2321/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9654 - val_loss: 1.3422\n",
      "Epoch 2322/10000\n",
      "350/350 [==============================] - ETA: 0s - loss: 1.371 - 0s 64us/step - loss: 2.1623 - val_loss: 1.9443\n",
      "Epoch 2323/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1444 - val_loss: 2.5813\n",
      "Epoch 2324/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.6927 - val_loss: 3.2418\n",
      "Epoch 2325/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2087 - val_loss: 2.4526\n",
      "Epoch 2326/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5316 - val_loss: 1.7875\n",
      "Epoch 2327/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.4579 - val_loss: 3.1977\n",
      "Epoch 2328/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.9326 - val_loss: 2.8825\n",
      "Epoch 2329/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2456 - val_loss: 1.3545\n",
      "Epoch 2330/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 63us/step - loss: 1.5593 - val_loss: 2.0485\n",
      "Epoch 2331/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9503 - val_loss: 1.9973\n",
      "Epoch 2332/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9569 - val_loss: 2.0337\n",
      "Epoch 2333/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.8509 - val_loss: 2.8944\n",
      "Epoch 2334/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.8305 - val_loss: 1.7916\n",
      "Epoch 2335/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.6642 - val_loss: 2.1550\n",
      "Epoch 2336/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.8914 - val_loss: 2.0728\n",
      "Epoch 2337/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.4230 - val_loss: 2.9212\n",
      "Epoch 2338/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0186 - val_loss: 3.1771\n",
      "Epoch 2339/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6145 - val_loss: 3.2469\n",
      "Epoch 2340/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.7452 - val_loss: 2.1696\n",
      "Epoch 2341/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7165 - val_loss: 2.2206\n",
      "Epoch 2342/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4710 - val_loss: 2.7153\n",
      "Epoch 2343/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.0747 - val_loss: 1.6884\n",
      "Epoch 2344/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1670 - val_loss: 1.8934\n",
      "Epoch 2345/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.9720 - val_loss: 2.3975\n",
      "Epoch 2346/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.1129 - val_loss: 2.6020\n",
      "Epoch 2347/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9195 - val_loss: 1.8731\n",
      "Epoch 2348/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9631 - val_loss: 2.1069\n",
      "Epoch 2349/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9837 - val_loss: 3.8465\n",
      "Epoch 2350/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1458 - val_loss: 2.7009\n",
      "Epoch 2351/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3015 - val_loss: 1.9296\n",
      "Epoch 2352/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9031 - val_loss: 1.5320\n",
      "Epoch 2353/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7916 - val_loss: 1.3682\n",
      "Epoch 2354/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6112 - val_loss: 1.6332\n",
      "Epoch 2355/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9537 - val_loss: 2.9063\n",
      "Epoch 2356/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5309 - val_loss: 1.9778\n",
      "Epoch 2357/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6248 - val_loss: 3.6672\n",
      "Epoch 2358/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.0743 - val_loss: 4.0518\n",
      "Epoch 2359/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.0020 - val_loss: 2.8413\n",
      "Epoch 2360/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0758 - val_loss: 2.2907\n",
      "Epoch 2361/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.7795 - val_loss: 4.0639\n",
      "Epoch 2362/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.0726 - val_loss: 2.4598\n",
      "Epoch 2363/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 3.0161 - val_loss: 3.9350\n",
      "Epoch 2364/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2542 - val_loss: 2.5515\n",
      "Epoch 2365/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5729 - val_loss: 2.1917\n",
      "Epoch 2366/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8698 - val_loss: 2.2095\n",
      "Epoch 2367/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5931 - val_loss: 3.0688\n",
      "Epoch 2368/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6992 - val_loss: 2.9621\n",
      "Epoch 2369/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.5479 - val_loss: 2.1271\n",
      "Epoch 2370/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5287 - val_loss: 2.5223\n",
      "Epoch 2371/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5128 - val_loss: 2.6969\n",
      "Epoch 2372/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4082 - val_loss: 2.2090\n",
      "Epoch 2373/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8944 - val_loss: 2.5206\n",
      "Epoch 2374/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0444 - val_loss: 1.9562\n",
      "Epoch 2375/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8090 - val_loss: 2.3750\n",
      "Epoch 2376/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8139 - val_loss: 2.2926\n",
      "Epoch 2377/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9896 - val_loss: 1.9151\n",
      "Epoch 2378/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1956 - val_loss: 1.7833\n",
      "Epoch 2379/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7834 - val_loss: 2.4265\n",
      "Epoch 2380/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.9472 - val_loss: 1.3809\n",
      "Epoch 2381/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.6542 - val_loss: 1.3794\n",
      "Epoch 2382/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1502 - val_loss: 2.1884\n",
      "Epoch 2383/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4181 - val_loss: 3.1616\n",
      "Epoch 2384/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6139 - val_loss: 2.1003\n",
      "Epoch 2385/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0492 - val_loss: 1.4287\n",
      "Epoch 2386/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2632 - val_loss: 2.2221\n",
      "Epoch 2387/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0403 - val_loss: 1.6672\n",
      "Epoch 2388/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 2.1344 - val_loss: 3.2066\n",
      "Epoch 2389/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.1224 - val_loss: 1.2278\n",
      "Epoch 2390/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1003 - val_loss: 2.0225\n",
      "Epoch 2391/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.5374 - val_loss: 1.2987\n",
      "Epoch 2392/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.3939 - val_loss: 1.4293\n",
      "Epoch 2393/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7861 - val_loss: 2.2112\n",
      "Epoch 2394/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8453 - val_loss: 2.0036\n",
      "Epoch 2395/10000\n",
      "350/350 [==============================] - 0s 56us/step - loss: 2.2298 - val_loss: 1.5580\n",
      "Epoch 2396/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9936 - val_loss: 1.8678\n",
      "Epoch 2397/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7193 - val_loss: 1.9455\n",
      "Epoch 2398/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9139 - val_loss: 2.0646\n",
      "Epoch 2399/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0250 - val_loss: 2.9037\n",
      "Epoch 2400/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3016 - val_loss: 1.7468\n",
      "Epoch 2401/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.7172 - val_loss: 3.0003\n",
      "Epoch 2402/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3163 - val_loss: 1.2823\n",
      "Epoch 2403/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6641 - val_loss: 1.4147\n",
      "Epoch 2404/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.3909 - val_loss: 1.3265\n",
      "Epoch 2405/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.2778 - val_loss: 1.1553\n",
      "Epoch 2406/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6322 - val_loss: 1.6155\n",
      "Epoch 2407/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.9679 - val_loss: 2.9383\n",
      "Epoch 2408/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5314 - val_loss: 1.9518\n",
      "Epoch 2409/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7414 - val_loss: 1.7109\n",
      "Epoch 2410/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1914 - val_loss: 2.7993\n",
      "Epoch 2411/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4284 - val_loss: 2.2307\n",
      "Epoch 2412/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8520 - val_loss: 2.7793\n",
      "Epoch 2413/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.7453 - val_loss: 2.2282\n",
      "Epoch 2414/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2127 - val_loss: 2.8354\n",
      "Epoch 2415/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.3600 - val_loss: 2.6513\n",
      "Epoch 2416/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0373 - val_loss: 1.9177\n",
      "Epoch 2417/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9634 - val_loss: 2.4557\n",
      "Epoch 2418/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9575 - val_loss: 3.4482\n",
      "Epoch 2419/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.2752 - val_loss: 2.5749\n",
      "Epoch 2420/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1173 - val_loss: 2.3731\n",
      "Epoch 2421/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1131 - val_loss: 1.8794\n",
      "Epoch 2422/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1504 - val_loss: 1.5626\n",
      "Epoch 2423/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2008 - val_loss: 2.9392\n",
      "Epoch 2424/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0272 - val_loss: 2.6014\n",
      "Epoch 2425/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2422 - val_loss: 1.6240\n",
      "Epoch 2426/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.1168 - val_loss: 2.1806\n",
      "Epoch 2427/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.9887 - val_loss: 3.4874\n",
      "Epoch 2428/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2000 - val_loss: 4.3799\n",
      "Epoch 2429/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.7929 - val_loss: 2.7901\n",
      "Epoch 2430/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.8948 - val_loss: 2.7210\n",
      "Epoch 2431/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7892 - val_loss: 3.4503\n",
      "Epoch 2432/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7957 - val_loss: 2.7871\n",
      "Epoch 2433/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6317 - val_loss: 2.7842\n",
      "Epoch 2434/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2029 - val_loss: 2.6876\n",
      "Epoch 2435/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0158 - val_loss: 1.8374\n",
      "Epoch 2436/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1516 - val_loss: 1.7501\n",
      "Epoch 2437/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9829 - val_loss: 2.5925\n",
      "Epoch 2438/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3562 - val_loss: 2.5564\n",
      "Epoch 2439/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4305 - val_loss: 1.7982\n",
      "Epoch 2440/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3155 - val_loss: 1.9422\n",
      "Epoch 2441/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4110 - val_loss: 3.3151\n",
      "Epoch 2442/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5989 - val_loss: 3.2065\n",
      "Epoch 2443/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9990 - val_loss: 2.7965\n",
      "Epoch 2444/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 3.0252 - val_loss: 2.2661\n",
      "Epoch 2445/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6275 - val_loss: 2.1081\n",
      "Epoch 2446/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8180 - val_loss: 2.3316\n",
      "Epoch 2447/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1376 - val_loss: 2.4450\n",
      "Epoch 2448/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1935 - val_loss: 3.1620\n",
      "Epoch 2449/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5467 - val_loss: 1.6639\n",
      "Epoch 2450/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3257 - val_loss: 1.5360\n",
      "Epoch 2451/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.8685 - val_loss: 1.3363\n",
      "Epoch 2452/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7866 - val_loss: 2.4436\n",
      "Epoch 2453/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8697 - val_loss: 1.8928\n",
      "Epoch 2454/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9498 - val_loss: 1.4015\n",
      "Epoch 2455/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8592 - val_loss: 1.7554\n",
      "Epoch 2456/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7453 - val_loss: 1.4613\n",
      "Epoch 2457/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.6535 - val_loss: 2.1138\n",
      "Epoch 2458/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.9067 - val_loss: 3.3587\n",
      "Epoch 2459/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.7552 - val_loss: 2.8105\n",
      "Epoch 2460/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9603 - val_loss: 1.7900\n",
      "Epoch 2461/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.8582 - val_loss: 1.3635\n",
      "Epoch 2462/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.2617 - val_loss: 1.3124\n",
      "Epoch 2463/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.4142 - val_loss: 1.4330\n",
      "Epoch 2464/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1058 - val_loss: 2.3777\n",
      "Epoch 2465/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.3490 - val_loss: 2.7017\n",
      "Epoch 2466/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0008 - val_loss: 1.6815\n",
      "Epoch 2467/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9932 - val_loss: 1.7713\n",
      "Epoch 2468/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2063 - val_loss: 3.1324\n",
      "Epoch 2469/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.6273 - val_loss: 2.2429\n",
      "Epoch 2470/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7364 - val_loss: 2.8055\n",
      "Epoch 2471/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5629 - val_loss: 2.4819\n",
      "Epoch 2472/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6361 - val_loss: 3.2466\n",
      "Epoch 2473/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9756 - val_loss: 2.3308\n",
      "Epoch 2474/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.1322 - val_loss: 1.9050\n",
      "Epoch 2475/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8805 - val_loss: 2.3822\n",
      "Epoch 2476/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0759 - val_loss: 2.2668\n",
      "Epoch 2477/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3977 - val_loss: 2.9300\n",
      "Epoch 2478/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.4138 - val_loss: 2.5035\n",
      "Epoch 2479/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5948 - val_loss: 2.3773\n",
      "Epoch 2480/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2082 - val_loss: 1.7576\n",
      "Epoch 2481/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5776 - val_loss: 2.6378\n",
      "Epoch 2482/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 66us/step - loss: 1.5454 - val_loss: 1.3496\n",
      "Epoch 2483/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.1716 - val_loss: 2.0821\n",
      "Epoch 2484/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.8646 - val_loss: 2.0823\n",
      "Epoch 2485/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8667 - val_loss: 1.9673\n",
      "Epoch 2486/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3825 - val_loss: 2.8180\n",
      "Epoch 2487/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1404 - val_loss: 2.8131\n",
      "Epoch 2488/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.3233 - val_loss: 2.3946\n",
      "Epoch 2489/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3071 - val_loss: 2.1263\n",
      "Epoch 2490/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.4857 - val_loss: 3.2869\n",
      "Epoch 2491/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8120 - val_loss: 2.0417\n",
      "Epoch 2492/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9401 - val_loss: 1.8670\n",
      "Epoch 2493/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0224 - val_loss: 1.7597\n",
      "Epoch 2494/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6534 - val_loss: 2.4797\n",
      "Epoch 2495/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.6629 - val_loss: 2.1518\n",
      "Epoch 2496/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3045 - val_loss: 3.1854\n",
      "Epoch 2497/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5208 - val_loss: 2.2405\n",
      "Epoch 2498/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2331 - val_loss: 1.0694\n",
      "Epoch 2499/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.8017 - val_loss: 1.7480\n",
      "Epoch 2500/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9909 - val_loss: 1.7317\n",
      "Epoch 2501/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0699 - val_loss: 2.7225\n",
      "Epoch 2502/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.7166 - val_loss: 2.9196\n",
      "Epoch 2503/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.8210 - val_loss: 2.9069\n",
      "Epoch 2504/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.8635 - val_loss: 3.7352\n",
      "Epoch 2505/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.7880 - val_loss: 1.8647\n",
      "Epoch 2506/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9580 - val_loss: 2.8006\n",
      "Epoch 2507/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.0961 - val_loss: 2.3221\n",
      "Epoch 2508/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3138 - val_loss: 1.5642\n",
      "Epoch 2509/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2659 - val_loss: 2.1989\n",
      "Epoch 2510/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4079 - val_loss: 2.8568\n",
      "Epoch 2511/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.7239 - val_loss: 3.0456\n",
      "Epoch 2512/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0057 - val_loss: 4.8851\n",
      "Epoch 2513/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.4296 - val_loss: 5.4596\n",
      "Epoch 2514/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 4.1453 - val_loss: 3.7285\n",
      "Epoch 2515/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.5564 - val_loss: 3.4226\n",
      "Epoch 2516/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5574 - val_loss: 2.7350\n",
      "Epoch 2517/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0746 - val_loss: 2.1544\n",
      "Epoch 2518/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 2.6702 - val_loss: 3.0333\n",
      "Epoch 2519/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 4.2365 - val_loss: 5.7456\n",
      "Epoch 2520/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 4.8767 - val_loss: 4.6993\n",
      "Epoch 2521/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 5.1823 - val_loss: 4.6183\n",
      "Epoch 2522/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 4.0142 - val_loss: 3.0182\n",
      "Epoch 2523/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.9317 - val_loss: 4.3607\n",
      "Epoch 2524/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.5972 - val_loss: 2.1957\n",
      "Epoch 2525/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.7244 - val_loss: 4.2643\n",
      "Epoch 2526/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2190 - val_loss: 1.2933\n",
      "Epoch 2527/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0036 - val_loss: 2.2231\n",
      "Epoch 2528/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2048 - val_loss: 2.2901\n",
      "Epoch 2529/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5346 - val_loss: 3.1575\n",
      "Epoch 2530/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4279 - val_loss: 2.5029\n",
      "Epoch 2531/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1504 - val_loss: 3.1839\n",
      "Epoch 2532/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1990 - val_loss: 2.0542\n",
      "Epoch 2533/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.7446 - val_loss: 1.4391\n",
      "Epoch 2534/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.7388 - val_loss: 1.9350\n",
      "Epoch 2535/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7737 - val_loss: 1.8382\n",
      "Epoch 2536/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3592 - val_loss: 1.9762\n",
      "Epoch 2537/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7213 - val_loss: 3.1646\n",
      "Epoch 2538/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8171 - val_loss: 3.3398\n",
      "Epoch 2539/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.3268 - val_loss: 2.6498\n",
      "Epoch 2540/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4841 - val_loss: 2.6257\n",
      "Epoch 2541/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 1.9823 - val_loss: 2.1842\n",
      "Epoch 2542/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.7083 - val_loss: 3.0592\n",
      "Epoch 2543/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.4731 - val_loss: 2.0914\n",
      "Epoch 2544/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6104 - val_loss: 3.0529\n",
      "Epoch 2545/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2138 - val_loss: 1.8099\n",
      "Epoch 2546/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.9221 - val_loss: 2.3904\n",
      "Epoch 2547/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7654 - val_loss: 1.7102\n",
      "Epoch 2548/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.5825 - val_loss: 1.6321\n",
      "Epoch 2549/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.8416 - val_loss: 1.6768\n",
      "Epoch 2550/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1.9327 - val_loss: 1.5685\n",
      "Epoch 2551/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.7677 - val_loss: 2.7779\n",
      "Epoch 2552/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9666 - val_loss: 1.8224\n",
      "Epoch 2553/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.2439 - val_loss: 1.7018\n",
      "Epoch 2554/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7639 - val_loss: 1.7150\n",
      "Epoch 2555/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9234 - val_loss: 2.6341\n",
      "Epoch 2556/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4585 - val_loss: 3.4172\n",
      "Epoch 2557/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 2.5000 - val_loss: 2.6261\n",
      "Epoch 2558/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8873 - val_loss: 1.3415\n",
      "Epoch 2559/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7951 - val_loss: 3.4865\n",
      "Epoch 2560/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7569 - val_loss: 2.7769\n",
      "Epoch 2561/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0638 - val_loss: 1.7245\n",
      "Epoch 2562/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6737 - val_loss: 2.6592\n",
      "Epoch 2563/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8101 - val_loss: 2.0200\n",
      "Epoch 2564/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.6670 - val_loss: 3.1877\n",
      "Epoch 2565/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 3.0135 - val_loss: 3.1379\n",
      "Epoch 2566/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.2934 - val_loss: 3.3715\n",
      "Epoch 2567/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7243 - val_loss: 2.1665\n",
      "Epoch 2568/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5994 - val_loss: 1.8619\n",
      "Epoch 2569/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9185 - val_loss: 2.1694\n",
      "Epoch 2570/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.7640 - val_loss: 3.3544\n",
      "Epoch 2571/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6241 - val_loss: 2.1150\n",
      "Epoch 2572/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.8066 - val_loss: 2.5256\n",
      "Epoch 2573/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0598 - val_loss: 1.3628\n",
      "Epoch 2574/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1.7733 - val_loss: 1.5192\n",
      "Epoch 2575/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.4341 - val_loss: 1.9911\n",
      "Epoch 2576/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9082 - val_loss: 2.0063\n",
      "Epoch 2577/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 1.7175 - val_loss: 1.8830\n",
      "Epoch 2578/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2501 - val_loss: 3.1141\n",
      "Epoch 2579/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5688 - val_loss: 2.5258\n",
      "Epoch 2580/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4219 - val_loss: 1.5261\n",
      "Epoch 2581/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9299 - val_loss: 2.1357\n",
      "Epoch 2582/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7907 - val_loss: 2.0873\n",
      "Epoch 2583/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4126 - val_loss: 2.6099\n",
      "Epoch 2584/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4359 - val_loss: 2.2491\n",
      "Epoch 2585/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.6017 - val_loss: 2.8608\n",
      "Epoch 2586/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5651 - val_loss: 2.0022\n",
      "Epoch 2587/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.8081 - val_loss: 3.1081\n",
      "Epoch 2588/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.0122 - val_loss: 2.8633\n",
      "Epoch 2589/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3200 - val_loss: 2.6080\n",
      "Epoch 2590/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.3670 - val_loss: 1.4365\n",
      "Epoch 2591/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.4883 - val_loss: 1.4202\n",
      "Epoch 2592/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.6748 - val_loss: 2.3360\n",
      "Epoch 2593/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5915 - val_loss: 3.1242\n",
      "Epoch 2594/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.9593 - val_loss: 2.9259\n",
      "Epoch 2595/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4811 - val_loss: 2.1826\n",
      "Epoch 2596/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.8381 - val_loss: 1.5569\n",
      "Epoch 2597/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.5694 - val_loss: 1.5692\n",
      "Epoch 2598/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8047 - val_loss: 2.4622\n",
      "Epoch 2599/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0323 - val_loss: 2.3858\n",
      "Epoch 2600/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0507 - val_loss: 2.7162\n",
      "Epoch 2601/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.3412 - val_loss: 1.9513\n",
      "Epoch 2602/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.0507 - val_loss: 2.7310\n",
      "Epoch 2603/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9714 - val_loss: 2.0974\n",
      "Epoch 2604/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0412 - val_loss: 1.7009\n",
      "Epoch 2605/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9989 - val_loss: 2.7474\n",
      "Epoch 2606/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9390 - val_loss: 1.4263\n",
      "Epoch 2607/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5489 - val_loss: 2.5708\n",
      "Epoch 2608/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3764 - val_loss: 3.0726\n",
      "Epoch 2609/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1747 - val_loss: 1.6649\n",
      "Epoch 2610/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7449 - val_loss: 1.9677\n",
      "Epoch 2611/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8441 - val_loss: 2.2392\n",
      "Epoch 2612/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.3334 - val_loss: 3.3704\n",
      "Epoch 2613/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4255 - val_loss: 3.1857\n",
      "Epoch 2614/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.5070 - val_loss: 4.6772\n",
      "Epoch 2615/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.4869 - val_loss: 3.4246\n",
      "Epoch 2616/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.4136 - val_loss: 2.1224\n",
      "Epoch 2617/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8352 - val_loss: 4.5074\n",
      "Epoch 2618/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.6147 - val_loss: 3.4173\n",
      "Epoch 2619/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.2359 - val_loss: 3.1107\n",
      "Epoch 2620/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.5093 - val_loss: 3.3612\n",
      "Epoch 2621/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 2.8120 - val_loss: 3.1138\n",
      "Epoch 2622/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 4.3159 - val_loss: 3.1647\n",
      "Epoch 2623/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9227 - val_loss: 2.6284\n",
      "Epoch 2624/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 2.7376 - val_loss: 1.7920\n",
      "Epoch 2625/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.9653 - val_loss: 3.1333\n",
      "Epoch 2626/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.2745 - val_loss: 2.5236\n",
      "Epoch 2627/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.5065 - val_loss: 3.7559\n",
      "Epoch 2628/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.7133 - val_loss: 4.2284\n",
      "Epoch 2629/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2511 - val_loss: 4.3939\n",
      "Epoch 2630/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 4.3317 - val_loss: 4.2114\n",
      "Epoch 2631/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.7677 - val_loss: 2.3594\n",
      "Epoch 2632/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.5017 - val_loss: 3.5433\n",
      "Epoch 2633/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 4.0033 - val_loss: 1.7745\n",
      "Epoch 2634/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 69us/step - loss: 3.5797 - val_loss: 3.6366\n",
      "Epoch 2635/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 4.1632 - val_loss: 3.8071\n",
      "Epoch 2636/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.8961 - val_loss: 2.9698\n",
      "Epoch 2637/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.4372 - val_loss: 2.9597\n",
      "Epoch 2638/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4031 - val_loss: 1.4939\n",
      "Epoch 2639/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.6728 - val_loss: 2.0524\n",
      "Epoch 2640/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5369 - val_loss: 1.8394\n",
      "Epoch 2641/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7640 - val_loss: 2.7653\n",
      "Epoch 2642/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0983 - val_loss: 1.9117\n",
      "Epoch 2643/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.3902 - val_loss: 1.6223\n",
      "Epoch 2644/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.9236 - val_loss: 1.9064\n",
      "Epoch 2645/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9961 - val_loss: 2.6469\n",
      "Epoch 2646/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5309 - val_loss: 2.8768\n",
      "Epoch 2647/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8374 - val_loss: 1.9643\n",
      "Epoch 2648/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9907 - val_loss: 1.4693\n",
      "Epoch 2649/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2340 - val_loss: 2.8622\n",
      "Epoch 2650/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2904 - val_loss: 2.4596\n",
      "Epoch 2651/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0412 - val_loss: 1.8653\n",
      "Epoch 2652/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.3582 - val_loss: 0.9854\n",
      "Epoch 2653/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.7389 - val_loss: 2.1090\n",
      "Epoch 2654/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2488 - val_loss: 1.5914\n",
      "Epoch 2655/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.4538 - val_loss: 1.8090\n",
      "Epoch 2656/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1037 - val_loss: 1.8505\n",
      "Epoch 2657/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.5306 - val_loss: 1.5172\n",
      "Epoch 2658/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.6364 - val_loss: 2.2393\n",
      "Epoch 2659/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.4382 - val_loss: 1.8722\n",
      "Epoch 2660/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.1143 - val_loss: 2.3439\n",
      "Epoch 2661/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2988 - val_loss: 2.2963\n",
      "Epoch 2662/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5634 - val_loss: 4.0750\n",
      "Epoch 2663/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.9826 - val_loss: 4.3262\n",
      "Epoch 2664/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5880 - val_loss: 2.1592\n",
      "Epoch 2665/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8438 - val_loss: 2.6797\n",
      "Epoch 2666/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.3465 - val_loss: 3.8544\n",
      "Epoch 2667/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.1238 - val_loss: 3.3890\n",
      "Epoch 2668/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.8951 - val_loss: 2.6686\n",
      "Epoch 2669/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4041 - val_loss: 2.6183\n",
      "Epoch 2670/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.1272 - val_loss: 3.1326\n",
      "Epoch 2671/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4216 - val_loss: 2.3361\n",
      "Epoch 2672/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.2107 - val_loss: 2.6186\n",
      "Epoch 2673/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8410 - val_loss: 2.1350\n",
      "Epoch 2674/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8934 - val_loss: 1.9340\n",
      "Epoch 2675/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2228 - val_loss: 3.2952\n",
      "Epoch 2676/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2138 - val_loss: 1.7094\n",
      "Epoch 2677/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.9303 - val_loss: 1.7481\n",
      "Epoch 2678/10000\n",
      "350/350 [==============================] - 0s 78us/step - loss: 1.8504 - val_loss: 3.2401\n",
      "Epoch 2679/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.0413 - val_loss: 1.6366\n",
      "Epoch 2680/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2653 - val_loss: 2.2131\n",
      "Epoch 2681/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.7812 - val_loss: 2.9385\n",
      "Epoch 2682/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3517 - val_loss: 1.5637\n",
      "Epoch 2683/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.5867 - val_loss: 0.9714\n",
      "Epoch 2684/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0404 - val_loss: 1.7611\n",
      "Epoch 2685/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.1574 - val_loss: 3.5203\n",
      "Epoch 2686/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.7483 - val_loss: 1.8538\n",
      "Epoch 2687/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7496 - val_loss: 2.0985\n",
      "Epoch 2688/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.6307 - val_loss: 1.4961\n",
      "Epoch 2689/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7094 - val_loss: 1.2535\n",
      "Epoch 2690/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.5651 - val_loss: 1.8065\n",
      "Epoch 2691/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.4694 - val_loss: 2.1049\n",
      "Epoch 2692/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.5790 - val_loss: 2.1271\n",
      "Epoch 2693/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4497 - val_loss: 2.0453\n",
      "Epoch 2694/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1593 - val_loss: 1.9503\n",
      "Epoch 2695/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.7797 - val_loss: 2.2991\n",
      "Epoch 2696/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6292 - val_loss: 3.4262\n",
      "Epoch 2697/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8077 - val_loss: 3.6331\n",
      "Epoch 2698/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8526 - val_loss: 3.3154\n",
      "Epoch 2699/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.5096 - val_loss: 1.9752\n",
      "Epoch 2700/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.7975 - val_loss: 1.9368\n",
      "Epoch 2701/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4035 - val_loss: 2.4743\n",
      "Epoch 2702/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4002 - val_loss: 2.4751\n",
      "Epoch 2703/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.2384 - val_loss: 2.1886\n",
      "Epoch 2704/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9695 - val_loss: 2.3202\n",
      "Epoch 2705/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.1797 - val_loss: 1.7384\n",
      "Epoch 2706/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.4166 - val_loss: 2.7665\n",
      "Epoch 2707/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.6523 - val_loss: 2.7575\n",
      "Epoch 2708/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.1261 - val_loss: 2.7371\n",
      "Epoch 2709/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9224 - val_loss: 4.0087\n",
      "Epoch 2710/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.3843 - val_loss: 1.2628\n",
      "Epoch 2711/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.3006 - val_loss: 2.3299\n",
      "Epoch 2712/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.7655 - val_loss: 1.0366\n",
      "Epoch 2713/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.5477 - val_loss: 1.5625\n",
      "Epoch 2714/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.5977 - val_loss: 1.4004\n",
      "Epoch 2715/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3368 - val_loss: 3.0642\n",
      "Epoch 2716/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.2254 - val_loss: 2.5847\n",
      "Epoch 2717/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.6871 - val_loss: 2.7523\n",
      "Epoch 2718/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.1413 - val_loss: 3.0621\n",
      "Epoch 2719/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9184 - val_loss: 4.3526\n",
      "Epoch 2720/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.2810 - val_loss: 2.4353\n",
      "Epoch 2721/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.9865 - val_loss: 1.6851\n",
      "Epoch 2722/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2872 - val_loss: 3.5193\n",
      "Epoch 2723/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.0305 - val_loss: 3.2566\n",
      "Epoch 2724/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2772 - val_loss: 2.5399\n",
      "Epoch 2725/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3700 - val_loss: 2.4173\n",
      "Epoch 2726/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8527 - val_loss: 2.1884\n",
      "Epoch 2727/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0594 - val_loss: 2.8496\n",
      "Epoch 2728/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2184 - val_loss: 1.8303\n",
      "Epoch 2729/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2374 - val_loss: 2.0148\n",
      "Epoch 2730/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0192 - val_loss: 2.4607\n",
      "Epoch 2731/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6625 - val_loss: 3.0676\n",
      "Epoch 2732/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4511 - val_loss: 3.6481\n",
      "Epoch 2733/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.7965 - val_loss: 3.4119\n",
      "Epoch 2734/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9244 - val_loss: 3.5778\n",
      "Epoch 2735/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2924 - val_loss: 4.4195\n",
      "Epoch 2736/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.2297 - val_loss: 2.7934\n",
      "Epoch 2737/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2504 - val_loss: 1.3508\n",
      "Epoch 2738/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.6004 - val_loss: 2.1659\n",
      "Epoch 2739/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9127 - val_loss: 1.7551\n",
      "Epoch 2740/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9667 - val_loss: 2.0089\n",
      "Epoch 2741/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8979 - val_loss: 1.5507\n",
      "Epoch 2742/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.6383 - val_loss: 1.1861\n",
      "Epoch 2743/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.5979 - val_loss: 1.7069\n",
      "Epoch 2744/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0261 - val_loss: 2.9112\n",
      "Epoch 2745/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7064 - val_loss: 3.9559\n",
      "Epoch 2746/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4745 - val_loss: 2.6792\n",
      "Epoch 2747/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4315 - val_loss: 3.0673\n",
      "Epoch 2748/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.7255 - val_loss: 2.9634\n",
      "Epoch 2749/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6137 - val_loss: 2.4179\n",
      "Epoch 2750/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0634 - val_loss: 1.6802\n",
      "Epoch 2751/10000\n",
      "350/350 [==============================] - ETA: 0s - loss: 1.403 - 0s 60us/step - loss: 1.6932 - val_loss: 2.6804\n",
      "Epoch 2752/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0871 - val_loss: 1.8319\n",
      "Epoch 2753/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0359 - val_loss: 1.8103\n",
      "Epoch 2754/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2879 - val_loss: 1.8574\n",
      "Epoch 2755/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2331 - val_loss: 1.9095\n",
      "Epoch 2756/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.5203 - val_loss: 2.3312\n",
      "Epoch 2757/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.3725 - val_loss: 2.6071\n",
      "Epoch 2758/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.2501 - val_loss: 4.1960\n",
      "Epoch 2759/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.4988 - val_loss: 3.3043\n",
      "Epoch 2760/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3421 - val_loss: 2.6206\n",
      "Epoch 2761/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.8399 - val_loss: 4.2231\n",
      "Epoch 2762/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3515 - val_loss: 3.5846\n",
      "Epoch 2763/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2616 - val_loss: 1.5804\n",
      "Epoch 2764/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9342 - val_loss: 3.5757\n",
      "Epoch 2765/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2856 - val_loss: 3.3992\n",
      "Epoch 2766/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1877 - val_loss: 3.3161\n",
      "Epoch 2767/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7911 - val_loss: 2.1768\n",
      "Epoch 2768/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.6275 - val_loss: 0.8359\n",
      "Epoch 2769/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.3176 - val_loss: 1.4418\n",
      "Epoch 2770/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1020 - val_loss: 2.7837\n",
      "Epoch 2771/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6877 - val_loss: 3.7850\n",
      "Epoch 2772/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6380 - val_loss: 2.7171\n",
      "Epoch 2773/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.1822 - val_loss: 3.7013\n",
      "Epoch 2774/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 3.4615 - val_loss: 4.2904\n",
      "Epoch 2775/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0925 - val_loss: 2.7330\n",
      "Epoch 2776/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0215 - val_loss: 1.8253\n",
      "Epoch 2777/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.5136 - val_loss: 1.2253\n",
      "Epoch 2778/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.9287 - val_loss: 2.7954\n",
      "Epoch 2779/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5361 - val_loss: 2.1127\n",
      "Epoch 2780/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2211 - val_loss: 2.0432\n",
      "Epoch 2781/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3224 - val_loss: 2.4655\n",
      "Epoch 2782/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2378 - val_loss: 2.2305\n",
      "Epoch 2783/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.7362 - val_loss: 2.3539\n",
      "Epoch 2784/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0623 - val_loss: 2.7125\n",
      "Epoch 2785/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4554 - val_loss: 2.4870\n",
      "Epoch 2786/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 61us/step - loss: 2.6093 - val_loss: 1.6605\n",
      "Epoch 2787/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2571 - val_loss: 2.5849\n",
      "Epoch 2788/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6181 - val_loss: 1.4319\n",
      "Epoch 2789/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2865 - val_loss: 1.8939\n",
      "Epoch 2790/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.8627 - val_loss: 2.0402\n",
      "Epoch 2791/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2546 - val_loss: 1.8689\n",
      "Epoch 2792/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0370 - val_loss: 2.3625\n",
      "Epoch 2793/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1464 - val_loss: 1.1071\n",
      "Epoch 2794/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9129 - val_loss: 2.6439\n",
      "Epoch 2795/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5686 - val_loss: 2.1475\n",
      "Epoch 2796/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2876 - val_loss: 2.5050\n",
      "Epoch 2797/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8953 - val_loss: 2.7303\n",
      "Epoch 2798/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 2.1411 - val_loss: 2.0230\n",
      "Epoch 2799/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8889 - val_loss: 2.0995\n",
      "Epoch 2800/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8556 - val_loss: 1.5039\n",
      "Epoch 2801/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1821 - val_loss: 2.5053\n",
      "Epoch 2802/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3292 - val_loss: 2.6734\n",
      "Epoch 2803/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5630 - val_loss: 1.5078\n",
      "Epoch 2804/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7418 - val_loss: 1.3467\n",
      "Epoch 2805/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0627 - val_loss: 2.1897\n",
      "Epoch 2806/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0615 - val_loss: 2.0780\n",
      "Epoch 2807/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.3689 - val_loss: 1.1509\n",
      "Epoch 2808/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.3624 - val_loss: 2.2851\n",
      "Epoch 2809/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2305 - val_loss: 1.9009\n",
      "Epoch 2810/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0262 - val_loss: 1.7670\n",
      "Epoch 2811/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2286 - val_loss: 3.6727\n",
      "Epoch 2812/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5323 - val_loss: 1.8283\n",
      "Epoch 2813/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.1728 - val_loss: 1.9120\n",
      "Epoch 2814/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2335 - val_loss: 2.4712\n",
      "Epoch 2815/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.4296 - val_loss: 1.5766\n",
      "Epoch 2816/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2170 - val_loss: 2.8462\n",
      "Epoch 2817/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.9121 - val_loss: 4.1649\n",
      "Epoch 2818/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.3571 - val_loss: 3.9639\n",
      "Epoch 2819/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.0680 - val_loss: 2.1099\n",
      "Epoch 2820/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6493 - val_loss: 1.2144\n",
      "Epoch 2821/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.5536 - val_loss: 1.7441\n",
      "Epoch 2822/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6242 - val_loss: 2.1551\n",
      "Epoch 2823/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.5214 - val_loss: 3.9144\n",
      "Epoch 2824/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 4.4600 - val_loss: 5.2751\n",
      "Epoch 2825/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.5867 - val_loss: 2.5398\n",
      "Epoch 2826/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5826 - val_loss: 2.3519\n",
      "Epoch 2827/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9089 - val_loss: 3.4275\n",
      "Epoch 2828/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.7523 - val_loss: 1.9083\n",
      "Epoch 2829/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8799 - val_loss: 2.5210\n",
      "Epoch 2830/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1614 - val_loss: 2.1208\n",
      "Epoch 2831/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9923 - val_loss: 1.8701\n",
      "Epoch 2832/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9916 - val_loss: 2.7997\n",
      "Epoch 2833/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 2.7448 - val_loss: 1.5479\n",
      "Epoch 2834/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.8637 - val_loss: 3.0563\n",
      "Epoch 2835/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7063 - val_loss: 3.2664\n",
      "Epoch 2836/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.0505 - val_loss: 3.3215\n",
      "Epoch 2837/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1153 - val_loss: 3.0627\n",
      "Epoch 2838/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2619 - val_loss: 3.7589\n",
      "Epoch 2839/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.2360 - val_loss: 3.1350\n",
      "Epoch 2840/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.2155 - val_loss: 3.5032\n",
      "Epoch 2841/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8459 - val_loss: 2.6180\n",
      "Epoch 2842/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1574 - val_loss: 1.8775\n",
      "Epoch 2843/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.9046 - val_loss: 3.1331\n",
      "Epoch 2844/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.0173 - val_loss: 2.7587\n",
      "Epoch 2845/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9903 - val_loss: 3.0742\n",
      "Epoch 2846/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4256 - val_loss: 2.2147\n",
      "Epoch 2847/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.4673 - val_loss: 1.8516\n",
      "Epoch 2848/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0916 - val_loss: 2.0600\n",
      "Epoch 2849/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9612 - val_loss: 1.4255\n",
      "Epoch 2850/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4644 - val_loss: 2.3288\n",
      "Epoch 2851/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8044 - val_loss: 1.6773\n",
      "Epoch 2852/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8640 - val_loss: 2.1511\n",
      "Epoch 2853/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2988 - val_loss: 2.6226\n",
      "Epoch 2854/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0656 - val_loss: 1.7381\n",
      "Epoch 2855/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2945 - val_loss: 2.7866\n",
      "Epoch 2856/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3581 - val_loss: 3.3845\n",
      "Epoch 2857/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.9316 - val_loss: 3.0632\n",
      "Epoch 2858/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5451 - val_loss: 2.0425\n",
      "Epoch 2859/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7899 - val_loss: 4.0766\n",
      "Epoch 2860/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9751 - val_loss: 2.3736\n",
      "Epoch 2861/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3605 - val_loss: 2.5458\n",
      "Epoch 2862/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.0523 - val_loss: 2.0889\n",
      "Epoch 2863/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2432 - val_loss: 2.8730\n",
      "Epoch 2864/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6777 - val_loss: 1.9560\n",
      "Epoch 2865/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5267 - val_loss: 2.8343\n",
      "Epoch 2866/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4014 - val_loss: 3.0293\n",
      "Epoch 2867/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 3.1964 - val_loss: 3.9459\n",
      "Epoch 2868/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.6261 - val_loss: 4.9660\n",
      "Epoch 2869/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.2009 - val_loss: 3.4313\n",
      "Epoch 2870/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7748 - val_loss: 2.2528\n",
      "Epoch 2871/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8335 - val_loss: 0.9494\n",
      "Epoch 2872/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.0853 - val_loss: 1.4672\n",
      "Epoch 2873/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.7405 - val_loss: 1.9945\n",
      "Epoch 2874/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7263 - val_loss: 1.9089\n",
      "Epoch 2875/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5131 - val_loss: 2.7886\n",
      "Epoch 2876/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8350 - val_loss: 3.1079\n",
      "Epoch 2877/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4731 - val_loss: 3.2017\n",
      "Epoch 2878/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4728 - val_loss: 3.6855\n",
      "Epoch 2879/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5927 - val_loss: 2.4974\n",
      "Epoch 2880/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.6501 - val_loss: 2.3767\n",
      "Epoch 2881/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7433 - val_loss: 3.3487\n",
      "Epoch 2882/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7065 - val_loss: 2.3993\n",
      "Epoch 2883/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.6988 - val_loss: 1.8655\n",
      "Epoch 2884/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0305 - val_loss: 2.8039\n",
      "Epoch 2885/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4256 - val_loss: 1.4329\n",
      "Epoch 2886/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2365 - val_loss: 3.1589\n",
      "Epoch 2887/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8820 - val_loss: 2.7725\n",
      "Epoch 2888/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.6503 - val_loss: 1.9336\n",
      "Epoch 2889/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8649 - val_loss: 1.4441\n",
      "Epoch 2890/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5407 - val_loss: 2.7826\n",
      "Epoch 2891/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2037 - val_loss: 3.1588\n",
      "Epoch 2892/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.9955 - val_loss: 3.7755\n",
      "Epoch 2893/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.4281 - val_loss: 2.7335\n",
      "Epoch 2894/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.0782 - val_loss: 1.3463\n",
      "Epoch 2895/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1723 - val_loss: 2.4601\n",
      "Epoch 2896/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6986 - val_loss: 2.4909\n",
      "Epoch 2897/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.3172 - val_loss: 2.9985\n",
      "Epoch 2898/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.4619 - val_loss: 3.9513\n",
      "Epoch 2899/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.7076 - val_loss: 3.8042\n",
      "Epoch 2900/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 4.1705 - val_loss: 3.6154\n",
      "Epoch 2901/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.9252 - val_loss: 3.2740\n",
      "Epoch 2902/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.5644 - val_loss: 4.0865\n",
      "Epoch 2903/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 4.1176 - val_loss: 3.5994\n",
      "Epoch 2904/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.4132 - val_loss: 3.4051\n",
      "Epoch 2905/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0405 - val_loss: 3.3772\n",
      "Epoch 2906/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5423 - val_loss: 2.5414\n",
      "Epoch 2907/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0182 - val_loss: 1.5767\n",
      "Epoch 2908/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9484 - val_loss: 3.0100\n",
      "Epoch 2909/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8011 - val_loss: 3.8598\n",
      "Epoch 2910/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0927 - val_loss: 0.9668\n",
      "Epoch 2911/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.4584 - val_loss: 2.0332\n",
      "Epoch 2912/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7624 - val_loss: 1.9378\n",
      "Epoch 2913/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.4525 - val_loss: 2.3108\n",
      "Epoch 2914/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1576 - val_loss: 2.1012\n",
      "Epoch 2915/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.5884 - val_loss: 2.3098\n",
      "Epoch 2916/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3653 - val_loss: 2.1895\n",
      "Epoch 2917/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.7537 - val_loss: 2.1634\n",
      "Epoch 2918/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.6789 - val_loss: 1.4420\n",
      "Epoch 2919/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.1882 - val_loss: 2.0902\n",
      "Epoch 2920/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.7312 - val_loss: 1.2421\n",
      "Epoch 2921/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.9438 - val_loss: 1.7146\n",
      "Epoch 2922/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1563 - val_loss: 3.4366\n",
      "Epoch 2923/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.4348 - val_loss: 2.6599\n",
      "Epoch 2924/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 2.2908 - val_loss: 2.8420\n",
      "Epoch 2925/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1788 - val_loss: 2.5540\n",
      "Epoch 2926/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.5617 - val_loss: 2.3648\n",
      "Epoch 2927/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5474 - val_loss: 2.4737\n",
      "Epoch 2928/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0734 - val_loss: 2.3585\n",
      "Epoch 2929/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2827 - val_loss: 1.7875\n",
      "Epoch 2930/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1820 - val_loss: 1.6292\n",
      "Epoch 2931/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9886 - val_loss: 1.3320\n",
      "Epoch 2932/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7127 - val_loss: 1.3475\n",
      "Epoch 2933/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8311 - val_loss: 1.8709\n",
      "Epoch 2934/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0221 - val_loss: 2.0534\n",
      "Epoch 2935/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0495 - val_loss: 1.7652\n",
      "Epoch 2936/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.7974 - val_loss: 2.4900\n",
      "Epoch 2937/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8706 - val_loss: 2.3309\n",
      "Epoch 2938/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 66us/step - loss: 1.9526 - val_loss: 2.0947\n",
      "Epoch 2939/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8427 - val_loss: 3.7536\n",
      "Epoch 2940/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8533 - val_loss: 3.6529\n",
      "Epoch 2941/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6324 - val_loss: 2.5751\n",
      "Epoch 2942/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2734 - val_loss: 2.1788\n",
      "Epoch 2943/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1664 - val_loss: 2.9077\n",
      "Epoch 2944/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3132 - val_loss: 1.8743\n",
      "Epoch 2945/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.8085 - val_loss: 1.2475\n",
      "Epoch 2946/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.7961 - val_loss: 1.4812\n",
      "Epoch 2947/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1566 - val_loss: 2.6573\n",
      "Epoch 2948/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5625 - val_loss: 3.5580\n",
      "Epoch 2949/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5665 - val_loss: 3.6604\n",
      "Epoch 2950/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2151 - val_loss: 2.1742\n",
      "Epoch 2951/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3903 - val_loss: 1.9604\n",
      "Epoch 2952/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4003 - val_loss: 2.0962\n",
      "Epoch 2953/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9964 - val_loss: 2.0982\n",
      "Epoch 2954/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0462 - val_loss: 2.0037\n",
      "Epoch 2955/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0161 - val_loss: 2.9820\n",
      "Epoch 2956/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2839 - val_loss: 2.3059\n",
      "Epoch 2957/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0489 - val_loss: 1.4694\n",
      "Epoch 2958/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.6949 - val_loss: 2.9694\n",
      "Epoch 2959/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 2.7395 - val_loss: 2.8792\n",
      "Epoch 2960/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7876 - val_loss: 1.5084\n",
      "Epoch 2961/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9162 - val_loss: 1.2903\n",
      "Epoch 2962/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1844 - val_loss: 2.4963\n",
      "Epoch 2963/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5083 - val_loss: 1.8250\n",
      "Epoch 2964/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.6664 - val_loss: 2.2730\n",
      "Epoch 2965/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9630 - val_loss: 3.2718\n",
      "Epoch 2966/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0927 - val_loss: 1.5292\n",
      "Epoch 2967/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4626 - val_loss: 1.9386\n",
      "Epoch 2968/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5572 - val_loss: 1.6208\n",
      "Epoch 2969/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3581 - val_loss: 2.4507\n",
      "Epoch 2970/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.6167 - val_loss: 3.1746\n",
      "Epoch 2971/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7945 - val_loss: 2.2682\n",
      "Epoch 2972/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7884 - val_loss: 2.2422\n",
      "Epoch 2973/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.7812 - val_loss: 1.9849\n",
      "Epoch 2974/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2415 - val_loss: 3.8258\n",
      "Epoch 2975/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1397 - val_loss: 2.0294\n",
      "Epoch 2976/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1197 - val_loss: 3.1414\n",
      "Epoch 2977/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3563 - val_loss: 2.6758\n",
      "Epoch 2978/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5345 - val_loss: 2.5281\n",
      "Epoch 2979/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2861 - val_loss: 1.3692\n",
      "Epoch 2980/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8966 - val_loss: 1.9853\n",
      "Epoch 2981/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.8341 - val_loss: 2.3226\n",
      "Epoch 2982/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4059 - val_loss: 1.8817\n",
      "Epoch 2983/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0780 - val_loss: 2.0118\n",
      "Epoch 2984/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5193 - val_loss: 2.0692\n",
      "Epoch 2985/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7375 - val_loss: 2.1831\n",
      "Epoch 2986/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.0167 - val_loss: 1.3974\n",
      "Epoch 2987/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.2987 - val_loss: 1.9652\n",
      "Epoch 2988/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.1586 - val_loss: 2.4499\n",
      "Epoch 2989/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4308 - val_loss: 2.0924\n",
      "Epoch 2990/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9195 - val_loss: 4.1319\n",
      "Epoch 2991/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7502 - val_loss: 2.0142\n",
      "Epoch 2992/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6482 - val_loss: 2.8162\n",
      "Epoch 2993/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.5602 - val_loss: 4.3961\n",
      "Epoch 2994/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3612 - val_loss: 3.1591\n",
      "Epoch 2995/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.4222 - val_loss: 3.6630\n",
      "Epoch 2996/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.0026 - val_loss: 3.6015\n",
      "Epoch 2997/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.7129 - val_loss: 2.9443\n",
      "Epoch 2998/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.2147 - val_loss: 1.5658\n",
      "Epoch 2999/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5154 - val_loss: 2.3540\n",
      "Epoch 3000/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.6740 - val_loss: 1.7585\n",
      "Epoch 3001/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.3160 - val_loss: 2.9587\n",
      "Epoch 3002/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.9169 - val_loss: 2.3911\n",
      "Epoch 3003/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.7754 - val_loss: 2.1558\n",
      "Epoch 3004/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5147 - val_loss: 2.0773\n",
      "Epoch 3005/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4927 - val_loss: 2.5554\n",
      "Epoch 3006/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.9416 - val_loss: 2.1062\n",
      "Epoch 3007/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.3174 - val_loss: 1.8172\n",
      "Epoch 3008/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.9477 - val_loss: 1.9331\n",
      "Epoch 3009/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2534 - val_loss: 1.6626\n",
      "Epoch 3010/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1918 - val_loss: 1.6686\n",
      "Epoch 3011/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.4908 - val_loss: 2.1132\n",
      "Epoch 3012/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2788 - val_loss: 1.5677\n",
      "Epoch 3013/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7655 - val_loss: 3.1812\n",
      "Epoch 3014/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5076 - val_loss: 2.2482\n",
      "Epoch 3015/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9609 - val_loss: 1.6475\n",
      "Epoch 3016/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0389 - val_loss: 1.4394\n",
      "Epoch 3017/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 2.1286 - val_loss: 1.3472\n",
      "Epoch 3018/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0361 - val_loss: 2.3535\n",
      "Epoch 3019/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0038 - val_loss: 1.6377\n",
      "Epoch 3020/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3246 - val_loss: 1.9933\n",
      "Epoch 3021/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7267 - val_loss: 1.7339\n",
      "Epoch 3022/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2816 - val_loss: 2.3944\n",
      "Epoch 3023/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5717 - val_loss: 1.2735\n",
      "Epoch 3024/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8256 - val_loss: 2.8134\n",
      "Epoch 3025/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0997 - val_loss: 1.4831\n",
      "Epoch 3026/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8151 - val_loss: 2.8824\n",
      "Epoch 3027/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3618 - val_loss: 1.5617\n",
      "Epoch 3028/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0839 - val_loss: 2.1194\n",
      "Epoch 3029/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4366 - val_loss: 1.5475\n",
      "Epoch 3030/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.1681 - val_loss: 2.3935\n",
      "Epoch 3031/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.3209 - val_loss: 2.3061\n",
      "Epoch 3032/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.7152 - val_loss: 3.2297\n",
      "Epoch 3033/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.1367 - val_loss: 2.6796\n",
      "Epoch 3034/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7982 - val_loss: 1.6515\n",
      "Epoch 3035/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9406 - val_loss: 1.8069\n",
      "Epoch 3036/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.6337 - val_loss: 2.3684\n",
      "Epoch 3037/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3643 - val_loss: 2.9773\n",
      "Epoch 3038/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0552 - val_loss: 3.7654\n",
      "Epoch 3039/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.3423 - val_loss: 2.6707\n",
      "Epoch 3040/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4047 - val_loss: 2.4273\n",
      "Epoch 3041/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.3068 - val_loss: 2.8342\n",
      "Epoch 3042/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9851 - val_loss: 3.5806\n",
      "Epoch 3043/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8612 - val_loss: 2.9968\n",
      "Epoch 3044/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5656 - val_loss: 4.3531\n",
      "Epoch 3045/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.3087 - val_loss: 3.4404\n",
      "Epoch 3046/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3427 - val_loss: 1.9570\n",
      "Epoch 3047/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9402 - val_loss: 2.6572\n",
      "Epoch 3048/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1738 - val_loss: 1.5321\n",
      "Epoch 3049/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2290 - val_loss: 1.6756\n",
      "Epoch 3050/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2363 - val_loss: 2.0070\n",
      "Epoch 3051/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8698 - val_loss: 2.1914\n",
      "Epoch 3052/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6814 - val_loss: 1.3721\n",
      "Epoch 3053/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.3793 - val_loss: 1.6168\n",
      "Epoch 3054/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.1423 - val_loss: 2.1041\n",
      "Epoch 3055/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1714 - val_loss: 2.2665\n",
      "Epoch 3056/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8192 - val_loss: 3.2820\n",
      "Epoch 3057/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2881 - val_loss: 3.6664\n",
      "Epoch 3058/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9521 - val_loss: 2.5803\n",
      "Epoch 3059/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.6061 - val_loss: 2.3859\n",
      "Epoch 3060/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0634 - val_loss: 2.1417\n",
      "Epoch 3061/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.5195 - val_loss: 1.3371\n",
      "Epoch 3062/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5253 - val_loss: 2.8362\n",
      "Epoch 3063/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6524 - val_loss: 1.5899\n",
      "Epoch 3064/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9612 - val_loss: 2.3585\n",
      "Epoch 3065/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5673 - val_loss: 2.5932\n",
      "Epoch 3066/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.9184 - val_loss: 3.7530\n",
      "Epoch 3067/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3245 - val_loss: 3.3639\n",
      "Epoch 3068/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3486 - val_loss: 4.4713\n",
      "Epoch 3069/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7828 - val_loss: 1.8663\n",
      "Epoch 3070/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.4505 - val_loss: 1.5750\n",
      "Epoch 3071/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5657 - val_loss: 2.8457\n",
      "Epoch 3072/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.0746 - val_loss: 2.4613\n",
      "Epoch 3073/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2993 - val_loss: 2.5088\n",
      "Epoch 3074/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.4852 - val_loss: 1.4190\n",
      "Epoch 3075/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9130 - val_loss: 3.0161\n",
      "Epoch 3076/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0373 - val_loss: 1.4281\n",
      "Epoch 3077/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9029 - val_loss: 2.6233\n",
      "Epoch 3078/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3005 - val_loss: 2.4736\n",
      "Epoch 3079/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9646 - val_loss: 2.9798\n",
      "Epoch 3080/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7448 - val_loss: 3.1145\n",
      "Epoch 3081/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6374 - val_loss: 3.8072\n",
      "Epoch 3082/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3733 - val_loss: 4.1322\n",
      "Epoch 3083/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.6052 - val_loss: 3.9279\n",
      "Epoch 3084/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7938 - val_loss: 2.7496\n",
      "Epoch 3085/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5020 - val_loss: 2.8842\n",
      "Epoch 3086/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.7263 - val_loss: 2.0519\n",
      "Epoch 3087/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0935 - val_loss: 1.8468\n",
      "Epoch 3088/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5892 - val_loss: 1.7643\n",
      "Epoch 3089/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8849 - val_loss: 2.4032\n",
      "Epoch 3090/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 65us/step - loss: 2.2628 - val_loss: 1.5127\n",
      "Epoch 3091/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.2030 - val_loss: 2.7311\n",
      "Epoch 3092/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.5090 - val_loss: 2.6079\n",
      "Epoch 3093/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3225 - val_loss: 1.6132\n",
      "Epoch 3094/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9677 - val_loss: 2.1887\n",
      "Epoch 3095/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.9195 - val_loss: 1.9400\n",
      "Epoch 3096/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7305 - val_loss: 2.0382\n",
      "Epoch 3097/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0315 - val_loss: 1.8495\n",
      "Epoch 3098/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.5110 - val_loss: 2.5501\n",
      "Epoch 3099/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9932 - val_loss: 2.1851\n",
      "Epoch 3100/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.1285 - val_loss: 1.9868\n",
      "Epoch 3101/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.7841 - val_loss: 1.4487\n",
      "Epoch 3102/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 3.3760 - val_loss: 3.5454\n",
      "Epoch 3103/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5191 - val_loss: 3.0313\n",
      "Epoch 3104/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9811 - val_loss: 1.2903\n",
      "Epoch 3105/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9670 - val_loss: 2.3921\n",
      "Epoch 3106/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9187 - val_loss: 1.7880\n",
      "Epoch 3107/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0755 - val_loss: 1.5147\n",
      "Epoch 3108/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 1.9314 - val_loss: 2.3899\n",
      "Epoch 3109/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.5793 - val_loss: 2.3469\n",
      "Epoch 3110/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.8491 - val_loss: 3.0446\n",
      "Epoch 3111/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.8022 - val_loss: 2.9545\n",
      "Epoch 3112/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5819 - val_loss: 2.5012\n",
      "Epoch 3113/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.1714 - val_loss: 3.6919\n",
      "Epoch 3114/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.0612 - val_loss: 3.0154\n",
      "Epoch 3115/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7040 - val_loss: 2.3113\n",
      "Epoch 3116/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.8007 - val_loss: 2.2529\n",
      "Epoch 3117/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.7250 - val_loss: 1.6536\n",
      "Epoch 3118/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2976 - val_loss: 3.2844\n",
      "Epoch 3119/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3538 - val_loss: 1.8203\n",
      "Epoch 3120/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.7914 - val_loss: 1.2238\n",
      "Epoch 3121/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1685 - val_loss: 2.4734\n",
      "Epoch 3122/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2948 - val_loss: 2.8243\n",
      "Epoch 3123/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3330 - val_loss: 2.9252\n",
      "Epoch 3124/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9410 - val_loss: 2.2130\n",
      "Epoch 3125/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5753 - val_loss: 2.5031\n",
      "Epoch 3126/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8687 - val_loss: 1.5573\n",
      "Epoch 3127/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5605 - val_loss: 1.5731\n",
      "Epoch 3128/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9460 - val_loss: 2.5503\n",
      "Epoch 3129/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 3.2517 - val_loss: 3.6885\n",
      "Epoch 3130/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4759 - val_loss: 2.1796\n",
      "Epoch 3131/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1069 - val_loss: 1.7850\n",
      "Epoch 3132/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.4386 - val_loss: 1.0452\n",
      "Epoch 3133/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.4515 - val_loss: 1.4864\n",
      "Epoch 3134/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.3025 - val_loss: 1.7447\n",
      "Epoch 3135/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.4175 - val_loss: 2.1071\n",
      "Epoch 3136/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.4189 - val_loss: 2.6342\n",
      "Epoch 3137/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9989 - val_loss: 2.4841\n",
      "Epoch 3138/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4600 - val_loss: 1.2588\n",
      "Epoch 3139/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.7288 - val_loss: 2.8175\n",
      "Epoch 3140/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.0167 - val_loss: 3.7297\n",
      "Epoch 3141/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.6187 - val_loss: 2.7358\n",
      "Epoch 3142/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8875 - val_loss: 1.6834\n",
      "Epoch 3143/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.9569 - val_loss: 3.1771\n",
      "Epoch 3144/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6955 - val_loss: 4.2511\n",
      "Epoch 3145/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8280 - val_loss: 2.8714\n",
      "Epoch 3146/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.9664 - val_loss: 2.8112\n",
      "Epoch 3147/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4717 - val_loss: 3.1501\n",
      "Epoch 3148/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2203 - val_loss: 2.0486\n",
      "Epoch 3149/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2389 - val_loss: 1.7156\n",
      "Epoch 3150/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 2.0735 - val_loss: 2.9524\n",
      "Epoch 3151/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.9809 - val_loss: 2.6648\n",
      "Epoch 3152/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.6529 - val_loss: 2.0645\n",
      "Epoch 3153/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.4807 - val_loss: 1.5388\n",
      "Epoch 3154/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8710 - val_loss: 2.2584\n",
      "Epoch 3155/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 1.6633 - val_loss: 1.2045\n",
      "Epoch 3156/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.5374 - val_loss: 2.0236\n",
      "Epoch 3157/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.0157 - val_loss: 2.7962\n",
      "Epoch 3158/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 1.9541 - val_loss: 1.0524\n",
      "Epoch 3159/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.3536 - val_loss: 2.2173\n",
      "Epoch 3160/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.0842 - val_loss: 1.8399\n",
      "Epoch 3161/10000\n",
      "350/350 [==============================] - 0s 58us/step - loss: 1.9576 - val_loss: 1.6290\n",
      "Epoch 3162/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2315 - val_loss: 2.7369\n",
      "Epoch 3163/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.7360 - val_loss: 2.2429\n",
      "Epoch 3164/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9951 - val_loss: 2.4991\n",
      "Epoch 3165/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2482 - val_loss: 1.7291\n",
      "Epoch 3166/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2156 - val_loss: 2.0233\n",
      "Epoch 3167/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4268 - val_loss: 2.0350\n",
      "Epoch 3168/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0517 - val_loss: 2.9702\n",
      "Epoch 3169/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2344 - val_loss: 2.4464\n",
      "Epoch 3170/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9991 - val_loss: 1.7162\n",
      "Epoch 3171/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6391 - val_loss: 1.5949\n",
      "Epoch 3172/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9536 - val_loss: 1.6632\n",
      "Epoch 3173/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1104 - val_loss: 1.9716\n",
      "Epoch 3174/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.9687 - val_loss: 2.6381\n",
      "Epoch 3175/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6176 - val_loss: 3.8417\n",
      "Epoch 3176/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.9186 - val_loss: 3.0727\n",
      "Epoch 3177/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9878 - val_loss: 3.6326\n",
      "Epoch 3178/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.3795 - val_loss: 2.6147\n",
      "Epoch 3179/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8508 - val_loss: 2.8549\n",
      "Epoch 3180/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8399 - val_loss: 3.4873\n",
      "Epoch 3181/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.3795 - val_loss: 4.1377\n",
      "Epoch 3182/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.6407 - val_loss: 2.9295\n",
      "Epoch 3183/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.0562 - val_loss: 4.1637\n",
      "Epoch 3184/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1988 - val_loss: 2.5072\n",
      "Epoch 3185/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5557 - val_loss: 2.5929\n",
      "Epoch 3186/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9959 - val_loss: 1.5814\n",
      "Epoch 3187/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8146 - val_loss: 1.9258\n",
      "Epoch 3188/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1944 - val_loss: 1.4219\n",
      "Epoch 3189/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.6840 - val_loss: 1.5100\n",
      "Epoch 3190/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.4825 - val_loss: 1.4084\n",
      "Epoch 3191/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5070 - val_loss: 2.3072\n",
      "Epoch 3192/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9508 - val_loss: 0.4801\n",
      "Epoch 3193/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.5459 - val_loss: 1.7113\n",
      "Epoch 3194/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.1859 - val_loss: 2.2075\n",
      "Epoch 3195/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3460 - val_loss: 1.5059\n",
      "Epoch 3196/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.7225 - val_loss: 2.0590\n",
      "Epoch 3197/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 2.7396 - val_loss: 2.3918\n",
      "Epoch 3198/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 3.2189 - val_loss: 3.4790\n",
      "Epoch 3199/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.1442 - val_loss: 3.6923\n",
      "Epoch 3200/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.7558 - val_loss: 1.8253\n",
      "Epoch 3201/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.1148 - val_loss: 1.4380\n",
      "Epoch 3202/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.8954 - val_loss: 2.2155\n",
      "Epoch 3203/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8284 - val_loss: 1.3662\n",
      "Epoch 3204/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.9357 - val_loss: 1.3242\n",
      "Epoch 3205/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.4414 - val_loss: 1.2615\n",
      "Epoch 3206/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.5052 - val_loss: 1.5257\n",
      "Epoch 3207/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.5648 - val_loss: 2.2734\n",
      "Epoch 3208/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.8537 - val_loss: 1.8246\n",
      "Epoch 3209/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7844 - val_loss: 2.0060\n",
      "Epoch 3210/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.9334 - val_loss: 1.5405\n",
      "Epoch 3211/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8663 - val_loss: 2.3892\n",
      "Epoch 3212/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3674 - val_loss: 2.0502\n",
      "Epoch 3213/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4828 - val_loss: 1.6424\n",
      "Epoch 3214/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1230 - val_loss: 2.6388\n",
      "Epoch 3215/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.2210 - val_loss: 3.4571\n",
      "Epoch 3216/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.0854 - val_loss: 3.0405\n",
      "Epoch 3217/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4274 - val_loss: 1.7278\n",
      "Epoch 3218/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5261 - val_loss: 3.2330\n",
      "Epoch 3219/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5130 - val_loss: 3.1754\n",
      "Epoch 3220/10000\n",
      "350/350 [==============================] - ETA: 0s - loss: 4.002 - 0s 66us/step - loss: 3.0893 - val_loss: 3.2107\n",
      "Epoch 3221/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.7381 - val_loss: 3.3944\n",
      "Epoch 3222/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.6297 - val_loss: 3.4917\n",
      "Epoch 3223/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.8284 - val_loss: 2.7201\n",
      "Epoch 3224/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.6855 - val_loss: 2.7515\n",
      "Epoch 3225/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.4059 - val_loss: 3.9324\n",
      "Epoch 3226/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.4076 - val_loss: 4.7894\n",
      "Epoch 3227/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.9992 - val_loss: 3.3724\n",
      "Epoch 3228/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.8255 - val_loss: 2.5445\n",
      "Epoch 3229/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2424 - val_loss: 1.5422\n",
      "Epoch 3230/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8060 - val_loss: 1.4638\n",
      "Epoch 3231/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 1.7838 - val_loss: 1.9082\n",
      "Epoch 3232/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5796 - val_loss: 3.3113\n",
      "Epoch 3233/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2203 - val_loss: 1.4282\n",
      "Epoch 3234/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.5895 - val_loss: 2.7666\n",
      "Epoch 3235/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.0235 - val_loss: 2.7653\n",
      "Epoch 3236/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4673 - val_loss: 1.7983\n",
      "Epoch 3237/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8487 - val_loss: 2.8668\n",
      "Epoch 3238/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.4903 - val_loss: 3.6512\n",
      "Epoch 3239/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9569 - val_loss: 2.6641\n",
      "Epoch 3240/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.4712 - val_loss: 2.9510\n",
      "Epoch 3241/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2376 - val_loss: 2.2517\n",
      "Epoch 3242/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 62us/step - loss: 2.5787 - val_loss: 2.1949\n",
      "Epoch 3243/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.0869 - val_loss: 2.3361\n",
      "Epoch 3244/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.7020 - val_loss: 1.8668\n",
      "Epoch 3245/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8834 - val_loss: 2.0822\n",
      "Epoch 3246/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5268 - val_loss: 3.7832\n",
      "Epoch 3247/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.4885 - val_loss: 2.8881\n",
      "Epoch 3248/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.9229 - val_loss: 3.1596\n",
      "Epoch 3249/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 3.2594 - val_loss: 2.8958\n",
      "Epoch 3250/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.3989 - val_loss: 3.1281\n",
      "Epoch 3251/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3472 - val_loss: 2.4620\n",
      "Epoch 3252/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2963 - val_loss: 1.9178\n",
      "Epoch 3253/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.5841 - val_loss: 1.5661\n",
      "Epoch 3254/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.5526 - val_loss: 2.6603\n",
      "Epoch 3255/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.3712 - val_loss: 2.7871\n",
      "Epoch 3256/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2894 - val_loss: 1.7169\n",
      "Epoch 3257/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.7096 - val_loss: 1.9110\n",
      "Epoch 3258/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1666 - val_loss: 1.7010\n",
      "Epoch 3259/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.5227 - val_loss: 1.5288\n",
      "Epoch 3260/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.0814 - val_loss: 1.6534\n",
      "Epoch 3261/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.0980 - val_loss: 1.8131\n",
      "Epoch 3262/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 1.5238 - val_loss: 2.1153\n",
      "Epoch 3263/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.2828 - val_loss: 3.1708\n",
      "Epoch 3264/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 3.1165 - val_loss: 3.0815\n",
      "Epoch 3265/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 2.3303 - val_loss: 1.2987\n",
      "Epoch 3266/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 1.6171 - val_loss: 3.3229\n",
      "Epoch 3267/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6521 - val_loss: 3.9714\n",
      "Epoch 3268/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5955 - val_loss: 2.6890\n",
      "Epoch 3269/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8418 - val_loss: 1.2432\n",
      "Epoch 3270/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.8774 - val_loss: 2.4643\n",
      "Epoch 3271/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2383 - val_loss: 2.3770\n",
      "Epoch 3272/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0519 - val_loss: 1.7523\n",
      "Epoch 3273/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.8475 - val_loss: 1.9478\n",
      "Epoch 3274/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.7633 - val_loss: 1.7415\n",
      "Epoch 3275/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4405 - val_loss: 3.1571\n",
      "Epoch 3276/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8189 - val_loss: 2.8227\n",
      "Epoch 3277/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.2702 - val_loss: 2.3230\n",
      "Epoch 3278/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1703 - val_loss: 2.3890\n",
      "Epoch 3279/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 2.6239 - val_loss: 3.3468\n",
      "Epoch 3280/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 2.2814 - val_loss: 1.0811\n",
      "Epoch 3281/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 1.6355 - val_loss: 1.6516\n",
      "Epoch 3282/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.5075 - val_loss: 2.0828\n",
      "Epoch 3283/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9827 - val_loss: 2.5449\n",
      "Epoch 3284/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0768 - val_loss: 1.5828\n",
      "Epoch 3285/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.7116 - val_loss: 2.6145\n",
      "Epoch 3286/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8978 - val_loss: 1.6734\n",
      "Epoch 3287/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.5796 - val_loss: 1.5853\n",
      "Epoch 3288/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8557 - val_loss: 0.7973\n",
      "Epoch 3289/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.5755 - val_loss: 2.2670\n",
      "Epoch 3290/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9123 - val_loss: 2.3002\n",
      "Epoch 3291/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.0493 - val_loss: 1.3223\n",
      "Epoch 3292/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.6971 - val_loss: 2.7174\n",
      "Epoch 3293/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.9771 - val_loss: 2.7557\n",
      "Epoch 3294/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 2.4666 - val_loss: 2.2479\n",
      "Epoch 3295/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.8261 - val_loss: 1.7070\n",
      "Epoch 3296/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.1188 - val_loss: 2.4550\n",
      "Epoch 3297/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.9086 - val_loss: 2.2711\n",
      "Epoch 3298/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.5519 - val_loss: 0.9564\n",
      "Epoch 3299/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1.5908 - val_loss: 1.0164\n",
      "Epoch 3300/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 1.3828 - val_loss: 1.3059\n",
      "Epoch 3301/10000\n",
      "350/350 [==============================] - 0s 79us/step - loss: 1.4787 - val_loss: 1.7936\n",
      "Epoch 3302/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.6832 - val_loss: 1.8553\n",
      "Epoch 3303/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.2020 - val_loss: 3.0180\n",
      "Epoch 3304/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8936 - val_loss: 2.2338\n",
      "Epoch 3305/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6344 - val_loss: 2.3053\n",
      "Epoch 3306/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.7734 - val_loss: 2.5017\n",
      "Epoch 3307/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.0759 - val_loss: 1.9854\n",
      "Epoch 3308/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.9545 - val_loss: 1.5698\n",
      "Epoch 3309/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.8395 - val_loss: 2.7390\n",
      "Epoch 3310/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.1475 - val_loss: 1.7512\n",
      "Epoch 3311/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 2.4755 - val_loss: 2.4992\n",
      "Epoch 3312/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.5225 - val_loss: 1.9325\n",
      "Epoch 3313/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5172 - val_loss: 3.3221\n",
      "Epoch 3314/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.2506 - val_loss: 3.1254\n",
      "Epoch 3315/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.7676 - val_loss: 2.6453\n",
      "Epoch 3316/10000\n",
      "350/350 [==============================] - 0s 87us/step - loss: 3.4074 - val_loss: 2.3192\n",
      "Epoch 3317/10000\n",
      "350/350 [==============================] - 0s 80us/step - loss: 2.4049 - val_loss: 1.8569\n",
      "Epoch 3318/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.2157 - val_loss: 2.7706\n",
      "Epoch 3319/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.8883 - val_loss: 1.8017\n",
      "Epoch 3320/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.9729 - val_loss: 2.3154\n",
      "Epoch 3321/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0699 - val_loss: 1.3596\n",
      "Epoch 3322/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.7101 - val_loss: 1.8310\n",
      "Epoch 3323/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.3293 - val_loss: 1.5496\n",
      "Epoch 3324/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.1107 - val_loss: 2.1195\n",
      "Epoch 3325/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8681 - val_loss: 2.2385\n",
      "Epoch 3326/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.5144 - val_loss: 4.0944\n",
      "Epoch 3327/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8038 - val_loss: 2.6192\n",
      "Epoch 3328/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.4471 - val_loss: 2.5269\n",
      "Epoch 3329/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.5180 - val_loss: 2.6214\n",
      "Epoch 3330/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.4970 - val_loss: 1.1815\n",
      "Epoch 3331/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 1.3640 - val_loss: 2.0563\n",
      "Epoch 3332/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 1.9867 - val_loss: 2.1970\n",
      "Epoch 3333/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 2.4823 - val_loss: 2.7006\n",
      "Epoch 3334/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.2348 - val_loss: 2.4158\n",
      "Epoch 3335/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.7976 - val_loss: 2.1716\n",
      "Epoch 3336/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.7195 - val_loss: 2.3838\n",
      "Epoch 3337/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.9660 - val_loss: 2.0763\n",
      "Epoch 3338/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 2.0586 - val_loss: 2.4991\n",
      "Epoch 3339/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.6447 - val_loss: 2.9312\n",
      "Epoch 3340/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.3275 - val_loss: 2.6005\n",
      "Epoch 3341/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.5661 - val_loss: 3.4412\n",
      "Epoch 3342/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.6818 - val_loss: 2.6466\n",
      "Epoch 3343/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5259 - val_loss: 3.3899\n",
      "Epoch 3344/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.6259 - val_loss: 3.2517\n",
      "Epoch 3345/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.0431 - val_loss: 1.7258\n",
      "Epoch 3346/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.8929 - val_loss: 2.2302\n",
      "Epoch 3347/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3946 - val_loss: 2.8293\n",
      "Epoch 3348/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8496 - val_loss: 2.7100\n",
      "Epoch 3349/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.1549 - val_loss: 4.8467\n",
      "Epoch 3350/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.5200 - val_loss: 3.6831\n",
      "Epoch 3351/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5273 - val_loss: 3.2697\n",
      "Epoch 3352/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4007 - val_loss: 1.8427\n",
      "Epoch 3353/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8825 - val_loss: 1.8386\n",
      "Epoch 3354/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.5377 - val_loss: 2.7412\n",
      "Epoch 3355/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.3740 - val_loss: 4.3027\n",
      "Epoch 3356/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 4.5138 - val_loss: 3.1285\n",
      "Epoch 3357/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.3553 - val_loss: 2.0135\n",
      "Epoch 3358/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.1002 - val_loss: 2.3080\n",
      "Epoch 3359/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6846 - val_loss: 3.8452\n",
      "Epoch 3360/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8014 - val_loss: 3.6778\n",
      "Epoch 3361/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1095 - val_loss: 2.1153\n",
      "Epoch 3362/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8149 - val_loss: 2.2500\n",
      "Epoch 3363/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4840 - val_loss: 2.5928\n",
      "Epoch 3364/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3000 - val_loss: 2.5539\n",
      "Epoch 3365/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7311 - val_loss: 2.0508\n",
      "Epoch 3366/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.5392 - val_loss: 2.7141\n",
      "Epoch 3367/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.5839 - val_loss: 2.8049\n",
      "Epoch 3368/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 2.5534 - val_loss: 1.9137\n",
      "Epoch 3369/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.0717 - val_loss: 2.3691\n",
      "Epoch 3370/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6178 - val_loss: 2.2665\n",
      "Epoch 3371/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8859 - val_loss: 1.9898\n",
      "Epoch 3372/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2753 - val_loss: 1.5125\n",
      "Epoch 3373/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2301 - val_loss: 2.0073\n",
      "Epoch 3374/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3713 - val_loss: 2.9812\n",
      "Epoch 3375/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.4507 - val_loss: 2.7298\n",
      "Epoch 3376/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6084 - val_loss: 2.3141\n",
      "Epoch 3377/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.6517 - val_loss: 3.2065\n",
      "Epoch 3378/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7292 - val_loss: 2.4941\n",
      "Epoch 3379/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.5191 - val_loss: 3.1614\n",
      "Epoch 3380/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3058 - val_loss: 1.7587\n",
      "Epoch 3381/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1.7538 - val_loss: 1.4529\n",
      "Epoch 3382/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4263 - val_loss: 1.8333\n",
      "Epoch 3383/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.8466 - val_loss: 3.4116\n",
      "Epoch 3384/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.0910 - val_loss: 3.2000\n",
      "Epoch 3385/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.7545 - val_loss: 2.7684\n",
      "Epoch 3386/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 2.4531 - val_loss: 2.9448\n",
      "Epoch 3387/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.5476 - val_loss: 2.0891\n",
      "Epoch 3388/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.7339 - val_loss: 3.4348\n",
      "Epoch 3389/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6438 - val_loss: 3.8435\n",
      "Epoch 3390/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.1610 - val_loss: 3.3808\n",
      "Epoch 3391/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.4311 - val_loss: 2.8668\n",
      "Epoch 3392/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6045 - val_loss: 3.1792\n",
      "Epoch 3393/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.9746 - val_loss: 3.1457\n",
      "Epoch 3394/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 66us/step - loss: 2.4076 - val_loss: 2.4993\n",
      "Epoch 3395/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3588 - val_loss: 2.4150\n",
      "Epoch 3396/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4572 - val_loss: 3.0312\n",
      "Epoch 3397/10000\n",
      "350/350 [==============================] - 0s 75us/step - loss: 2.4712 - val_loss: 3.5504\n",
      "Epoch 3398/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.4212 - val_loss: 2.9408\n",
      "Epoch 3399/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.3068 - val_loss: 2.6691\n",
      "Epoch 3400/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 2.5528 - val_loss: 2.4449\n",
      "Epoch 3401/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.1852 - val_loss: 2.3826\n",
      "Epoch 3402/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 1.9897 - val_loss: 1.6923\n",
      "Epoch 3403/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.1013 - val_loss: 1.2234\n",
      "Epoch 3404/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.1027 - val_loss: 1.6066\n",
      "Epoch 3405/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 1.7268 - val_loss: 1.9192\n",
      "Epoch 3406/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.7932 - val_loss: 2.2245\n",
      "Epoch 3407/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.8357 - val_loss: 3.4455\n",
      "Epoch 3408/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1419 - val_loss: 1.5905\n",
      "Epoch 3409/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2042 - val_loss: 2.5725\n",
      "Epoch 3410/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.2189 - val_loss: 2.5029\n",
      "Epoch 3411/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1284 - val_loss: 1.8128\n",
      "Epoch 3412/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7787 - val_loss: 2.5413\n",
      "Epoch 3413/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.7814 - val_loss: 3.3722\n",
      "Epoch 3414/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.8519 - val_loss: 2.9285\n",
      "Epoch 3415/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8786 - val_loss: 2.2182\n",
      "Epoch 3416/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3244 - val_loss: 1.7728\n",
      "Epoch 3417/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.1908 - val_loss: 2.1060\n",
      "Epoch 3418/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 1.6306 - val_loss: 2.0456\n",
      "Epoch 3419/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1277 - val_loss: 1.8478\n",
      "Epoch 3420/10000\n",
      "350/350 [==============================] - 0s 57us/step - loss: 1.4142 - val_loss: 0.7598\n",
      "Epoch 3421/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.4760 - val_loss: 1.2054\n",
      "Epoch 3422/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.7435 - val_loss: 1.4823\n",
      "Epoch 3423/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.3689 - val_loss: 1.5170\n",
      "Epoch 3424/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.3275 - val_loss: 3.2114\n",
      "Epoch 3425/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5006 - val_loss: 2.3077\n",
      "Epoch 3426/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3797 - val_loss: 2.2861\n",
      "Epoch 3427/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6673 - val_loss: 2.8798\n",
      "Epoch 3428/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1808 - val_loss: 1.7469\n",
      "Epoch 3429/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1697 - val_loss: 2.6240\n",
      "Epoch 3430/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.9920 - val_loss: 2.8106\n",
      "Epoch 3431/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8690 - val_loss: 2.8478\n",
      "Epoch 3432/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8103 - val_loss: 2.6190\n",
      "Epoch 3433/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2359 - val_loss: 2.3708\n",
      "Epoch 3434/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.5627 - val_loss: 2.9089\n",
      "Epoch 3435/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 3.1617 - val_loss: 2.9137\n",
      "Epoch 3436/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.9133 - val_loss: 1.8386\n",
      "Epoch 3437/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.3701 - val_loss: 4.7973\n",
      "Epoch 3438/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 3.7329 - val_loss: 3.3834\n",
      "Epoch 3439/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5311 - val_loss: 2.6205\n",
      "Epoch 3440/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0405 - val_loss: 2.0534\n",
      "Epoch 3441/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2105 - val_loss: 2.3256\n",
      "Epoch 3442/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1714 - val_loss: 2.3938\n",
      "Epoch 3443/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3214 - val_loss: 2.9976\n",
      "Epoch 3444/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.4872 - val_loss: 2.7419\n",
      "Epoch 3445/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.1507 - val_loss: 2.7470\n",
      "Epoch 3446/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.5485 - val_loss: 3.2841\n",
      "Epoch 3447/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.7694 - val_loss: 2.2492\n",
      "Epoch 3448/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.0377 - val_loss: 2.9467\n",
      "Epoch 3449/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5935 - val_loss: 3.1977\n",
      "Epoch 3450/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 3.2272 - val_loss: 3.0383\n",
      "Epoch 3451/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.7453 - val_loss: 1.6083\n",
      "Epoch 3452/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.8698 - val_loss: 1.3357\n",
      "Epoch 3453/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.8216 - val_loss: 2.0730\n",
      "Epoch 3454/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.6001 - val_loss: 2.1605\n",
      "Epoch 3455/10000\n",
      "350/350 [==============================] - 0s 59us/step - loss: 2.2316 - val_loss: 2.6279\n",
      "Epoch 3456/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9879 - val_loss: 2.2917\n",
      "Epoch 3457/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.4602 - val_loss: 2.2689\n",
      "Epoch 3458/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2882 - val_loss: 2.1915\n",
      "Epoch 3459/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1494 - val_loss: 1.8991\n",
      "Epoch 3460/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.8713 - val_loss: 2.8203\n",
      "Epoch 3461/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.5645 - val_loss: 2.9971\n",
      "Epoch 3462/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3851 - val_loss: 2.9518\n",
      "Epoch 3463/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7193 - val_loss: 3.7772\n",
      "Epoch 3464/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.3653 - val_loss: 1.3170\n",
      "Epoch 3465/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 1.8488 - val_loss: 3.2102\n",
      "Epoch 3466/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7315 - val_loss: 1.7298\n",
      "Epoch 3467/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7191 - val_loss: 2.6091\n",
      "Epoch 3468/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.0769 - val_loss: 2.6504\n",
      "Epoch 3469/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5318 - val_loss: 2.7432\n",
      "Epoch 3470/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 1.8927 - val_loss: 3.2437\n",
      "Epoch 3471/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.5885 - val_loss: 3.2706\n",
      "Epoch 3472/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.4442 - val_loss: 1.5380\n",
      "Epoch 3473/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3084 - val_loss: 1.9283\n",
      "Epoch 3474/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4679 - val_loss: 2.7821\n",
      "Epoch 3475/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8179 - val_loss: 3.5980\n",
      "Epoch 3476/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 2.2602 - val_loss: 0.8303\n",
      "Epoch 3477/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.8992 - val_loss: 2.2440\n",
      "Epoch 3478/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1879 - val_loss: 2.9417\n",
      "Epoch 3479/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6611 - val_loss: 2.9331\n",
      "Epoch 3480/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.6701 - val_loss: 2.0638\n",
      "Epoch 3481/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.2754 - val_loss: 3.3127\n",
      "Epoch 3482/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.3896 - val_loss: 2.6804\n",
      "Epoch 3483/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.0815 - val_loss: 3.9677\n",
      "Epoch 3484/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.2584 - val_loss: 3.5128\n",
      "Epoch 3485/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.8518 - val_loss: 2.3125\n",
      "Epoch 3486/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.2576 - val_loss: 1.9682\n",
      "Epoch 3487/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.9872 - val_loss: 2.1078\n",
      "Epoch 3488/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.0277 - val_loss: 1.5477\n",
      "Epoch 3489/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1959 - val_loss: 2.3591\n",
      "Epoch 3490/10000\n",
      "350/350 [==============================] - 0s 61us/step - loss: 1.9038 - val_loss: 2.8774\n",
      "Epoch 3491/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7073 - val_loss: 1.9209\n",
      "Epoch 3492/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.2773 - val_loss: 2.1968\n",
      "Epoch 3493/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 3.2031 - val_loss: 2.6037\n",
      "Epoch 3494/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.4407 - val_loss: 2.5634\n",
      "Epoch 3495/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.6522 - val_loss: 2.0637\n",
      "Epoch 3496/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.5254 - val_loss: 2.8457\n",
      "Epoch 3497/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.1070 - val_loss: 2.4119\n",
      "Epoch 3498/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.5211 - val_loss: 2.6657\n",
      "Epoch 3499/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.3692 - val_loss: 2.3489\n",
      "Epoch 3500/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1028 - val_loss: 3.7975\n",
      "Epoch 3501/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8506 - val_loss: 3.5248\n",
      "Epoch 3502/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3259 - val_loss: 2.3226\n",
      "Epoch 3503/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.6725 - val_loss: 1.2452\n",
      "Epoch 3504/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.5621 - val_loss: 2.2759\n",
      "Epoch 3505/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.1496 - val_loss: 2.2910\n",
      "Epoch 3506/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3955 - val_loss: 2.6535\n",
      "Epoch 3507/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0467 - val_loss: 2.4446\n",
      "Epoch 3508/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.7639 - val_loss: 2.2925\n",
      "Epoch 3509/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.6354 - val_loss: 2.5980\n",
      "Epoch 3510/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.8100 - val_loss: 2.8949\n",
      "Epoch 3511/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.3489 - val_loss: 2.2029\n",
      "Epoch 3512/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 2.4545 - val_loss: 2.0288\n",
      "Epoch 3513/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.4991 - val_loss: 3.1591\n",
      "Epoch 3514/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.3381 - val_loss: 1.7343\n",
      "Epoch 3515/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.2283 - val_loss: 3.1085\n",
      "Epoch 3516/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9030 - val_loss: 2.5451\n",
      "Epoch 3517/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.8299 - val_loss: 2.9510\n",
      "Epoch 3518/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 3.0484 - val_loss: 2.6682\n",
      "Epoch 3519/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.8714 - val_loss: 3.4480\n",
      "Epoch 3520/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.6466 - val_loss: 3.1254\n",
      "Epoch 3521/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.3804 - val_loss: 3.0227\n",
      "Epoch 3522/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.0692 - val_loss: 2.6706\n",
      "Epoch 3523/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.7051 - val_loss: 2.5567\n",
      "Epoch 3524/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.4372 - val_loss: 1.9940\n",
      "Epoch 3525/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1714 - val_loss: 0.8853\n",
      "Epoch 3526/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.6417 - val_loss: 1.4073\n",
      "Epoch 3527/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.5062 - val_loss: 3.5478\n",
      "Epoch 3528/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.8607 - val_loss: 2.6567\n",
      "Epoch 3529/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.7131 - val_loss: 2.9424\n",
      "Epoch 3530/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 2.7712 - val_loss: 3.2522\n",
      "Epoch 3531/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 2.9234 - val_loss: 2.2470\n",
      "Epoch 3532/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 1.9845 - val_loss: 2.1980\n",
      "Epoch 3533/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1.7519 - val_loss: 1.8186\n",
      "Epoch 3534/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 2.0880 - val_loss: 2.1496\n",
      "Epoch 3535/10000\n",
      "350/350 [==============================] - 0s 70us/step - loss: 3.1181 - val_loss: 3.1898\n",
      "Epoch 3536/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 2.9898 - val_loss: 4.2923\n",
      "Epoch 3537/10000\n",
      "350/350 [==============================] - 0s 77us/step - loss: 2.4465 - val_loss: 2.5204\n",
      "Epoch 3538/10000\n",
      "350/350 [==============================] - ETA: 0s - loss: 2.066 - 0s 70us/step - loss: 2.0041 - val_loss: 2.0546\n",
      "Epoch 3539/10000\n",
      "350/350 [==============================] - 0s 73us/step - loss: 1.9337 - val_loss: 1.7944\n",
      "Epoch 3540/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1.4773 - val_loss: 2.6029\n",
      "Epoch 3541/10000\n",
      "350/350 [==============================] - 0s 74us/step - loss: 1.7867 - val_loss: 1.8621\n",
      "Epoch 3542/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.0477 - val_loss: 2.3410\n",
      "Epoch 3543/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.2954 - val_loss: 2.1996\n",
      "Epoch 3544/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.6689 - val_loss: 2.4833\n",
      "Epoch 3545/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.1199 - val_loss: 2.7045\n",
      "Epoch 3546/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 0s 67us/step - loss: 3.2306 - val_loss: 3.6833\n",
      "Epoch 3547/10000\n",
      "350/350 [==============================] - 0s 68us/step - loss: 2.8755 - val_loss: 1.8505\n",
      "Epoch 3548/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.0431 - val_loss: 1.6266\n",
      "Epoch 3549/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0386 - val_loss: 1.3481\n",
      "Epoch 3550/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 1.3286 - val_loss: 1.3010\n",
      "Epoch 3551/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 1.6767 - val_loss: 1.9117\n",
      "Epoch 3552/10000\n",
      "350/350 [==============================] - 0s 72us/step - loss: 2.3825 - val_loss: 3.3509\n",
      "Epoch 3553/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 3.3602 - val_loss: 4.0683\n",
      "Epoch 3554/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 3.6828 - val_loss: 1.9929\n",
      "Epoch 3555/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.1887 - val_loss: 2.5223\n",
      "Epoch 3556/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.5777 - val_loss: 3.2366\n",
      "Epoch 3557/10000\n",
      "350/350 [==============================] - 0s 71us/step - loss: 2.7545 - val_loss: 2.6026\n",
      "Epoch 3558/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.3015 - val_loss: 2.0155\n",
      "Epoch 3559/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.2849 - val_loss: 1.2777\n",
      "Epoch 3560/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7089 - val_loss: 2.2705\n",
      "Epoch 3561/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 2.0931 - val_loss: 2.0987\n",
      "Epoch 3562/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0895 - val_loss: 1.6987\n",
      "Epoch 3563/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.2685 - val_loss: 1.8452\n",
      "Epoch 3564/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.8848 - val_loss: 2.1551\n",
      "Epoch 3565/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.9451 - val_loss: 2.1472\n",
      "Epoch 3566/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 3.0719 - val_loss: 1.7418\n",
      "Epoch 3567/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8436 - val_loss: 2.3013\n",
      "Epoch 3568/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.8383 - val_loss: 3.5157\n",
      "Epoch 3569/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 3.7984 - val_loss: 3.4622\n",
      "Epoch 3570/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 3.6022 - val_loss: 2.1179\n",
      "Epoch 3571/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 2.0954 - val_loss: 1.2295\n",
      "Epoch 3572/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9544 - val_loss: 2.0765\n",
      "Epoch 3573/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 1.7586 - val_loss: 1.7613\n",
      "Epoch 3574/10000\n",
      "350/350 [==============================] - 0s 60us/step - loss: 1.7464 - val_loss: 1.5714\n",
      "Epoch 3575/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.5324 - val_loss: 1.3913\n",
      "Epoch 3576/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 1.1323 - val_loss: 1.1131\n",
      "Epoch 3577/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.4048 - val_loss: 1.1692\n",
      "Epoch 3578/10000\n",
      "350/350 [==============================] - 0s 66us/step - loss: 2.0196 - val_loss: 2.8508\n",
      "Epoch 3579/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 2.6302 - val_loss: 3.7801\n",
      "Epoch 3580/10000\n",
      "350/350 [==============================] - 0s 67us/step - loss: 2.8701 - val_loss: 4.3080\n",
      "Epoch 3581/10000\n",
      "350/350 [==============================] - 0s 62us/step - loss: 3.0989 - val_loss: 1.9856\n",
      "Epoch 3582/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7759 - val_loss: 1.5106\n",
      "Epoch 3583/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.7935 - val_loss: 1.9619\n",
      "Epoch 3584/10000\n",
      "350/350 [==============================] - 0s 63us/step - loss: 2.1401 - val_loss: 2.2168\n",
      "Epoch 3585/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.9283 - val_loss: 1.8269\n",
      "Epoch 3586/10000\n",
      "350/350 [==============================] - 0s 65us/step - loss: 1.8431 - val_loss: 1.7154\n",
      "Epoch 3587/10000\n",
      "350/350 [==============================] - 0s 69us/step - loss: 2.0627 - val_loss: 1.9226\n",
      "Epoch 3588/10000\n",
      "350/350 [==============================] - 0s 64us/step - loss: 1.7488 - val_loss: 1.8929\n",
      "Epoch 3589/10000\n",
      " 32/350 [=>............................] - ETA: 0s - loss: 1.4201"
     ]
    }
   ],
   "source": [
    "fitDetails = model.fit(X, y, epochs=10000, validation_split=0.3, verbose=1)\n",
    "#fitDetails = model.fit(trainingInput['input'], trainingInput['target'], validation_split=0.3, epochs=30, verbose=1)\n",
    "#fitDetails = model.fit(trainingInput['input'], trainingInput['target'], validation_split=0.3, epochs=30, verbose=1)\n",
    "\n",
    "history = plotHistory(fitDetails)\n",
    "\n",
    "evaluation = model.evaluate(validationInput['input'], validationInput['target'], verbose=0)\n",
    "models.append({'model':model, 'name':str(prefixName) + name, 'evaluation':evaluation, 'history':history})\n",
    "prefixName = prefixName + 1\n",
    "\n",
    "model.summary()\n",
    "for i in range(len(model.layers)):\n",
    "    print('Layer ' + str(i) + ': ', model.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0:  [array([[1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.],\n",
      "       [1.]], dtype=float32)]\n",
      "500/500 [==============================] - 0s 32us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00688037108629942"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers[0].set_weights([np.array([[1] for i in range(8)])])\n",
    "for i in range(len(model.layers)):\n",
    "    print('Layer ' + str(i) + ': ', model.layers[i].get_weights())\n",
    "\n",
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: 1 Dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 128)               1024      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 128       \n",
      "=================================================================\n",
      "Total params: 1,152\n",
      "Trainable params: 1,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Layer 0:  [array([[-0.08771282,  0.12342571, -0.1509666 , ..., -0.03016002,\n",
      "        -0.1680963 , -0.2023501 ],\n",
      "       [-0.05916896,  0.03884724,  0.05492501, ...,  0.15051104,\n",
      "        -0.03237598, -0.12645647],\n",
      "       [-0.07728605,  0.07725902, -0.03697509, ...,  0.00518212,\n",
      "        -0.08640809, -0.13549057],\n",
      "       ...,\n",
      "       [ 0.0167397 ,  0.18020485,  0.19262971, ..., -0.10134975,\n",
      "         0.20032914, -0.11687154],\n",
      "       [-0.00192995,  0.08819692,  0.06544609, ...,  0.03379193,\n",
      "        -0.17945603,  0.0691653 ],\n",
      "       [-0.20784023,  0.1045015 ,  0.04245366, ...,  0.13423811,\n",
      "         0.05399852, -0.10895137]], dtype=float32)]\n",
      "Layer 1:  [array([[-0.1957838 ],\n",
      "       [-0.11552928],\n",
      "       [-0.1581114 ],\n",
      "       [-0.17604437],\n",
      "       [ 0.11751157],\n",
      "       [-0.21279073],\n",
      "       [-0.13617316],\n",
      "       [-0.13052656],\n",
      "       [-0.0843129 ],\n",
      "       [ 0.10944918],\n",
      "       [ 0.18879631],\n",
      "       [-0.10294003],\n",
      "       [-0.07349883],\n",
      "       [ 0.15561709],\n",
      "       [-0.1057857 ],\n",
      "       [-0.08242609],\n",
      "       [-0.05019157],\n",
      "       [ 0.099235  ],\n",
      "       [-0.10745984],\n",
      "       [ 0.00230813],\n",
      "       [ 0.18524638],\n",
      "       [-0.02826089],\n",
      "       [ 0.11476916],\n",
      "       [-0.19368745],\n",
      "       [-0.13743985],\n",
      "       [-0.07365735],\n",
      "       [-0.13514401],\n",
      "       [-0.20002882],\n",
      "       [ 0.1733158 ],\n",
      "       [ 0.13446301],\n",
      "       [ 0.0956426 ],\n",
      "       [ 0.15754837],\n",
      "       [-0.13022247],\n",
      "       [-0.06600399],\n",
      "       [-0.09194116],\n",
      "       [ 0.17623734],\n",
      "       [-0.11359619],\n",
      "       [-0.08598714],\n",
      "       [-0.11069032],\n",
      "       [-0.21101144],\n",
      "       [ 0.01862487],\n",
      "       [ 0.18422097],\n",
      "       [ 0.05240324],\n",
      "       [-0.18998685],\n",
      "       [-0.14821917],\n",
      "       [-0.18984061],\n",
      "       [-0.20707226],\n",
      "       [ 0.10219857],\n",
      "       [-0.13546893],\n",
      "       [-0.00955293],\n",
      "       [-0.18924884],\n",
      "       [ 0.17712668],\n",
      "       [-0.10123195],\n",
      "       [-0.08471514],\n",
      "       [-0.0352038 ],\n",
      "       [-0.16266435],\n",
      "       [-0.01155892],\n",
      "       [ 0.1231772 ],\n",
      "       [-0.10879163],\n",
      "       [ 0.1967377 ],\n",
      "       [-0.04403125],\n",
      "       [-0.02328315],\n",
      "       [ 0.11717868],\n",
      "       [-0.05981287],\n",
      "       [ 0.00482637],\n",
      "       [ 0.16289681],\n",
      "       [ 0.17179787],\n",
      "       [-0.07111712],\n",
      "       [ 0.15576994],\n",
      "       [-0.14801626],\n",
      "       [-0.15696332],\n",
      "       [ 0.09898984],\n",
      "       [-0.13263628],\n",
      "       [ 0.21447927],\n",
      "       [-0.14152732],\n",
      "       [ 0.02399515],\n",
      "       [-0.14324614],\n",
      "       [ 0.18813092],\n",
      "       [ 0.20225391],\n",
      "       [ 0.18707764],\n",
      "       [-0.12567809],\n",
      "       [ 0.1201545 ],\n",
      "       [-0.1905739 ],\n",
      "       [ 0.18185449],\n",
      "       [-0.0193073 ],\n",
      "       [-0.2111947 ],\n",
      "       [-0.08134131],\n",
      "       [-0.10219678],\n",
      "       [-0.20873067],\n",
      "       [-0.04950693],\n",
      "       [ 0.09800857],\n",
      "       [ 0.1926521 ],\n",
      "       [ 0.16945714],\n",
      "       [-0.13499048],\n",
      "       [-0.14443278],\n",
      "       [ 0.18751481],\n",
      "       [-0.11704531],\n",
      "       [-0.10514836],\n",
      "       [ 0.17493615],\n",
      "       [ 0.01191735],\n",
      "       [ 0.19178501],\n",
      "       [ 0.16170314],\n",
      "       [-0.1693236 ],\n",
      "       [ 0.10078385],\n",
      "       [ 0.2053993 ],\n",
      "       [-0.14431366],\n",
      "       [ 0.18087557],\n",
      "       [ 0.07320133],\n",
      "       [ 0.06852794],\n",
      "       [ 0.21341309],\n",
      "       [ 0.01391883],\n",
      "       [-0.01726408],\n",
      "       [ 0.05909291],\n",
      "       [ 0.08484229],\n",
      "       [-0.09158988],\n",
      "       [-0.08687051],\n",
      "       [-0.20654923],\n",
      "       [ 0.00864872],\n",
      "       [ 0.171125  ],\n",
      "       [-0.00448284],\n",
      "       [ 0.11164418],\n",
      "       [ 0.05075023],\n",
      "       [-0.09419603],\n",
      "       [ 0.06733796],\n",
      "       [ 0.14968306],\n",
      "       [ 0.08512619],\n",
      "       [ 0.00481443],\n",
      "       [-0.01113173]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "name = 'LinearNetwork'\n",
    "settings = [['Dense', 128, 'relu', False],['Dense', 1, 'linear', False]]\n",
    "\n",
    "\n",
    "learningRate = 1*10**-5\n",
    "\n",
    "#model = makeDenseNetwork(inputSize, settings, learningRate, kernalInitializer='he_uniform')#, lossFunction=lossMetric)#eras.initializers.glorot_uniform(seed=None)\n",
    "model = Sequential()\n",
    "opt = adam(lr=learningRate)\n",
    "model.add(Dense(128, activation=\"relu\", use_bias=False, input_dim=8))\n",
    "model.add(Dense(1, activation=\"linear\", use_bias=False))\n",
    "model.compile(loss=MAE, optimizer=opt)\n",
    "\n",
    "model.summary()\n",
    "for i in range(len(model.layers)):\n",
    "    print('Layer ' + str(i) + ': ', model.layers[i].get_weights())\n",
    "    #print(model.evaluate(validationInput['input'], validationInput['target'], verbose=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 350 samples, validate on 150 samples\n",
      "Epoch 1/30\n",
      "350/350 [==============================] - 0s 246us/step - loss: 117530.0948 - val_loss: 126352.8845\n",
      "Epoch 2/30\n",
      "350/350 [==============================] - 0s 71us/step - loss: 117418.3660 - val_loss: 126235.5803\n",
      "Epoch 3/30\n",
      "350/350 [==============================] - 0s 79us/step - loss: 117309.0592 - val_loss: 126118.7166\n",
      "Epoch 4/30\n",
      "350/350 [==============================] - 0s 71us/step - loss: 117199.6881 - val_loss: 126002.6214\n",
      "Epoch 5/30\n",
      "350/350 [==============================] - 0s 78us/step - loss: 117090.1912 - val_loss: 125887.0228\n",
      "Epoch 6/30\n",
      "350/350 [==============================] - 0s 77us/step - loss: 116981.9828 - val_loss: 125769.5468\n",
      "Epoch 7/30\n",
      "350/350 [==============================] - 0s 80us/step - loss: 116871.7790 - val_loss: 125654.1638\n",
      "Epoch 8/30\n",
      "350/350 [==============================] - 0s 86us/step - loss: 116763.2093 - val_loss: 125537.7729\n",
      "Epoch 9/30\n",
      "350/350 [==============================] - 0s 85us/step - loss: 116653.4252 - val_loss: 125421.9472\n",
      "Epoch 10/30\n",
      "350/350 [==============================] - 0s 73us/step - loss: 116543.8538 - val_loss: 125305.3349\n",
      "Epoch 11/30\n",
      "350/350 [==============================] - 0s 74us/step - loss: 116433.9283 - val_loss: 125188.5401\n",
      "Epoch 12/30\n",
      "350/350 [==============================] - 0s 77us/step - loss: 116323.8721 - val_loss: 125071.5085\n",
      "Epoch 13/30\n",
      "350/350 [==============================] - 0s 77us/step - loss: 116214.2438 - val_loss: 124955.8451\n",
      "Epoch 14/30\n",
      "350/350 [==============================] - 0s 79us/step - loss: 116105.0842 - val_loss: 124838.9846\n",
      "Epoch 15/30\n",
      "350/350 [==============================] - 0s 89us/step - loss: 115996.8819 - val_loss: 124720.9267\n",
      "Epoch 16/30\n",
      "350/350 [==============================] - 0s 80us/step - loss: 115885.6038 - val_loss: 124606.1186\n",
      "Epoch 17/30\n",
      "350/350 [==============================] - 0s 78us/step - loss: 115774.6292 - val_loss: 124489.5784\n",
      "Epoch 18/30\n",
      "350/350 [==============================] - 0s 77us/step - loss: 115667.9613 - val_loss: 124371.5366\n",
      "Epoch 19/30\n",
      "350/350 [==============================] - 0s 75us/step - loss: 115556.6408 - val_loss: 124255.0598\n",
      "Epoch 20/30\n",
      "350/350 [==============================] - 0s 79us/step - loss: 115446.2379 - val_loss: 124139.6230\n",
      "Epoch 21/30\n",
      "350/350 [==============================] - 0s 75us/step - loss: 115337.6866 - val_loss: 124022.4205\n",
      "Epoch 22/30\n",
      "350/350 [==============================] - 0s 78us/step - loss: 115226.8409 - val_loss: 123906.9289\n",
      "Epoch 23/30\n",
      "350/350 [==============================] - 0s 74us/step - loss: 115118.7235 - val_loss: 123789.1105\n",
      "Epoch 24/30\n",
      "350/350 [==============================] - 0s 77us/step - loss: 115008.0558 - val_loss: 123673.1721\n",
      "Epoch 25/30\n",
      "350/350 [==============================] - 0s 74us/step - loss: 114898.1015 - val_loss: 123557.5671\n",
      "Epoch 26/30\n",
      "350/350 [==============================] - 0s 73us/step - loss: 114789.3814 - val_loss: 123438.8055\n",
      "Epoch 27/30\n",
      "350/350 [==============================] - 0s 75us/step - loss: 114678.6838 - val_loss: 123322.3605\n",
      "Epoch 28/30\n",
      "350/350 [==============================] - 0s 74us/step - loss: 114569.3496 - val_loss: 123204.9828\n",
      "Epoch 29/30\n",
      "350/350 [==============================] - 0s 85us/step - loss: 114460.2158 - val_loss: 123087.5583\n",
      "Epoch 30/30\n",
      "350/350 [==============================] - 0s 74us/step - loss: 114349.9972 - val_loss: 122971.0801\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x1152 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_32 (Dense)             (None, 128)               1024      \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 1)                 128       \n",
      "=================================================================\n",
      "Total params: 1,152\n",
      "Trainable params: 1,152\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Layer 0:  [array([[-0.09020173,  0.12061298, -0.15367521, ..., -0.02737004,\n",
      "        -0.16576841, -0.20521998],\n",
      "       [-0.06198069,  0.03651678,  0.0527227 , ...,  0.15288927,\n",
      "        -0.03051089, -0.12901895],\n",
      "       [-0.07956905,  0.07454852, -0.03948636, ...,  0.00734849,\n",
      "        -0.08479637, -0.13747586],\n",
      "       ...,\n",
      "       [ 0.01435929,  0.17853758,  0.19083801, ..., -0.09933987,\n",
      "         0.20143633, -0.11917718],\n",
      "       [-0.00468052,  0.08538616,  0.06254947, ...,  0.0364071 ,\n",
      "        -0.17707908,  0.06619792],\n",
      "       [-0.21022339,  0.10322456,  0.04092119, ...,  0.13565978,\n",
      "         0.0552313 , -0.11147749]], dtype=float32)]\n",
      "Layer 1:  [array([[-0.19886512],\n",
      "       [-0.11238661],\n",
      "       [-0.15980539],\n",
      "       [-0.17559865],\n",
      "       [ 0.11938144],\n",
      "       [-0.21194132],\n",
      "       [-0.13807446],\n",
      "       [-0.12812953],\n",
      "       [-0.08485974],\n",
      "       [ 0.11135199],\n",
      "       [ 0.19133578],\n",
      "       [-0.10347089],\n",
      "       [-0.07173885],\n",
      "       [ 0.1587525 ],\n",
      "       [-0.10874107],\n",
      "       [-0.08154637],\n",
      "       [-0.05324853],\n",
      "       [ 0.09796756],\n",
      "       [-0.1088175 ],\n",
      "       [-0.00076993],\n",
      "       [ 0.18781464],\n",
      "       [-0.0308184 ],\n",
      "       [ 0.11270615],\n",
      "       [-0.19260101],\n",
      "       [-0.14036372],\n",
      "       [-0.07484406],\n",
      "       [-0.13747202],\n",
      "       [-0.20164102],\n",
      "       [ 0.17091615],\n",
      "       [ 0.13509579],\n",
      "       [ 0.09414835],\n",
      "       [ 0.1552626 ],\n",
      "       [-0.132766  ],\n",
      "       [-0.06615037],\n",
      "       [-0.09143191],\n",
      "       [ 0.17847127],\n",
      "       [-0.11116483],\n",
      "       [-0.08375847],\n",
      "       [-0.11359325],\n",
      "       [-0.21399231],\n",
      "       [ 0.01606806],\n",
      "       [ 0.18219374],\n",
      "       [ 0.0506208 ],\n",
      "       [-0.1905056 ],\n",
      "       [-0.15106231],\n",
      "       [-0.18686906],\n",
      "       [-0.20975094],\n",
      "       [ 0.10434545],\n",
      "       [-0.13859645],\n",
      "       [-0.01257177],\n",
      "       [-0.18706962],\n",
      "       [ 0.17497264],\n",
      "       [-0.0987873 ],\n",
      "       [-0.08322012],\n",
      "       [-0.03776946],\n",
      "       [-0.15963718],\n",
      "       [-0.01298587],\n",
      "       [ 0.1251905 ],\n",
      "       [-0.11169568],\n",
      "       [ 0.19494851],\n",
      "       [-0.04718817],\n",
      "       [-0.02394717],\n",
      "       [ 0.11853366],\n",
      "       [-0.05809096],\n",
      "       [ 0.0018477 ],\n",
      "       [ 0.16203757],\n",
      "       [ 0.17402987],\n",
      "       [-0.07345761],\n",
      "       [ 0.15349112],\n",
      "       [-0.15091662],\n",
      "       [-0.15998985],\n",
      "       [ 0.09615227],\n",
      "       [-0.13017985],\n",
      "       [ 0.21645245],\n",
      "       [-0.14098349],\n",
      "       [ 0.02555481],\n",
      "       [-0.14625941],\n",
      "       [ 0.18507412],\n",
      "       [ 0.20503789],\n",
      "       [ 0.18464544],\n",
      "       [-0.12831254],\n",
      "       [ 0.12142735],\n",
      "       [-0.19064151],\n",
      "       [ 0.18470152],\n",
      "       [-0.02150238],\n",
      "       [-0.21411426],\n",
      "       [-0.08213191],\n",
      "       [-0.10418751],\n",
      "       [-0.21064988],\n",
      "       [-0.05137571],\n",
      "       [ 0.09504442],\n",
      "       [ 0.19109488],\n",
      "       [ 0.1664802 ],\n",
      "       [-0.13647708],\n",
      "       [-0.14201547],\n",
      "       [ 0.18481867],\n",
      "       [-0.11964709],\n",
      "       [-0.10388673],\n",
      "       [ 0.17614092],\n",
      "       [ 0.01434174],\n",
      "       [ 0.19027136],\n",
      "       [ 0.1626072 ],\n",
      "       [-0.1717871 ],\n",
      "       [ 0.10338867],\n",
      "       [ 0.20730525],\n",
      "       [-0.14517364],\n",
      "       [ 0.1806028 ],\n",
      "       [ 0.07634785],\n",
      "       [ 0.06718944],\n",
      "       [ 0.21088539],\n",
      "       [ 0.01425511],\n",
      "       [-0.0150421 ],\n",
      "       [ 0.05677384],\n",
      "       [ 0.08223235],\n",
      "       [-0.09171516],\n",
      "       [-0.08986657],\n",
      "       [-0.20927055],\n",
      "       [ 0.00551411],\n",
      "       [ 0.17378673],\n",
      "       [-0.00674513],\n",
      "       [ 0.11385336],\n",
      "       [ 0.05088249],\n",
      "       [-0.0973074 ],\n",
      "       [ 0.06473131],\n",
      "       [ 0.1527911 ],\n",
      "       [ 0.087854  ],\n",
      "       [ 0.0019855 ],\n",
      "       [-0.01431668]], dtype=float32)]\n"
     ]
    }
   ],
   "source": [
    "fitDetails = model.fit(X, y, epochs=30, validation_split=0.3, verbose=1)\n",
    "#fitDetails = model.fit(trainingInput['input'], trainingInput['target'], validation_split=0.3, epochs=30, verbose=1)\n",
    "#fitDetails = model.fit(trainingInput['input'], trainingInput['target'], validation_split=0.3, epochs=30, verbose=1)\n",
    "\n",
    "history = plotHistory(fitDetails)\n",
    "\n",
    "evaluation = model.evaluate(validationInput['input'], validationInput['target'], verbose=0)\n",
    "models.append({'model':model, 'name':str(prefixName) + name, 'evaluation':evaluation, 'history':history})\n",
    "prefixName = prefixName + 1\n",
    "\n",
    "model.summary()\n",
    "for i in range(len(model.layers)):\n",
    "    print('Layer ' + str(i) + ': ', model.layers[i].get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer weight shape (8, 128) not compatible with provided weight shape (8, 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-380-73006b9622ba>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Layer '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m': '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\BEPGPU\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36meager_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meager_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m                 \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m             \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprev_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\BEPGPU\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36mset_weights\u001b[1;34m(self, weights)\u001b[0m\n\u001b[0;32m   1124\u001b[0m                                  \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1125\u001b[0m                                  \u001b[1;34m' not compatible with '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1126\u001b[1;33m                                  'provided weight shape ' + str(w.shape))\n\u001b[0m\u001b[0;32m   1127\u001b[0m             \u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m         \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Layer weight shape (8, 128) not compatible with provided weight shape (8, 1)"
     ]
    }
   ],
   "source": [
    "model.layers[0].set_weights([np.array([[1] for i in range(8)])])\n",
    "for i in range(len(model.layers)):\n",
    "    print('Layer ' + str(i) + ': ', model.layers[i].get_weights())\n",
    "\n",
    "model.evaluate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finishing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print(evaluation)\n",
    "\n",
    "y = [[],[]]\n",
    "for i in range(len(x)):\n",
    "    y[0].append(model.predict([[testData['eigenvalues'][i]]])[0][0])\n",
    "    y[1].append(testData['potentialEnergy'][i])\n",
    "\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(8, 8))\n",
    "ax1.plot(list(x), y[0], label='Predicted potential energy')\n",
    "ax1.plot(list(x), y[1], label='Real potential energy')\n",
    "ax1.legend()\n",
    "ax1.set_xlabel('Position of atom')\n",
    "ax1.set_ylabel('Potential energy')\n",
    "ax1.set_title('Predicted potential energy and real potential energy for different positions.')\n",
    "plt.show()\n",
    "\n",
    "\"\"\"\n",
    "makePredictionPlot(model, validationData)\n",
    "\n",
    "for i in validationPlots:\n",
    "    plotAndPredict(model, validationData, i)\n",
    "\"\"\"\n",
    "#\"\"\"\n",
    "print(model.summary())\n",
    "for layer in model.layers:\n",
    "    print(layer.get_weights())\n",
    "#\"\"\"\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(models)):\n",
    "    model = models[i]['model']\n",
    "    print(models[i]['name'])\n",
    "    print(model.summary())\n",
    "    print(models[i]['evaluation'])\n",
    "    model.save(saveFolder + '/' + models[i]['name'] + str(models[i]['evaluation']) + '.h5')\n",
    "    print('\\n \\n')\n",
    "\n",
    "#model = models[1][0]\n",
    "#model.save('model1.h5')\n",
    "#model = load_model('model1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TEMP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.shape(trainingInput['input']))\n",
    "print(np.shape(trainingInput['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "#print(dataset)\n",
    "# split into input (X) and output (y) variables\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "\n",
    "print(np.shape(X))\n",
    "print(np.shape(y))\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, y, epochs=150, batch_size=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python-BEPGPU",
   "language": "python",
   "name": "python-bepgpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
